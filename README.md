[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://github.com/amrzv/awesome-colab-notebooks)](https://hits.seeyoufarm.com)
# Awesome colab notebooks collection for ML experiments
## Research
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| nb-repo-test | Sample Notebook to test E2E Notebook Collections | <ul><li>[Shivansh Jain](https://github.com/shivansh-jain-e2e)</li> </ul> | [![](https://img.shields.io/github/stars/automl/TabPFN?style=social)](https://github.com/automl/TabPFN) <ul><li>[<img src="images/e2elogo.png" alt="E2E MyAccount" height=20/>](https://thor-gpu.e2enetworks.net/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/?notebookURL=https://github.com/shivansh-jain-e2e/e2e-notebook-collection/blob/nb-repo-test/nb-repo-test.ipynb/) | 19.06.2023 |
| nb-repo-test | Sample Notebook to test E2E Notebook Collections | <ul><li>[Shivansh Jain](https://github.com/shivansh-jain-e2e)</li> </ul> | [![](https://img.shields.io/github/stars/automl/TabPFN?style=social)](https://github.com/automl/TabPFN) <ul><li>[<img src="images/e2elogo.png" alt="E2E MyAccount" height=20/>](https://thor-gpu.e2enetworks.net/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/shivansh-jain-e2e/e2e-notebook-collection/blob/nb-repo-test/nb-repo-test.ipynb//) | 11.07.2023 |
<!-- | TabPFN | Neural network that learned to do tabular data prediction | <ul><li>[Noah Hollmann](https://github.com/noahho)</li> <li>[Samuel Müller](https://scholar.google.com/citations?user=pevYEjAAAAAJ)</li> <li>[Katharina Eggensperger](https://github.com/KEggensperger)</li> <li>[Frank Hutter](https://ml.informatik.uni-freiburg.de/profile/hutter/)</li></ul> | [![](https://img.shields.io/github/stars/automl/TabPFN?style=social)](https://github.com/automl/TabPFN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2207.01848), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.11189), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.01342), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.03253), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.11189), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10510)</li><li>[blog post](https://www.automl.org/tabpfn-a-transformer-that-solves-small-tabular-classification-problems-in-a-second/)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/tunguz/status/1578730907711655937)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/BGTO5N5-ack)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/194mCs6SEPEW6C0rcP7xWzcEtt1RBc8jJ) | 31.05.2023 | -->
| AudioLDM | Text-to-audio system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining latents | <ul><li>[Haohe Liu](https://haoheliu.github.io/)</li> <li>[Zehua Chen](https://github.com/zehuachenImperial)</li> <li>[Yi Yuan](https://www.surrey.ac.uk/people/yi-yuan)</li><details><summary>others</summary><li>[Xinhao Mei](https://xinhaomei.github.io/)</li> <li>[Xubo Liu](https://liuxubo717.github.io/)</li> <li>[Danilo Mandic](https://www.imperial.ac.uk/people/d.mandic)</li> <li>[Wenwu Wang](http://personal.ee.surrey.ac.uk/Personal/W.Wang/)</li> <li>[Mark Plumbley](https://www.surrey.ac.uk/people/mark-plumbley)</li></ul></details> | [![](https://img.shields.io/github/stars/haoheliu/AudioLDM?style=social)](https://github.com/haoheliu/AudioLDM) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2301.12503)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/LAION-AI/CLAP), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/stable-diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/toshas/torch-fidelity)</li><li>[project](https://audioldm.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/_0VTltNYhao)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/olaviinha/NeuralTextToAudio/blob/main/AudioLDM_pub.ipynb/) | 31.05.2023 |
| AlphaFold | Highly accurate protein structure prediction | <ul><li>[John Jumper](https://scholar.google.com/citations?user=a5goOh8AAAAJ)</li> <li>[Richard Evans](http://www.doc.ic.ac.uk/~re14/)</li> <li>[Alexander Pritzel](https://scholar.google.com/citations?user=GPgAyU0AAAAJ)</li><details><summary>others</summary><li>[Tim Green](http://tfgg.me/)</li> <li>[Michael Figurnov](https://figurnov.ru/)</li> <li>[Olaf Ronneberger](https://lmb.informatik.uni-freiburg.de/people/ronneber/)</li> <li>[Kathryn Tunyasuvunakool](https://scholar.google.com/citations?user=eEqNGagAAAAJ)</li> <li>[Russ Bates](https://scholar.google.com/citations?user=Koes5ewAAAAJ)</li> <li>[Augustin Žídek](https://augustin.zidek.eu/)</li> <li>[Anna Potapenko](http://apotapenko.com/)</li> <li>[Alex Bridgland](https://scholar.google.com/citations?user=VWmXKPMAAAAJ)</li> <li>[Clemens Meyer](https://scholar.google.com/citations?user=EWLZiM8AAAAJ)</li> <li>[Simon Kohl](https://www.simonkohl.com/)</li> <li>[Andrew Ballard](https://scholar.google.com/citations?user=syjQhAMAAAAJ)</li> <li>[Bernardino Romera-Paredes](https://sites.google.com/site/romeraparedes/)</li> <li>[Stanislav Nikolov](https://scholar.google.co.uk/citations?user=O-b7pBEAAAAJ)</li> <li>[Rishub Jain](http://rishub.me/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/alphafold?style=social)](https://github.com/deepmind/alphafold/) <ul><li>[blog post](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology), [blog post](https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/tree), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/chex)</li><li>[paper](https://www.nature.com/articles/s41586-021-03819-2), [paper](https://www.nature.com/articles/s41586-021-03828-1)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/alphafold)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/AlphaFold)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=gg7WjuFs8F4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=B9PL__gVxLI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb/) | 03.05.2023 |
| DFL-Colab | This project provides you IPython Notebook to use DeepFaceLab | [chervonij](https://github.com/chervonij) | [![](https://img.shields.io/github/stars/iperov/DeepFaceLab?style=social)](https://github.com/iperov/DeepFaceLab) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05535)</li><li>[guide](https://mrdeepfakes.com/forums/thread-guide-deepfacelab-google-colab-tutorial)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCTKBl8kB6DJ_qLnk1NGDGbQ)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/chervonij/DFL-Colab/blob/master/DFL_Colab.ipynb/) | 30.04.2023 |
| MiniGPT-4 | Enhancing Vision-language Understanding with Advanced Large Language Models | <ul><li>[Deyao Zhu](https://tsutikgiau.github.io/)</li> <li>[Jun Chen](https://junchen14.github.io/)</li> <li>[Xiaoqian Shen](https://xiaoqian-shen.github.io/)</li><details><summary>others</summary><li>[Xiang Li](https://xiangli.ac.cn/)</li> <li>[Mohamed Elhoseiny](https://www.mohamed-elhoseiny.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4?style=social)](https://github.com/Vision-CAIR/MiniGPT-4) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2304.10592)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lm-sys/FastChat)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/Vision-CAIR/MiniGPT-4)</li><li>[project](https://minigpt-4.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/__tftoxpBAw), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/wUONNv7guXI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/hNAFuuXYL58), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/SAjrpYjx0ps)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R) | 23.04.2023 |
| YOLOv5 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | [![](https://img.shields.io/github/stars/ultralytics/yolov5?style=social)](https://github.com/ultralytics/yolov5) <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov5), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ultralytics/yolov5/blob/master/tutorial.ipynb/) | 21.04.2023 |
| YOLOv3 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | [![](https://img.shields.io/github/stars/ultralytics/yolov3?style=social)](https://github.com/ultralytics/yolov3) <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov3), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ultralytics/yolov3/blob/master/tutorial.ipynb/) | 21.04.2023 |
| Segment Anything | The Segment Anything Model produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image | <ul><li>[Alexander Kirillov](https://alexander-kirillov.github.io/)</li> <li>[Eric Mintun](https://ericmintun.github.io/)</li> <li>[Nikhila Ravi](https://nikhilaravi.com/)</li><details><summary>others</summary><li>[Hanzi Mao](https://hanzimao.me/)</li> <li>[Chloé Rolland](https://www.linkedin.com/in/chlo%C3%A9-rolland-223135a/)</li> <li>[Laura Gustafson](https://scholar.google.com/citations?user=c8IpF9gAAAAJ)</li> <li>[Tete Xiao](https://tetexiao.com/)</li> <li>[Spencer Whitehead](https://www.spencerwhitehead.com/)</li> <li>[Alex Berg](http://acberg.com/)</li> <li>[Wan-Yen Lo](https://github.com/wanyenlo)</li> <li>[Piotr Dollar](https://pdollar.github.io/)</li> <li>[Ross Girshick](https://www.rossgirshick.info/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social)](https://github.com/facebookresearch/segment-anything) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2304.02643)</li><li>[blog post](https://ai.facebook.com/research/publications/segment-anything/), [blog post](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)</li><li>[data](https://ai.facebook.com/datasets/segment-anything/)</li><li>[website](https://segment-anything.com/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/2O_vecl28OA), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/fVeW9a6wItM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/FjYE0tKWOiY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb/) | 10.04.2023 |
| EVA3D | High-quality unconditional 3D human generative model that only requires 2D image collections for training | <ul><li>[Fangzhou Hong](https://hongfz16.github.io/)</li> <li>[Zhaoxi Chen](https://frozenburning.github.io/)</li> <li>[Yushi Lan](https://github.com/NIRVANALAN)</li><details><summary>others</summary><li>[Liang Pan](https://github.com/paul007pl)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/hongfz16/EVA3D?style=social)](https://github.com/hongfz16/EVA3D) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2210.04888)</li><li>[project](https://hongfz16.github.io/projects/EVA3D.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/JNV0FJ0aDWM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/M-kyvzTQrBI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/hongfz16/EVA3D/blob/main/notebook/EVA3D_Demo.ipynb/) | 06.04.2023 |
| Stable Dreamfusion | Using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis | <ul><li>[Jiaxiang Tang](https://me.kiui.moe/)</li> <li>[Ben Poole](https://cs.stanford.edu/~poole/)</li> <li>[Ajay Jain](https://ajayj.com/)</li><details><summary>others</summary><li>[Jon Barron](https://jonbarron.info/)</li> <li>[Ben Mildenhall](https://bmild.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/ashawkey/stable-dreamfusion?style=social)](https://github.com/ashawkey/stable-dreamfusion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.14988)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ashawkey/torch-ngp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hoffstadt/DearPyGui)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/runwayml/stable-diffusion-v1-5)</li><li>[project](https://dreamfusion3d.github.io/)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/uM5NPodZZ1U?t=219), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/zWD5ZR5GtJM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L3G0dx1Q0R8), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/dIgDbBTztUM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF) | 04.04.2023 |
| Parallel WaveGAN | State-of-the-art non-autoregressive models to build your own great vocoder | [Tomoki Hayashi](https://kan-bayashi.github.io/) | [![](https://img.shields.io/github/stars/kan-bayashi/ParallelWaveGAN?style=social)](https://github.com/kan-bayashi/ParallelWaveGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.11480), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.06711), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05106)</li><li>[demo](https://kan-bayashi.github.io/ParallelWaveGAN/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/tacotron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/espnet/espnet)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb/) | 03.04.2023 |
| Wav2Lip | A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild | <ul><li>[Prajwal Renukanand](https://github.com/prajwalkr)</li> <li>[Rudrabha Mukhopadhyay](https://rudrabha.github.io/)</li> <li>[Vinay Namboodiri](https://vinaypn.github.io/)</li> <li>[C. V. Jawahar](https://faculty.iiit.ac.in/~jawahar/)</li></ul> | [![](https://img.shields.io/github/stars/Rudrabha/Wav2Lip?style=social)](https://github.com/Rudrabha/Wav2Lip) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.10010)</li><li>[data](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)</li><li>[demo](http://bhaasha.iiit.ac.in/lipsync/)</li><li>[project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=0fXaDCZNOJc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/eyaler/avatars4all/blob/master/melaflefon.ipynb/) | 31.03.2023 |
| UniFormer | Unified Transformer for Efficient Spatiotemporal Representation Learning | <ul><li>[Kunchang Li](https://github.com/Andy1621)</li> <li>[Yali Wang](https://scholar.google.com/citations?user=hD948dkAAAAJ)</li> <li>[Peng Gao](https://gaopengcuhk.github.io/)</li><details><summary>others</summary><li>[Guanglu Song](https://songguanglu.github.io/)</li> <li>[Yu Liu](https://liuyu.us/)</li> <li>[Hongsheng Li](http://www.ee.cuhk.edu.hk/~hsli/)</li> <li>[Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/index.html)</li></ul></details> | [![](https://img.shields.io/github/stars/Sense-X/UniFormer?style=social)](https://github.com/Sense-X/UniFormer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.04676), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.09450), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.10858), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17239)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/zihangJiang/TokenLabeling), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/deit), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fvcore), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookincubator/submitit), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/SlowFast), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/whai362/PVT/tree/v2/segmentation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HRNet/HRFormer/tree/main/pose)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Sense-X/uniformer_image_demo), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Sense-X/uniformer_video_demo), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Andy1621/uniformer_image_detection), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Andy1621/uniformer_image_segmentation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb/) | 31.03.2023 |
| PIFuHD | Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization | <ul><li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Tomas Simon](http://www.cs.cmu.edu/~tsimon/)</li> <li>[Jason Saragih](https://scholar.google.com/citations?user=ss-IvjMAAAAJ)</li> <li>[Hanbyul Joo](https://jhugestar.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/facebookresearch/pifuhd?style=social)](https://github.com/facebookresearch/pifuhd) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00452)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/uEDqCxvF5yc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=8qnwbbDS8xk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt) | 26.03.2023 |
| Visual ChatGPT | Connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting | <ul><li>[Chenfei Wu](https://github.com/chenfei-wu)</li> <li>[Shengming Yin](https://github.com/shengming-yin)</li> <li>[Weizhen Qi](https://github.com/WeizhenQ)</li><details><summary>others</summary><li>[Xiaodong Wang](https://wang-xiaodong1899.github.io/)</li> <li>[Zecheng Tang](https://github.com/CODINNLG)</li> <li>[Nan Duan](https://nanduan.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social)](https://github.com/microsoft/visual-chatgpt) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2303.04671)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/hwchase17/langchain), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lllyasviel/ControlNet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/timothybrooks/instruct-pix2pix), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/timojl/clipseg)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/0UfXlFUwLms), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/7YEiEyfPF5U)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/11BtP3h-w0dZjA-X8JsS9_eo8OeGYvxXB) | 15.03.2023 |
| LaMa | Resolution-robust Large Mask Inpainting with Fourier Convolutions | <ul><li>[Roman Suvorov](https://github.com/windj007)</li> <li>[Elizaveta Logacheva](https://github.com/elimohl)</li> <li>[Anton Mashikhin](https://www.linkedin.com/in/heyt0ny/)</li><details><summary>others</summary><li>[Anastasia Remizova](https://github.com/feathernox)</li> <li>[Arsenii Ashukha](https://ashukha.com/)</li> <li>[Aleksei Silvestrov](https://www.linkedin.com/in/%D0%B0%D0%BB%D0%B5%D0%BA%D1%81%D0%B5%D0%B9-%D1%81%D0%B8%D0%BB%D1%8C%D0%B2%D0%B5%D1%81%D1%82%D1%80%D0%BE%D0%B2-141b99b6/)</li> <li>[Naejin Kong](https://github.com/naejin-kong)</li> <li>[Harshith Goka](https://github.com/h9399-goka)</li> <li>[Kiwoong Park](https://github.com/kyoong-park)</li> <li>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/)</li></ul></details> | [![](https://img.shields.io/github/stars/saic-mdal/lama?style=social)](https://github.com/saic-mdal/lama) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.07161)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/andy971022/auto-lama), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/richzhang/PerceptualSimilarity), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Po-Hsun-Su/pytorch-ssim), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mseitzer/pytorch-fid)</li><li>[project](https://saic-mdal.github.io/lama-project/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb/) | 15.02.2023 |
| GPEN | GAN Prior Embedded Network for Blind Face Restoration in the Wild | <ul><li>[Tao Yang](https://cg.cs.tsinghua.edu.cn/people/~tyang/)</li> <li>[Peiran Ren](https://scholar.google.com/citations?&user=x5dEuxsAAAAJ)</li> <li>[Xuansong Xie](https://scholar.google.com/citations?user=M0Ei1zkAAAAJ)</li> <li>[Lei Zhang](http://www4.comp.polyu.edu.hk/~cslzhang/)</li></ul> | [![](https://img.shields.io/github/stars/yangxy/GPEN?style=social)](https://github.com/yangxy/GPEN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.06070)</li><li>[demo](https://vision.aliyun.com/experience/detail?spm=a211p3.14020179.J_7524944390.17.66cd4850wVDkUQ&tagName=facebody&children=EnhanceFace)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/biubug6/Pytorch_Retinaface), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yangxy/GPEN/blob/main/GPEN.ipynb/) | 15.02.2023 |
| YOLOv6 | Single-stage object detection framework dedicated to industrial applications | <ul><li>[Kaiheng Weng](https://github.com/khwengXU)</li> <li>[Meng Cheng](https://github.com/MTChengMeng)</li> <li>[Yiduo Li](https://github.com/yili123123)</li><details><summary>others</summary><li>[Xiangxiang Chu](https://scholar.google.com/citations?&user=jn21pUsAAAAJ)</li> <li>[Xiaolin Wei](https://scholar.google.com/citations?user=s5b7lU4AAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/meituan/YOLOv6?style=social)](https://github.com/meituan/YOLOv6) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.02976), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2301.05586)</li><li>[blog post](https://learnopencv.com/yolov6-object-detection/)</li><li>[data](https://cocodataset.org/#download)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://yolov6-docs.readthedocs.io/zh_CN/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/FeiGeChuanShu/ncnn-android-yolov6), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/ort/cv/yolov6.cpp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Linaom1214/TensorRT-For-YOLO-Series), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zhiqwang/yolov5-rt-stack/tree/main/deployment/tensorrt-yolov6)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/3OpwcGU7VvE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/GJ0lVOE3a7c), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/3hqkbqJ5ag8), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/fFCWrMFH2UY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/meituan/YOLOv6/blob/master/turtorial.ipynb/) | 14.02.2023 |
| CutLER | Simple approach for training unsupervised object detection and segmentation models | <ul><li>[Xudong Wang](https://people.eecs.berkeley.edu/~xdwang/)</li> <li>[Rohit Girdhar](https://rohitgirdhar.github.io/)</li> <li>[Stella Yu](https://www1.icsi.berkeley.edu/~stellayu/)</li> <li>[Ishan Misra](https://imisra.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/facebookresearch/CutLER?style=social)](https://github.com/facebookresearch/CutLER) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2301.11320), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.02677)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html)</li><li>[project](http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1NgEyFHvOfuA2MZZnfNPWg1w5gSr3HOBb) | 11.02.2023 |
| Disco Diffusion | A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations | <ul><li>[Max Ingham](https://github.com/somnai-dreams)</li> <li>[Adam Letts](https://linktr.ee/gandamu)</li> <li>[Daniel Russell](https://github.com/russelldc)</li> <li>[Chigozie Nri](https://github.com/chigozienri)</li></ul> | [![](https://img.shields.io/github/stars/alembics/disco-diffusion?style=social)](https://github.com/alembics/disco-diffusion) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/guided-diffusion)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/_DtWfh9oS54), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/gWxmtdZL8FE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/yVJB6oD0_gM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb/) | 11.02.2023 |
| DALL·E Mini | Generate images from a text prompt | <ul><li>[Boris Dayma](https://github.com/borisdayma)</li> <li>[Suraj Patil](https://github.com/patil-suraj)</li> <li>[Pedro Cuenca](https://github.com/pcuenca)</li><details><summary>others</summary><li>[Khalid Saifullah](https://khalidsaifullaah.github.io/)</li> <li>[Tanishq Abraham](https://github.com/tmabraham)</li> <li>[Phúc H. Lê Khắc](https://lkhphuc.com/)</li> <li>[Luke Melas](https://lukemelas.github.io/)</li> <li>[Ritobrata Ghosh](https://ghosh-r.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/borisdayma/dalle-mini?style=social)](https://github.com/borisdayma/dalle-mini) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.08981), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.13461), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.04015)</li><li>[blog post](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA)</li><li>[data](https://aclanthology.org/P18-1238/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/flax-community/dalle-mini)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb/) | 10.02.2023 |
| Open-Unmix | A deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists | <ul><li>[Fabian-Robert Stöter](http://faroit.com/)</li> <li>[Antoine Liutkus](https://github.com/aliutkus)</li></ul> | [![](https://img.shields.io/github/stars/sigsep/open-unmix-pytorch?style=social)](https://github.com/sigsep/open-unmix-pytorch) <ul><li>[data](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sigsep/norbert)</li><li>[paper](https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/sota/music-source-separation-on-musdb18?p=open-unmix-a-reference-implementation-for)</li><li>[project](https://sigsep.github.io/open-unmix/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ) | 09.02.2023 |
| OWL-ViT | Simple Open-Vocabulary Object Detection with Vision Transformers | <ul><li>[Matthias Minderer](http://matthias.minderer.net/)</li> <li>[Alexey Gritsenko](https://github.com/AlexeyG)</li> <li>[Austin Stone](https://github.com/AustinCStone)</li><details><summary>others</summary><li>[Maxim Neumann](https://github.com/maximneumann)</li> <li>[Dirk Weissenborn](https://github.com/dirkweissenborn)</li> <li>[Alexey Dosovitskiy](https://scholar.google.com/citations?user=FXNJRDoAAAAJ)</li> <li>[Aravindh Mahendran](https://github.com/aravindhm)</li> <li>[Anurag Arnab](https://github.com/anuragarnab)</li> <li>[Mostafa Dehghani](https://mostafadehghani.com/)</li> <li>[Zhuoran Shen](https://cmsflash.github.io/)</li> <li>[Xiao Wang](https://scholar.google.com/citations?user=ukyXqzMAAAAJ)</li> <li>[Xiaohua Zhai](https://github.com/xiaohuazhai)</li> <li>[Thomas Kipf](https://tkipf.github.io/)</li> <li>[Neil Houlsby](https://neilhoulsby.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/google-research/scenic?style=social)](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.06230)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/transformers/model_doc/owlvit)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb/) | 08.02.2023 |
| GrooVAE | Some applications of machine learning for generating and manipulating beats and drum performances | <ul><li>[Jon Gillick](https://www.jongillick.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li></ul> | [![](https://img.shields.io/github/stars/magenta/magenta?style=social)](https://github.com/magenta/magenta/tree/main/magenta/models/music_vae) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.06118)</li><li>[blog post](https://g.co/magenta/groovae)</li><li>[data](https://g.co/magenta/groove-datasets)</li><li>[web app](https://groove-drums.glitch.me/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=x2YLmXzovDo)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/magenta-demos/blob/master/colab-notebooks/GrooVAE.ipynb/) | 01.02.2023 |
| Multitrack MusicVAE | The models in this notebook are capable of encoding and decoding single measures of up to 8 tracks, optionally conditioned on an underlying chord | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Colin Raffel](https://colinraffel.com//)</li><details><summary>others</summary><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul></details> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.00195)</li><li>[blog post](http://g.co/magenta/multitrack)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb/) | 01.02.2023 |
| MusicVAE | A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music | <ul><li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Colin Raffel](https://colinraffel.com//)</li><details><summary>others</summary><li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul></details> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1803.05428)</li><li>[blog post](https://g.co/magenta/music-vae)</li><li>[project](https://magenta.tensorflow.org/music-vae)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb/) | 01.02.2023 |
| Learning to Paint | Learning to Paint With Model-based Deep Reinforcement Learning | [Manuel Romero](https://mrm8488.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.04411)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YmOgKZ5oipk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mrm8488/shared_colab_notebooks/blob/master/custom_learningtopaint.ipynb/) | 01.02.2023 |
| VALL-E | Language modeling approach for text to speech synthesis | <ul><li>[Chengyi Wang](https://cywang97.github.io/)</li> <li>[Sanyuan Chen](https://sanyuan-chen.github.io/)</li> <li>[Yu Wu](https://www.microsoft.com/en-us/research/people/yuwu1/)</li><details><summary>others</summary><li>[Ziqiang Zhang](https://github.com/zz12375)</li> <li>[Long Zhou](https://long-zhou.github.io/)</li> <li>[Shujie Liu](https://www.microsoft.com/en-us/research/people/shujliu/)</li> <li>[Zhuo Chen](https://www.microsoft.com/en-us/research/people/zhuc/)</li> <li>[Yanqing Liu](https://scholar.google.com/citations?user=dIJFz4UAAAAJ)</li> <li>[Huaming Wang](https://scholar.google.com/citations?user=aJDLg5IAAAAJ)</li> <li>[Jinyu Li](https://www.microsoft.com/en-us/research/people/jinyli/)</li> <li>[Lei He](https://scholar.google.com/citations?user=EKl9yY8AAAAJ)</li> <li>[Sheng Zhao](https://scholar.google.com/citations?user=689bIIwAAAAJ)</li> <li>[Furu Wei](https://www.microsoft.com/en-us/research/people/fuwei/)</li></ul></details> | [![](https://img.shields.io/github/stars/enhuiz/vall-e?style=social)](https://github.com/enhuiz/vall-e) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2301.02111)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/encodec), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeed#requirements)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://vidrihmarko.medium.com/mind-blowing-vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers-f002560ecd6)</li><li>[project](https://valle-demo.github.io/)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/MachineLearning/comments/104ixvi/r_neural_codec_language_models_are_zeroshot_text/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/F6HSsVIkqIU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ZehhrrQGmt4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-3MPZxRxvV4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ha2WjP7zfno)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1wEze0kQ0gt9B3bQmmbtbSXCoCTpq5vg-) | 18.01.2023 |
| Instant-NGP | Instant Neural Graphics Primitives with a Multiresolution Hash Encoding | <ul><li>[Thomas Müller](https://tom94.net/)</li> <li>[Alex Evans](https://research.nvidia.com/person/alex-evans)</li> <li>[Christoph Schied](https://research.nvidia.com/person/christoph-schied)</li> <li>[Alexander Keller](https://research.nvidia.com/person/alex-keller)</li></ul> | [![](https://img.shields.io/github/stars/NVlabs/instant-ngp?style=social)](https://github.com/NVlabs/instant-ngp) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.05989)</li><li>[blog post](https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/tiny-cuda-nn), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/IDLabMedia/large-lightfields-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nickponline/dd-nerf-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ocornut/imgui), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nothings/stb)</li><li>[project](https://nvlabs.github.io/instant-ngp/)</li><li>[tutorial](https://www.nvidia.com/en-us/on-demand/session/siggraph2022-sigg22-s-16/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/j8tMk-GE8hY), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/8GbENSmdVeE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/DJ2hcC1orc4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/z3-fjYzd0BA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/NVlabs/instant-ngp/blob/master/notebooks/instant_ngp.ipynb/) | 18.01.2023 |
| Fourier Feature Networks | Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains | <ul><li>[Matthew Tancik](https://www.matthewtancik.com/)</li> <li>[Pratul Srinivasan](https://pratulsrinivasan.github.io/)</li> <li>[Ben Mildenhall](https://bmild.github.io/)</li><details><summary>others</summary><li>[Sara Fridovich-Keil](https://people.eecs.berkeley.edu/~sfk/)</li> <li>[Nithin Raghavan](https://cseweb.ucsd.edu//~n2raghavan/)</li> <li>[Utkarsh Singhal](https://scholar.google.com/citations?user=lvA86MYAAAAJ)</li> <li>[Ravi Ramamoorthi](https://cseweb.ucsd.edu//~ravir/)</li> <li>[Jon Barron](https://jonbarron.info/)</li> <li>[Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html)</li></ul></details> | [![](https://img.shields.io/github/stars/tancik/fourier-feature-networks?style=social)](https://github.com/tancik/fourier-feature-networks) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.07572)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html)</li><li>[project](https://bmild.github.io/fourfeat/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nVA6K6Sn2S4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tancik/fourier-feature-networks/blob/master/Demo.ipynb/) | 17.01.2023 |
| HybrIK | Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation | <ul><li>[Jiefeng Li](https://jeffli.site/)</li> <li>[Chao Xu](https://www.isdas.cn/)</li> <li>[Zhicun Chen](https://github.com/chenzhicun)</li><details><summary>others</summary><li>[Siyuan Bian](https://github.com/biansy000)</li> <li>[Lixin Yang](https://lixiny.github.io/)</li> <li>[Cewu Lu](https://www.mvig.org/)</li></ul></details> | [![](https://img.shields.io/github/stars/Jeff-sjtu/HybrIK?style=social)](https://github.com/Jeff-sjtu/HybrIK) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.14672)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mks0601/3DMPPE_POSENET_RELEASE)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/sota/3d-human-pose-estimation-on-3dpw?p=hybrik-a-hybrid-analytical-neural-inverse)</li><li>[project](https://jeffli.site/HybrIK/)</li><li>[supp](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Li_HybrIK_A_Hybrid_CVPR_2021_supplemental.zip)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/tvwnXXH7xIw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1n41l7I2NxWseuruVQEU8he2XqzSXhu2f) | 01.01.2023 |
| First Order Motion Model for Image Animation | Transferring facial movements from video to image | [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/) | [![](https://img.shields.io/github/stars/AliaksandrSiarohin/first-order-model?style=social)](https://github.com/AliaksandrSiarohin/first-order-model) <ul><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html)</li><li>[project](https://aliaksandrsiarohin.github.io/first-order-model-website/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=u-0cQ-grXBQ)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb/) | 08.12.2022 |
| FILM | A frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion | <ul><li>[Fitsum Reda](https://fitsumreda.github.io/)</li> <li>[Janne Kontkanen](https://scholar.google.com/citations?user=MnXc4JQAAAAJ)</li> <li>[Eric Tabellion](http://www.tabellion.org/et/)</li><details><summary>others</summary><li>[Deqing Sun](https://deqings.github.io/)</li> <li>[Caroline Pantofaru](https://scholar.google.com/citations?user=vKAKE1gAAAAJ)</li> <li>[Brian Curless](https://homes.cs.washington.edu/~curless/)</li></ul></details> | [![](https://img.shields.io/github/stars/google-research/frame-interpolation?style=social)](https://github.com/google-research/frame-interpolation) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.04901)</li><li>[data](http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip), [data](https://vision.middlebury.edu/flow/data), [data](https://people.cs.umass.edu/~hzjiang/projects/superslomo/UCF101_results.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sniklaus/softmax-splatting/blob/master/benchmark.py)</li><li>[project](https://film-net.github.io/)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/tutorials/load_data/tfrecord), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/api_docs/python/tf/train/Example), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/saved_model)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/OAD-BieIjH4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1sK0uc-GJxmdnaxHhYqD2afRknakpdTNZ) | 26.11.2022 |
| Demucs | Hybrid Spectrogram and Waveform Source Separation | [Alexandre Défossez](https://ai.honu.io/) | [![](https://img.shields.io/github/stars/facebookresearch/demucs?style=social)](https://github.com/facebookresearch/demucs) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.03600), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.01733), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.05418), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.02410)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/adefossez/mdx21_demucs), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CarlGao4/Demucs-Gui), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/kuielab/mdx-net-submission), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/f90/Wave-U-Net)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH) | 21.11.2022 |
| ESM | Evolutionary Scale Modeling: Pretrained language models for proteins | <ul><li>[Zeming Lin](https://research.facebook.com/people/lin-zeming/)</li> <li>[Roshan Rao](https://rmrao.github.io/)</li> <li>[Brian Hie](https://brianhie.com/)</li><details><summary>others</summary><li>[Zhongkai Zhu](https://www.linkedin.com/in/zhongkai-zhu-03a27424)</li> <li>[Allan dos Santos Costa](https://scholar.google.com/citations?user=Zb4RsFsAAAAJ)</li> <li>[Maryam Fazel-Zarandi](https://www.maryamfazel.com/)</li> <li>[Tom Sercu](https://tom.sercu.me/)</li> <li>[Salvatore Candido](https://scholar.google.com/citations?user=BDgbhmEAAAAJ)</li> <li>[Alexander Rives](https://scholar.google.com/citations?user=vqb78-gAAAAJ)</li> <li>[Joshua Meier](https://scholar.google.com/citations?user=2M0OltAAAAAJ)</li> <li>[Robert Verkuil](https://dblp.org/pid/296/8930.html)</li> <li>[Jason Liu](https://www.linkedin.com/in/liujiayi/)</li> <li>[Chloe Hsu](https://chloe-hsu.com/)</li> <li>[Adam Lerer](https://scholar.google.com/citations?user=Ad6O4-0AAAAJ)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1101/622803)](https://doi.org/10.1101/622803) [![](https://img.shields.io/github/stars/facebookresearch/esm?style=social)](https://github.com/facebookresearch/esm) <ul><li>[ESM Atlas](https://esmatlas.com/)</li><li>[FSDP](https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html)</li><li>[ICML](https://proceedings.mlr.press/v139/rao21a.html)</li><li>[data](https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2018_03/uniref/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sokrypton/ColabFold)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/transformers/model_doc/esm)</li><li>[paper](https://doi.org/10.1101/2022.07.20.500902), [paper](https://doi.org/10.1101/2021.07.09.450648), [paper](https://doi.org/10.1101/2022.04.10.487779), [paper](https://doi.org/10.1101/2022.12.21.521521)</li><li>[pubmed](https://pubmed.ncbi.nlm.nih.gov/33876751/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/N-eisTvUYrk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/GHoE4VkDehY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb/) | 02.11.2022 |
| Musika | Music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU | <ul><li>[Marco Pasini](https://github.com/marcoppasini)</li> <li>[Jan Schlüter](https://www.ofai.at/~jan.schlueter/)</li></ul> | [![](https://img.shields.io/github/stars/marcoppasini/musika?style=social)](https://github.com/marcoppasini/musika) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2208.08706), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.08526)</li><li>[data](https://magenta.tensorflow.org/datasets/maestro)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/hendriks73/tempo-cnn), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CPJKU/madmom)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/marcop/musika)</li><li>[project](https://marcoppasini.github.io/musika)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/QBl8y2Z_i7Y), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/0l7OSM-bFvc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb) | 29.10.2022 |
| ICON | Given a set of images, method estimates a detailed 3D surface from each image and then combines these into an animatable avatar | <ul><li>[Yuliang Xiu](https://xiuyuliang.cn/)</li> <li>[Jinlong Yang](https://is.mpg.de/~jyang)</li> <li>[Dimitrios Tzionas](https://ps.is.mpg.de/~dtzionas)</li> <li>[Michael Black](https://ps.is.mpg.de/~black)</li></ul> | [![](https://img.shields.io/github/stars/yuliangxiu/icon?style=social)](https://github.com/yuliangxiu/icon) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.09127)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/KeypointNeRF), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/YadiraF/PIXIE), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/YuliangXiu/bvh-distance-queries), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Project-Splinter/MonoPortDataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ZhengZerong/PaMIR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Project-Splinter/MonoPort), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/shunsukesaito/SCANimate), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/aistplusplus_api)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Yuliang/ICON)</li><li>[project](https://icon.is.tue.mpg.de/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/hZd6AYin2DE)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1-AWeWhPvCTBX0KfMtgtMk10uPU05ihoA) | 25.10.2022 |
| MotionDiffuse | The first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods | <ul><li>[Mingyuan Zhang](https://mingyuan-zhang.github.io/)</li> <li>[Zhongang Cai](https://caizhongang.github.io/)</li> <li>[Liang Pan](https://github.com/paul007pl)</li><details><summary>others</summary><li>[Fangzhou Hong](https://hongfz16.github.io/)</li> <li>[Xinying Guo](https://gxyes.github.io/)</li> <li>[Lei Yang](https://scholar.google.com/citations?user=jZH2IPYAAAAJ)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/mingyuan-zhang/MotionDiffuse?style=social)](https://github.com/mingyuan-zhang/MotionDiffuse) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2208.15001)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/mingyuan/MotionDiffuse)</li><li>[project](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/U5PTnw490SA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1Dp6VsZp2ozKuu9ccMmsDjyij_vXfCYb3) | 13.10.2022 |
| VToonify | Leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details | <ul><li>[Shuai Yang](https://williamyang1991.github.io/)</li> <li>[Liming Jiang](https://liming-jiang.com/)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li></ul> | [![](https://img.shields.io/github/stars/williamyang1991/VToonify?style=social)](https://github.com/williamyang1991/VToonify) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.11224), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2001.02890)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zllrunning/face-parsing.PyTorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zhujiapeng/LowRankGAN)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/PKUWilliamYang/VToonify), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/PKUWilliamYang/VToonify/tree/main/models)</li><li>[project](https://www.mmlab-ntu.com/project/vtoonify/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/0_OmVhDgYuY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb/) | 07.10.2022 |
| PyMAF | Pyramidal Mesh Alignment Feedback loop in regression network for well-aligned body mesh recovery and extend it for the recovery of expressive full-body models | <ul><li>[Hongwen Zhang](https://hongwenzhang.github.io/)</li> <li>[Yating Tian](https://github.com/tinatiansjz)</li> <li>[Yuxiang Zhang](https://zhangyux15.github.io/)</li><details><summary>others</summary><li>[Mengcheng Li](https://github.com/Dw1010)</li> <li>[Liang An](https://anl13.github.io/)</li> <li>[Zhenan Sun](http://www.cbsr.ia.ac.cn/users/znsun/)</li> <li>[Yebin Liu](https://www.liuyebin.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/HongwenZhang/PyMAF?style=social)](https://github.com/HongwenZhang/PyMAF) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2207.06400), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.16507)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/eft), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HongwenZhang/DaNet-DensePose2SMPL), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/DensePose), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Microsoft/human-pose-estimation.pytorch)</li><li>[project](https://www.liuyebin.com/pymaf-x/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/yqEmznSKjYI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ylOB0wCeV34)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/11RXLsH9BdoSCwY6G-IX7KgqDxVoImu6K) | 06.10.2022 |
| AlphaTensor | Discovering faster matrix multiplication algorithms with reinforcement learning | <ul><li>[Alhussein Fawzi](http://www.alhusseinfawzi.info/)</li> <li>[Matej Balog](http://matejbalog.eu/)</li> <li>[Aja Huang](https://en.wikipedia.org/wiki/Aja_Huang)</li><details><summary>others</summary><li>[Thomas Hubert](https://scholar.google.com/citations?user=WXG0QfMAAAAJ)</li> <li>[Bernardino Romera-Paredes](https://sites.google.com/site/romeraparedes/)</li> <li>[Mohammadamin Barekatain](http://barekatain.me/)</li> <li>[Alexander Novikov](https://scholar.google.com/citations?user=jMUkLqwAAAAJ)</li> <li>[Francisco Ruiz](https://franrruiz.github.io/)</li> <li>[Julian Schrittwieser](https://www.furidamu.org/)</li> <li>[Grzegorz Swirszcz](https://sites.google.com/site/grzegorzswirszcz/home)</li> <li>[David Silver](https://www.davidsilver.uk/)</li> <li>[Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis)</li> <li>[Pushmeet Kohli](https://sites.google.com/site/pushmeet/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/alphatensor?style=social)](https://github.com/deepmind/alphatensor) <ul><li>[blog post](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)</li><li>[paper](https://www.nature.com/articles/s41586-022-05172-4)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/3N3Bl5AA5QU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/gpYnDls4PdQ), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/IYgZS2EvnLI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/8ILk4Wjo5rc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/alphatensor/blob/master/nonequivalence/inspect_factorizations_notebook.ipynb/) | 04.10.2022 |
| Swin2SR | Novel Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario | <ul><li>[Marcos Conde](https://mv-lab.github.io/)</li> <li>[Ui-Jin Choi](https://github.com/Choiuijin1125)</li> <li>[Maxime Burchi](https://scholar.google.com/citations?user=7S_l2eAAAAAJ)</li> <li>[Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/)</li></ul> | [![](https://img.shields.io/github/stars/mv-lab/swin2sr?style=social)](https://github.com/mv-lab/swin2sr) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.11345), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.10257), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2208.11184), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.09883)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/KAIR/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mv-lab/AISP), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/Swin-Transformer)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/jjourney1125/swin2sr)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/code/jesucristo/super-resolution-demo-swin2sr-official/), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/datasets/jesucristo/super-resolution-benchmarks), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/jinssaa/official-swin2sr-demo-results/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1paPrt62ydwLv2U2eZqfcFsePI4X4WRR1) | 03.10.2022 |
| Thin-Plate Spline Motion Model | End-to-end unsupervised motion transfer framework | <ul><li>[Jian Zhao](https://scholar.google.com/citations?user=OKm5CQYAAAAJ)</li> <li>[Hui Zhang](https://scholar.google.com/citations?user=w3mzCiwAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/yoyo-nb/Thin-Plate-Spline-Motion-Model?style=social)](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.14367)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/monkey-net), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/video-preprocessing), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/pose-evaluation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TalkUHulk/Image-Animation-Turbo-Boost)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)</li><li>[supp](https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_) | 30.09.2022 |
| Functa | From data to functa: Your data point is a function and you can treat it like one | <ul><li>[Emilien Dupont](https://emiliendupont.github.io/)</li> <li>[Hyunjik Kim](https://hyunjik11.github.io/)</li> <li>[Ali Eslami](http://arkitus.com/)</li><details><summary>others</summary><li>[Danilo Rezende](https://danilorezende.com/about/)</li> <li>[Dan Rosenbaum](https://danrsm.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/functa?style=social)](https://github.com/deepmind/functa) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.12204)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sxyu/pixel-nerf), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/jaxline)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/celeb_a_hq)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/functa/blob/main/modulation_visualization_colab.ipynb/) | 24.09.2022 |
| Whisper | Automatic speech recognition system trained on 680,000 hours of multilingual and multitask supervised data collected from the web | <ul><li>[Alec Radford](http://newmu.github.io/)</li> <li>[Jong Wook Kim](https://jongwook.kim/)</li> <li>[Tao Xu](https://github.com/bayesian)</li><details><summary>others</summary><li>[Greg Brockman](https://gregbrockman.com/)</li> <li>[Christine McLeavey](http://christinemcleavey.com/)</li> <li>[Ilya Sutskever](http://www.cs.toronto.edu/~ilya/)</li></ul></details> | [![](https://img.shields.io/github/stars/openai/whisper?style=social)](https://github.com/openai/whisper) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2212.04356)</li><li>[blog post](https://openai.com/research/whisper)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kkroening/ffmpeg-python)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/OCBZtgQGt1I), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/8SQV-B83tPU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nE5iVtwKerA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb/) | 21.09.2022 |
| DeOldify (video) | Colorize your own videos! | [Jason Antic](https://github.com/jantic) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42)</li><li>[model](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/Nickelodeons/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/silentmoviegifs/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=l3UXXid04Ys), [<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=EXn-n2iqEjI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb/) | 19.09.2022 |
| DeOldify (photo) | Colorize your own photos! | <ul><li>[Jason Antic](https://github.com/jantic)</li> <li>[Matt Robinson](https://github.com/mc-robinson)</li> <li>[María Benavente](https://github.com/mariabg)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[model](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/TheWayWeWere/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb/) | 19.09.2022 |
| Real-ESRGAN | Extend the powerful ESRGAN to a practical restoration application, which is trained with pure synthetic data | <ul><li>[Xintao Wang](https://xinntao.github.io/)</li> <li>[Liangbin Xie](https://liangbinxie.github.io/)</li> <li>[Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ)</li> <li>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/xinntao/Real-ESRGAN?style=social)](https://github.com/xinntao/Real-ESRGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.10833)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/ESRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/facexlib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyView), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Tencent/ncnn), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nihui/waifu2x-ncnn-vulkan)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo) | 18.09.2022 |
| IDE-3D | Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis | <ul><li>[Jingxiang Sun](https://mrtornado24.github.io/)</li> <li>[Xuan Wang](https://xuanwangvc.github.io/)</li> <li>[Yichun Shi](https://seasonsh.github.io/)</li><details><summary>others</summary><li>[Lizhen Wang](https://lizhenwangt.github.io/)</li> <li>[Jue Wang](https://juewang725.github.io/)</li> <li>[Yebin Liu](http://www.liuyebin.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/MrTornado24/IDE-3D?style=social)](https://github.com/MrTornado24/IDE-3D) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://arxiv.org/abs/2205.15517), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/eg3d), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan3)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Kj5XY_J2Alk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/MrTornado24/IDE-3D/blob/main/inversion/notebooks/inference_playground.ipynb/) | 08.09.2022 |
| Decision Transformers | An architecture that casts the problem of RL as conditional sequence modeling | <ul><li>[Lili Chen](http://www.lilichen.me/)</li> <li>[Kevin Lu](https://kzl.github.io/)</li> <li>[Aravind Rajeswaran](https://aravindr93.github.io/)</li><details><summary>others</summary><li>[Kimin Lee](https://sites.google.com/view/kiminlee)</li> <li>[Aditya Grover](https://aditya-grover.github.io/)</li> <li>[Michael Laskin](https://www.mishalaskin.com/)</li> <li>[Pieter Abbeel](http://people.eecs.berkeley.edu/~pabbeel/)</li> <li>[Aravind Srinivas](https://github.com/aravindsrinivas)</li> <li>[Igor Mordatch](https://scholar.google.com/citations?user=Vzr1RukAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/kzl/decision-transformer?style=social)](https://github.com/kzl/decision-transformer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.01345)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/models?other=gym-continous-control), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/transformers/model_doc/decision_transformer)</li><li>[project](https://sites.google.com/berkeley.edu/decision-transformer)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Autoregressive_model)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/k08N5a0gG0A), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-buULmf7dec), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/83QN9S-0I84), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/w4Bw8WYL8Ps)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1K3UuajwoPY1MzRKNkONNRS3gS5DxZ-qF) | 06.09.2022 |
| Dream Fields | Zero-Shot Text-Guided Object Generation | <ul><li>[Ajay Jain](https://ajayj.com/)</li> <li>[Ben Mildenhall](https://bmild.github.io/)</li> <li>[Jon Barron](https://jonbarron.info/)</li><details><summary>others</summary><li>[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)</li> <li>[Ben Poole](https://cs.stanford.edu/~poole/)</li></ul></details> | [![](https://img.shields.io/github/stars/google-research/google-research?style=social)](https://github.com/google-research/google-research/tree/master/dreamfields) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.01455), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00677), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.13415)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ajayjain/DietNeRF), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/mipnerf)</li><li>[project](https://ajayj.com/dreamfields)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/1Fke6w46tv4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1TjCWS2_Q0HJKdi9wA2OSY7avmFUQYGje) | 05.09.2022 |
| GANgealing | Framework for learning discriminative models and their GAN-generated training data jointly end-to-end | <ul><li>[William Peebles](https://www.wpeebles.com/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li> <li>[Richard Zhang](https://richzhang.github.io/)</li><details><summary>others</summary><li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li> <li>[Alexei Efros](https://people.eecs.berkeley.edu/~efros/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li></ul></details> | [![](https://img.shields.io/github/stars/wpeebles/gangealing?style=social)](https://github.com/wpeebles/gangealing) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.05143)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/nileshkulkarni/acsm), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://jitengmu.github.io/CoordGAN/)</li><li>[project](https://www.wpeebles.com/gangealing)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Qa1ASS_NuzE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/qtOkktTNs-k)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1JkUjhTjR8MyLxwarJjqnh836BICfocTu) | 01.09.2022 |
| textual-inversion | An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion | <ul><li>[Rinon Gal](https://rinongal.github.io/)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yuval Atzmon](https://research.nvidia.com/person/yuval-atzmon)</li><details><summary>others</summary><li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)</li> <li>[Gal Chechik](https://research.nvidia.com/person/gal-chechik)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/rinongal/textual_inversion?style=social)](https://github.com/rinongal/textual_inversion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2208.01618)</li><li>[project](https://textual-inversion.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/f3oXa7_SYek), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/opD_H9bED9Y)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/rinongal/textual_inversion/blob/master/scripts/latent_imagenet_diffusion.ipynb/) | 21.08.2022 |
| StyleGAN-Human | A Data-Centric Odyssey of Human Generation | <ul><li>[Jianglin Fu](https://github.com/arleneF)</li> <li>[Shikai Li](https://github.com/leeskyed)</li> <li>[Yuming Jiang](https://yumingj.github.io/)</li><details><summary>others</summary><li>[Kwan-Yee Lin](https://kwanyeelin.github.io/)</li> <li>[Chen Qian](https://scholar.google.com/citations?user=AerkT0YAAAAJ)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li> <li>[Wayne Wu](https://wywu.github.io/)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1007/978-3-031-19787-1_1)](https://doi.org/10.1007/978-3-031-19787-1_1) [![](https://img.shields.io/github/stars/stylegan-human/stylegan-human?style=social)](https://github.com/stylegan-human/stylegan-human) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.11823)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan3)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/dataset/market-1501)</li><li>[project](https://stylegan-human.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nIrb9hwsdcI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/86b49sCz0Gg), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/g3nmM6MdxwY), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/p2uwqh_SFL8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1sgxoDM55iM07FS54vz9ALg1XckiYA2On) | 19.08.2022 |
| Make-A-Scene | Scene-Based Text-to-Image Generation with Human Priors | <ul><li>[Oran Gafni](https://github.com/ogafni)</li> <li>[Adam Polyak](https://scholar.google.com/citations?user=CP62OTMAAAAJ)</li> <li>[Oron Ashual](https://scholar.google.com/citations?user=CUA9JCkAAAAJ)</li><details><summary>others</summary><li>[Shelly Sheynin](https://github.com/shellysheynin)</li> <li>[Devi Parikh](https://faculty.cc.gatech.edu/~parikh/)</li> <li>[Yaniv Taigman](https://ytaigman.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/CasualGANPapers/Make-A-Scene?style=social)](https://github.com/CasualGANPapers/Make-A-Scene) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.13131)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ZM06MjPdoxw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1SPyQ-epTsAOAu8BEohUokN4-b5RM_TnE) | 12.08.2022 |
| StyleGAN-NADA | Zero-Shot non-adversarial domain adaptation of pre-trained generators | <ul><li>[Rinon Gal](https://rinongal.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Haggai Maron](https://haggaim.github.io/)</li><details><summary>others</summary><li>[Gal Chechik](https://research.nvidia.com/person/gal-chechik)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/rinongal/StyleGAN-nada?style=social)](https://github.com/rinongal/StyleGAN-nada) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17249), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.02699)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada)</li><li>[project](https://stylegan-nada.github.io/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb/) | 09.08.2022 |
| YOLOv7 | Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors | <ul><li>[Chien-Yao Wang](https://scholar.google.com/citations?user=DkQh4M4AAAAJ)</li> <li>[Alexey Bochkovskiy](http://www.alexeyab.com/)</li> <li>[Mark Liao](https://www.iis.sinica.edu.tw/pages/liao/)</li></ul> | [![](https://img.shields.io/github/stars/WongKinYiu/yolov7?style=social)](https://github.com/WongKinYiu/yolov7) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2207.02696)</li><li>[data](http://images.cocodataset.org/annotations/annotations_trainval2017.zip), [data](http://images.cocodataset.org/zips/train2017.zip), [data](http://images.cocodataset.org/zips/val2017.zip), [data](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/WongKinYiu/yolor), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/WongKinYiu/PyTorch_YOLOv4), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/WongKinYiu/ScaledYOLOv4), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Megvii-BaseDetection/YOLOX), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/DingXiaoH/RepVGG), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/JUGGHM/OREPA_CVPR2022), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PL_Nji0JOuXg2QMohGK7wfzgJ-MavzXRHW), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-QWxJ0j9EY8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/WongKinYiu/yolov7/blob/main/tools/compare_YOLOv7_vs_YOLOv5m6_half.ipynb/) | 09.08.2022 |
| Anycost GAN | Interactive natural image editing | <ul><li>[Ji Lin](http://linji.me/)</li> <li>[Richard Zhang](https://richzhang.github.io/)</li> <li>[Frieder Ganz](https://scholar.google.com/citations?user=u9ySZkUAAAAJ)</li><details><summary>others</summary><li>[Song Han](https://songhan.mit.edu/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li></ul></details> | [![](https://img.shields.io/github/stars/mit-han-lab/anycost-gan?style=social)](https://github.com/mit-han-lab/anycost-gan) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.03243)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/switchablenorms/CelebAMask-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun)</li><li>[project](https://hanlab.mit.edu/projects/anycost-gan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_yEziPl9AkM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb/) | 20.07.2022 |
| GFPGAN | Towards Real-World Blind Face Restoration with Generative Facial Prior | <ul><li>[Xintao Wang](https://xinntao.github.io/)</li> <li>[Yu Li](https://yu-li.github.io/)</li> <li>[Honglun Zhang](https://scholar.google.com/citations?user=KjQLROoAAAAJ)</li> <li>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social)](https://github.com/TencentARC/GFPGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.04061)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/facexlib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyView), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset)</li><li>[project](https://xinntao.github.io/projects/gfpgan)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) | 13.07.2022 |
| EPro-PnP | Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation | <ul><li>[Hansheng Chen](https://lakonik.github.io/)</li> <li>[Pichao Wang](https://wangpichao.github.io/)</li> <li>[Fan Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ)</li><details><summary>others</summary><li>[Wei Tian](https://scholar.google.com/citations?user=aYKQn88AAAAJ)</li> <li>[Lu Xiong](https://ieeexplore.ieee.org/author/37401835800)</li> <li>[Hao Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/tjiiv-cprg/EPro-PnP?style=social)](https://github.com/tjiiv-cprg/EPro-PnP) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.13254)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/megvii-research/petr), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HuangJunJie2017/BEVDet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fudan-zvg/PolarFormer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zhiqi-li/BEVFormer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/open-mmlab/mmdetection3d)</li><li>[nuScenes](https://www.nuscenes.org/object-detection?externalData=no&mapData=no&modalities=Camera)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/TonBodQ6EUU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tjiiv-cprg/EPro-PnP/blob/main/demo/fit_identity.ipynb/) | 12.07.2022 |
| VQ-Diffusion | Based on a VQ-VAE whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model | <ul><li>[Shuyang Gu](https://github.com/cientgu)</li> <li>[Dong Chen](http://www.dongchen.pro/)</li> <li>[Jianmin Bao](https://jianminbao.github.io/)</li><details><summary>others</summary><li>[Fang Wen](https://www.microsoft.com/en-us/research/people/fangwen/)</li> <li>[Bo Zhang](https://bo-zhang.me/)</li> <li>[Dongdong Chen](http://www.dongdongchen.bid/)</li> <li>[Lu Yuan](https://scholar.google.com/citations?&user=k9TsUVsAAAAJ)</li> <li>[Baining Guo](https://scholar.google.com/citations?user=h4kYmRYAAAAJ)</li> <li>[Shuyang Gu](https://github.com/cientgu)</li> <li>[Zhicong Tang](https://github.com/zzctan)</li></ul></details> | [![](https://img.shields.io/github/stars/microsoft/VQ-Diffusion?style=social)](https://github.com/microsoft/VQ-Diffusion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.14822), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.16007)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ehoogeboom/multinomial_diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/improved-diffusion)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1Ws0_wK2cnsWEnfB7HtmPT4bjCPElb40C) | 30.06.2022 |
| OPT | Open Pre-trained Transformers is a family of NLP models trained on billions of tokens of text obtained from the internet | <ul><li>[Susan Zhang](https://github.com/suchenzang)</li> <li>[Stephen Roller](https://stephenroller.com/)</li> <li>[Naman Goyal](https://github.com/ngoyal2707)</li><details><summary>others</summary><li>[Mikel Artetxe](https://github.com/artetxem)</li> <li>[Moya Chen](https://moyachen.com/)</li> <li>[Christopher Dewan](https://github.com/m3rlin45)</li> <li>[Mona Diab](https://scholar.google.com/citations?user=-y6SIhQAAAAJ)</li> <li>[Xi Victoria Lin](http://victorialin.net/)</li> <li>[Todor Mihaylov](https://github.com/tbmihailov)</li> <li>[Myle Ott](https://myleott.com/)</li> <li>[Sam Shleifer](https://github.com/sshleifer)</li> <li>[Kurt Shuster](https://github.com/klshuster)</li> <li>[Daniel Simig](https://scholar.google.com/citations?user=TtWU9fsAAAAJ)</li> <li>[Punit Singh Koura](https://github.com/punitkoura)</li> <li>[Anjali Sridhar](https://www.linkedin.com/in/anjalisridhar/)</li> <li>[Tianlu Wang](https://tianlu-wang.github.io/)</li> <li>[Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/lsz/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/metaseq?style=social)](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.01068), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.02243), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.10350), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.11990)</li><li>[blog post](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/Megatron-LM)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Ejg0OunCi9U)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7) | 29.06.2022 |
| Customizing a Transformer Encoder | We will learn how to customize the encoder to employ new network architectures | [Chen Chen](https://github.com/chenGitHuber) | [![](https://img.shields.io/github/stars/tensorflow/models?style=social)](https://github.com/tensorflow/models/tree/master/official/nlp/modeling) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/encoder_scaffold.py)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/official/colab/nlp/customize_encoder.ipynb/) | 22.06.2022 |
| MTTR | End-to-End Referring Video Object Segmentation with Multimodal Transformers | <ul><li>[Adam Botach](https://www.linkedin.com/in/adam-botach)</li> <li>[Evgenii Zheltonozhskii](https://evgeniizh.com/)</li> <li>[Chaim Baskin](https://github.com/chaimbaskin)</li></ul> | [![](https://img.shields.io/github/stars/mttr2021/MTTR?style=social)](https://github.com/mttr2021/MTTR) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.14821), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.11692), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.13230)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Video-Swin-Transformer)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/MTTR/MTTR-Referring-Video-Object-Segmentation)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YqlhXgq6hcs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/12p0jpSx3pJNfZk-y_L44yeHZlhsKVra-) | 20.06.2022 |
| SwinIR | Image Restoration Using Swin Transformer | <ul><li>[Jingyun Liang](https://jingyunliang.github.io/)</li> <li>[Jiezhang Cao](https://github.com/caojiezhang)</li> <li>[Guolei Sun](https://github.com/GuoleiSun)</li><details><summary>others</summary><li>[Kai Zhang](https://cszn.github.io/)</li> <li>[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ)</li> <li>[Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/)</li></ul></details> | [![](https://img.shields.io/github/stars/JingyunLiang/SwinIR?style=social)](https://github.com/JingyunLiang/SwinIR) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.10257), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.10833)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/BSRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/Swin-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/KAIR)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb/) | 17.06.2022 |
| VRT | A Video Restoration Transformer | <ul><li>[Jingyun Liang](https://jingyunliang.github.io/)</li> <li>[Jiezhang Cao](https://github.com/caojiezhang)</li> <li>[Yuchen Fan](https://ychfan.github.io/)</li><details><summary>others</summary><li>[Kai Zhang](https://cszn.github.io/)</li> <li>[Yawei Li](https://ofsoundof.github.io/)</li> <li>[Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/)</li> <li>[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/JingyunLiang/VRT?style=social)](https://github.com/JingyunLiang/VRT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.12288)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cszn/KAIR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Video-Swin-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/open-mmlab/mmediting)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/gist/JingyunLiang/deb335792768ad9eb73854a8efca4fe0/vrt-demo-on-video-restoration.ipynb/) | 15.06.2022 |
| Omnivore | A single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters | <ul><li>[Rohit Girdhar](http://rohitgirdhar.github.io/)</li> <li>[Mannat Singh](https://scholar.google.com/citations?user=QOO8OCcAAAAJ)</li> <li>[Nikhila Ravi](https://nikhilaravi.com/)</li><details><summary>others</summary><li>[Laurens Maaten](https://lvdmaaten.github.io/)</li> <li>[Armand Joulin](https://ai.facebook.com/people/armand-joulin/)</li> <li>[Ishan Misra](https://imisra.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/omnivore?style=social)](https://github.com/facebookresearch/omnivore) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.08377), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2206.08356)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/omnivore)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/dataset/epic-kitchens-100)</li><li>[project](https://facebookresearch.github.io/omnivore/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/omnivore/blob/main/inference_tutorial.ipynb/) | 14.06.2022 |
| Detic | Detecting Twenty-thousand Classes using Image-level Supervision | <ul><li>[Xingyi Zhou](https://www.cs.utexas.edu/~zhouxy/)</li> <li>[Rohit Girdhar](https://rohitgirdhar.github.io/)</li> <li>[Armand Joulin](https://ai.facebook.com/people/armand-joulin/)</li><details><summary>others</summary><li>[Philipp Krähenbühl](https://github.com/philkr)</li> <li>[Ishan Misra](https://imisra.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/Detic?style=social)](https://github.com/facebookresearch/Detic) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.02605)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lvis-dataset/lvis-api)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1QtTW9-ukX2HKZGvt0QvVGqjuqEykoZKI) | 07.06.2022 |
| AMARETTO | Multiscale and multimodal inference of regulatory networks to identify cell circuits and their drivers shared and distinct within and across biological systems of human disease | <ul><li>[Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)</li> <li>[Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)</li> <li>[Mohsen Nabian](https://github.com/monabiyan)</li><details><summary>others</summary><li>[Jayendra Shinde](https://jayendrashinde91.github.io/)</li> <li>[Celine Everaert](http://www.crig.ugent.be/en/node/510)</li> <li>[Thorin Tabor](http://thorin.tabcreations.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/gevaertlab/AMARETTO?style=social)](https://github.com/gevaertlab/AMARETTO) <ul><li>[bioconductor](https://bioconductor.org/packages/release/bioc/html/AMARETTO.html)</li><li>[project](http://portals.broadinstitute.org/pochetlab/amaretto.html)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1JfnRoNgTVX_7VEGAAmjGjwP_yX2tdDxs) | 01.06.2022 |
| T0 | Multitask Prompted Training Enables Zero-Shot Task Generalization | <ul><li>[Victor Sanh](https://github.com/VictorSanh)</li> <li>[Albert Webson](https://representation.ai/)</li> <li>[Colin Raffel](https://colinraffel.com//)</li><details><summary>others</summary><li>[Stephen Bach](http://cs.brown.edu/people/sbach/)</li> <li>[Lintang Sutawika](https://github.com/lintangsutawika)</li> <li>[Zaid Alyafeai](https://github.com/zaidalyafeai)</li> <li>[Antoine Chaffin](https://antoine.chaffin.fr/)</li> <li>[Arnaud Stiegler](https://github.com/arnaudstiegler)</li> <li>[Teven Le Scao](https://scholar.google.com/citations?user=ik0_vxsAAAAJ)</li> <li>[Arun Raja](https://www.arunraja.dev/)</li> <li>[Manan Dey](https://github.com/manandey)</li> <li>[M Saiful Bari](https://sbmaruf.github.io/)</li> <li>[Canwen Xu](https://www.canwenxu.net/)</li> <li>[Urmish Thakker](https://github.com/Urmish)</li> <li>[Shanya Sharma](https://shanyas10.github.io/)</li> <li>[Eliza Szczechla](https://elsanns.github.io/)</li> <li>[Taewoon Kim](https://tae898.github.io/)</li> <li>[Gunjan Chhablani](https://gchhablani.github.io/)</li> <li>[Nihal Nayak](https://nihalnayak.github.io/)</li> <li>[Debajyoti Datta](http://debajyotidatta.github.io/)</li> <li>[Jonathan Chang](https://github.com/cccntu/)</li> <li>[Mike Tian-Jian Jiang](https://github.com/tianjianjiang)</li> <li>[Matteo Manica](https://github.com/drugilsberg)</li> <li>[Sheng Shen](https://sincerass.github.io/)</li> <li>[Zheng Xin Yong](https://yongzx.github.io/)</li> <li>[Harshit Pandey](https://scholar.google.com/citations?user=BPIs78gAAAAJ)</li> <li>[Rachel Bawden](https://rbawden.github.io/)</li> <li>[Trishala Neeraj](https://github.com/trishalaneeraj)</li> <li>[Jos Rozen](https://scholar.google.com/citations?user=OxEDKogAAAAJ)</li> <li>[Abheesht Sharma](https://github.com/abheesht-sharma)</li> <li>[Andrea Santilli](https://teelinsan.github.io/)</li> <li>[Thibault Fevry](http://thibaultfevry.com/)</li> <li>[Jason Alan Fries](https://web.stanford.edu/~jfries/)</li> <li>[Ryan Teehan](https://github.com/rteehas)</li> <li>[Stella Biderman](https://www.stellabiderman.com/)</li> <li>[Leo Gao](https://github.com/leogao2)</li> <li>[Tali Bers](https://github.com/tbers-coursera)</li> <li>[Thomas Wolf](https://thomwolf.io/)</li> <li>[Alexander M. Rush](https://scholar.google.com/citations?user=LIjnUGgAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/bigscience-workshop/promptsource?style=social)](https://github.com/bigscience-workshop/promptsource) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2110.08207)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/iJ0IVZgGjTM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YToXXfrIu6w)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_) | 29.05.2022 |
| AvatarCLIP | A zero-shot text-driven framework for 3D avatar generation and animation | <ul><li>[Fangzhou Hong](https://hongfz16.github.io/)</li> <li>[Mingyuan Zhang](https://scholar.google.com/citations?user=2QLD4fAAAAAJ)</li> <li>[Liang Pan](https://scholar.google.com/citations?user=lSDISOcAAAAJ)</li><details><summary>others</summary><li>[Zhongang Cai](https://caizhongang.github.io/)</li> <li>[Lei Yang](https://scholar.google.com/citations?user=jZH2IPYAAAAJ)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/hongfz16/AvatarCLIP?style=social)](https://github.com/hongfz16/AvatarCLIP) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.08535), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.01455), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.03221), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.05139), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.13333)</li><li>[data](https://www.di.ens.fr/willow/research/surreal/data/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/daniilidis-group/neural_renderer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/GuyTevet/MotionCLIP), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Totoro97/NeuS), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/vchoutas/smplx), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nghorbani/human_body_prior)</li><li>[project](https://hongfz16.github.io/projects/AvatarCLIP.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-l2ZMeoASGY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1dfaecX7xF3nP6fyXc8XBljV5QY1lc1TR) | 15.05.2022 |
| Text2Mesh | Text-Driven Neural Stylization for Meshes | <ul><li>[Oscar Michel](https://ojmichel.github.io/)</li> <li>[Roi Bar-On](https://github.com/roibaron)</li> <li>[Richard Liu](https://github.com/factoryofthesun)</li><details><summary>others</summary><li>[Sagie Benaim](https://sagiebenaim.github.io/)</li> <li>[Rana Hanocka](http://people.cs.uchicago.edu/~ranahanocka/)</li></ul></details> | [![](https://img.shields.io/github/stars/threedle/text2mesh?style=social)](https://github.com/threedle/text2mesh) <ul><li>[CLIP](https://openai.com/blog/clip/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.03221)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/code/neverix/text2mesh/notebook)</li><li>[project](https://threedle.github.io/text2mesh/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/threedle/text2mesh/blob/master/colab_demo.ipynb/) | 14.05.2022 |
| T5 | Text-To-Text Transfer Transformer | <ul><li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Noam Shazeer](https://scholar.google.com/citations?user=wsGvgA8AAAAJ)</li> <li>[Adam Roberts](https://github.com/adarob)</li><details><summary>others</summary><li>[Katherine Lee](https://github.com/katelee168)</li> <li>[Sharan Narang](https://github.com/sharannarang)</li> <li>[Michael Matena](https://scholar.google.com/citations?user=rN_9vroAAAAJ)</li> <li>[Yanqi Zhou](https://zhouyanqi.github.io)</li> <li>[Wei Li](https://research.google/people/106528/)</li> <li>[Peter J. Liu](https://scholar.google.com/citations?user=1EPxhywAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/google-research/text-to-text-transfer-transformer?style=social)](https://github.com/google-research/text-to-text-transfer-transformer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.10683)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb/) | 11.05.2022 |
| XLS-R | Self-supervised Cross-lingual Speech Representation Learning at Scale | <ul><li>[Arun Babu](https://github.com/arbabu123)</li> <li>[Changhan Wang](https://www.changhan.me/)</li> <li>[Andros Tjandra](https://github.com/androstj)</li><details><summary>others</summary><li>[Kushal Lakhotia](https://about.me/hikushalhere)</li> <li>[Qiantong Xu](https://github.com/xuqiantong)</li> <li>[Naman Goyal](https://github.com/ngoyal2707)</li> <li>[Kritika Singh](https://scholar.google.com/citations?user=Ltk3SykAAAAJ)</li> <li>[Patrick von Platen](https://github.com/patrickvonplaten)</li> <li>[Yatharth Saraf](https://scholar.google.com/citations?user=KJTtNJwAAAAJ)</li> <li>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)</li> <li>[Alexei Baevski](https://github.com/alexeib)</li> <li>[Alexis Conneau](https://github.com/aconneau)</li> <li>[Michael Auli](https://github.com/michaelauli)</li></ul></details> | [![](https://img.shields.io/github/stars/pytorch/fairseq?style=social)](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/xlsr/README.md) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.09296)</li><li>[blog post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fairscale)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb/) | 10.05.2022 |
| DiffCSE | Unsupervised contrastive learning framework for learning sentence embeddings | <ul><li>[Yung-Sung Chuang](https://people.csail.mit.edu/yungsung/)</li> <li>[Rumen Dangovski](http://super-ms.mit.edu/rumen.html)</li> <li>[Hongyin Luo](https://luohongyin.github.io/)</li><details><summary>others</summary><li>[Yang Zhang](https://mitibmwatsonailab.mit.edu/people/yang-zhang/)</li> <li>[Shiyu Chang](https://code-terminator.github.io/)</li> <li>[Marin Soljačić](http://www.mit.edu/~soljacic/marin.html)</li> <li>[Shang-Wen Li](https://swdanielli.github.io/)</li> <li>[Scott Wen-tau Yih](https://scottyih.org/)</li> <li>[Yoon Kim](https://people.csail.mit.edu/yoonkim/)</li> <li>[James Glass](http://groups.csail.mit.edu/sls/people/glass.shtml)</li></ul></details> | [![](https://img.shields.io/github/stars/voidism/diffcse?style=social)](https://github.com/voidism/diffcse) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.10298), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.08821), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.00899)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/princeton-nlp/SimCSE)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/voidism)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/YungSungChuang/status/1517518077902000129)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/voidism/DiffCSE/blob/master/diffcse_evaluation.ipynb/) | 24.04.2022 |
| ViDT+ | An Extendable, Efficient and Effective Transformer-based Object Detector | <ul><li>[Hwanjun Song](https://songhwanjun.github.io/)</li> <li>[Deqing Sun](https://deqings.github.io/)</li> <li>[Sanghyuk Chun](https://sanghyukchun.github.io/home/)</li><details><summary>others</summary><li>[Varun Jampani](https://varunjampani.github.io/)</li> <li>[Dongyoon Han](https://sites.google.com/site/dyhan0920/)</li> <li>[Byeongho Heo](https://sites.google.com/view/byeongho-heo/home)</li> <li>[Wonjae Kim](https://wonjae.kim/)</li> <li>[Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)</li></ul></details> | [![](https://img.shields.io/github/stars/naver-ai/vidt?style=social)](https://github.com/naver-ai/vidt/tree/vidt-plus) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.07962), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2110.03921)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/fundamentalvision/Deformable-DETR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EherSenaw/ViDT_colab)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/EherSenaw/ViDT_colab/blob/main/vidt_colab.ipynb/) | 20.04.2022 |
| NAFNet | Nonlinear Activation Free Network for Image Restoration | <ul><li>[Liangyu Chen](https://github.com/mayorx)</li> <li>[Xiaojie Chu](https://github.com/chuxiaojie)</li> <li>[Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ)</li> <li>[Jian Sun](http://www.jiansun.org/)</li></ul> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1007/978-3-031-20071-7_2)](https://doi.org/10.1007/978-3-031-20071-7_2) [![](https://img.shields.io/github/stars/megvii-research/NAFNet?style=social)](https://github.com/megvii-research/NAFNet) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.04676), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.08714)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/sota/image-deblurring-on-gopro?p=simple-baselines-for-image-restoration), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/sota/image-denoising-on-sidd?p=simple-baselines-for-image-restoration)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1dkO5AyktmBoWwxBwoKFUurIDn0m4qDXT) | 15.04.2022 |
| Panini-Net | GAN Prior based Degradation-Aware Feature Interpolation for Face Restoration | <ul><li>[Yinhuai Wang](https://github.com/wyhuai)</li> <li>[Yujie Hu](https://villa.jianzhang.tech/people/yujie-hu/)</li> <li>[Jian Zhang](http://jianzhang.tech/)</li></ul> | [![](https://img.shields.io/github/stars/jianzhangcs/panini?style=social)](https://github.com/jianzhangcs/panini) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.08444)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tkarras/progressive_growing_of_gans)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/GeeveGeorge/Panini-Net-Colab/blob/main/PaniniNet_Working.ipynb/) | 13.04.2022 |
| Deep Painterly Harmonization | Algorithm produces significantly better results than photo compositing or global stylization techniques and that it enables creative painterly edits that would be otherwise difficult to achieve | <ul><li>[Fujun Luan](https://luanfujun.github.io/)</li> <li>[Sylvain Paris](http://people.csail.mit.edu/sparis/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Kavita Bala](https://www.cs.cornell.edu/~kb/)</li></ul> | [![](https://img.shields.io/github/stars/luanfujun/deep-painterly-harmonization?style=social)](https://github.com/luanfujun/deep-painterly-harmonization) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1804.03189), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.08893)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jcjohnson/neural-style), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/torch/torch7), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/szagoruyko/loadcaffe)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/gist/eyaler/5303782669fb43510d398bd346c6e3e6/deep-painterly-harmonization.ipynb/) | 07.04.2022 |
| E2FGVI | An End-to-End framework for Flow-Guided Video Inpainting through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules | <ul><li>[Zhen Li](https://paper99.github.io/)</li> <li>[Cheng-Ze Lu](https://github.com/LGYoung)</li> <li>[Jianhua Qin](https://scholar.google.com/citations?&user=TAr7TU4AAAAJ)</li><details><summary>others</summary><li>[Chun-Le Guo](https://scholar.google.com/citations?user=RZLYwR0AAAAJ)</li> <li>[Ming-Ming Cheng](https://mmcheng.net/)</li></ul></details> | [![](https://img.shields.io/github/stars/MCG-NKU/E2FGVI?style=social)](https://github.com/MCG-NKU/E2FGVI) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.02663)</li><li>[data](https://competitions.codalab.org/competitions/19544#participate-get-data), [data](https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/researchmm/STTN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/Focal-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ruiliu-ai/FuseFormer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/phoenix104104/fast_blind_video_consistency#evaluation)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/mlearning-ai/end-to-end-framework-for-flow-guided-video-inpainting-c5e2d8b61d20)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/N--qC3T2wc4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/3eH3Fm6gOFk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/12rwY2gtG8jVWlNx9pjmmM8uGmh5ue18G) | 06.04.2022 |
| LDM | High-Resolution Image Synthesis with Latent Diffusion Models | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Andreas Blattmann](https://github.com/ablattmann)</li> <li>[Dominik Lorenz](https://github.com/qp-qp)</li><details><summary>others</summary><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li></ul></details> | [![](https://img.shields.io/github/stars/CompVis/latent-diffusion?style=social)](https://github.com/CompVis/latent-diffusion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10752), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.09778), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.02114)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/guided-diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/denoising-diffusion-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/x-transformers)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/multimodalart/latentdiffusion)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CompVis/latent-diffusion/blob/master/scripts/latent_imagenet_diffusion.ipynb/) | 04.04.2022 |
| GP-UNIT | Novel framework, Generative Prior-guided UNsupervised Image-to-image Translation, to improve the overall quality and applicability of the translation algorithm | <ul><li>[Shuai Yang](https://williamyang1991.github.io/)</li> <li>[Liming Jiang](https://liming-jiang.com/)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li></ul> | [![](https://img.shields.io/github/stars/williamyang1991/GP-UNIT?style=social)](https://github.com/williamyang1991/GP-UNIT) <ul><li>[ImageNet](https://image-net.org/download.php)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.03641)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/clovaai/stargan-v2#datasets-and-pre-trained-networks), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/switchablenorms/CelebAMask-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/metfaces-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/SPADE), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nvlabs/imaginaire)</li><li>[project](https://www.mmlab-ntu.com/project/gpunit/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/dDApWs_oDrM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/williamyang1991/GP-UNIT/blob/main/notebooks/inference_playground.ipynb/) | 02.04.2022 |
| DualStyleGAN | More challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain | <ul><li>[Shuai Yang](https://williamyang1991.github.io/)</li> <li>[Liming Jiang](https://liming-jiang.com/)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li></ul> | [![](https://img.shields.io/github/stars/williamyang1991/DualStyleGAN?style=social)](https://github.com/williamyang1991/DualStyleGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.13248)</li><li>[data](https://cs.nju.edu.cn/rl/WebCaricature.htm), [data](https://www.gwern.net/Crops#danbooru2019-portraits)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lowfuel/progrock-stable), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Gradio-Blocks/DualStyleGAN), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/hysts/DualStyleGAN)</li><li>[project](https://www.mmlab-ntu.com/project/dualstylegan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/scZTu77jixI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb/) | 24.03.2022 |
| CLIPasso | Semantically-Aware Object Sketching | <ul><li>[Yael Vinker](https://yaelvi116.wixsite.com/mysite)</li> <li>[Ehsan Pajouheshgar](https://pajouheshgar.github.io/)</li> <li>[Jessica Y. Bo](https://jessica-bo.github.io/)</li><details><summary>others</summary><li>[Roman Bachmann](https://roman-bachmann.github.io/)</li> <li>[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li> <li>[Amir Zamir](https://vilab.epfl.ch/zamir/)</li> <li>[Ariel Shamir](https://faculty.runi.ac.il/arik/site/index.asp)</li></ul></details> | [![](https://img.shields.io/github/stars/yael-vinker/CLIPasso?style=social)](https://github.com/yael-vinker/CLIPasso) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.05822), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.14843)</li><li>[demo](https://replicate.com/yael-vinker/clipasso)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/BachiLi/diffvg)</li><li>[project](https://clipasso.github.io/clipasso/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yael-vinker/CLIPasso/blob/main/CLIPasso.ipynb/) | 21.03.2022 |
| StyleSDF | A high resolution, 3D-consistent image and shape generation technique | <ul><li>[Roy Or-El](https://homes.cs.washington.edu/~royorel/)</li> <li>[Xuan Luo](https://roxanneluo.github.io/)</li> <li>[Mengyi Shan](https://shanmy.github.io/)</li><details><summary>others</summary><li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Jeong Joon Park](https://jjparkcv.github.io/)</li> <li>[Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/royorel/StyleSDF?style=social)](https://github.com/royorel/StyleSDF) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.11427)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/yenchenlin/nerf-pytorch)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/SerdarHelli/StyleSDF-3D)</li><li>[project](https://stylesdf.github.io/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/royorel/StyleSDF/blob/main/StyleSDF_demo.ipynb/) | 05.03.2022 |
| VideoGPT | A conceptually simple architecture for scaling likelihood based generative modeling to natural videos | <ul><li>[Wilson Yan](https://wilson1yan.github.io/)</li> <li>[Yunzhi Zhang](https://zzyunzhi.github.io/)</li> <li>[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)</li> <li>[Aravind Srinivas](https://people.eecs.berkeley.edu/~aravind/)</li></ul> | [![](https://img.shields.io/github/stars/wilson1yan/VideoGPT?style=social)](https://github.com/wilson1yan/VideoGPT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.10157), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.10509)</li><li>[data](https://www.crcv.ucf.edu/data/UCF101.php)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/VideoGPT)</li><li>[project](https://wilson1yan.github.io/videogpt/index.html)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb/) | 02.03.2022 |
| Disentangled Lifespan Face Synthesis | LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively | <ul><li>[Sen He](https://senhe.github.io/)</li> <li>[Wentong Liao](https://www.tnt.uni-hannover.de/en/staff/liao/)</li> <li>[Michael Yang](https://sites.google.com/site/michaelyingyang/)</li><details><summary>others</summary><li>[Yi-Zhe Song](http://personal.ee.surrey.ac.uk/Personal/Y.Song/)</li> <li>[Bodo Rosenhahn](https://scholar.google.com/citations?user=qq3TxtcAAAAJ)</li> <li>[Tao Xiang](http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html)</li></ul></details> | [![](https://img.shields.io/github/stars/SenHe/DLFS?style=social)](https://github.com/SenHe/DLFS) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.02874)</li><li>[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=uklX03ns0m0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1fgVAoxCSaqPkj0rUK4RmBh7GTQRqLNpE) | 22.02.2022 |
| Mask2Former | Masked-attention Mask Transformer for Universal Image Segmentation | <ul><li>[Bowen Cheng](https://bowenc0221.github.io/)</li> <li>[Ishan Misra](https://imisra.github.io/)</li> <li>[Alexander Schwing](https://alexander-schwing.de/)</li><details><summary>others</summary><li>[Alexander Kirillov](https://alexander-kirillov.github.io/)</li> <li>[Rohit Girdhar](https://rohitgirdhar.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/Mask2Former?style=social)](https://github.com/facebookresearch/Mask2Former) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.01527), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10764)</li><li>[demo](https://replicate.com/facebookresearch/mask2former)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/MaskFormer)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/Mask2Former)</li><li>[project](https://bowenc0221.github.io/mask2former/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1uIWE5KbGFSjrxey2aRd5pWkKNY1_SaNq) | 09.02.2022 |
| SpecVQGAN | Taming the visually guided sound generation by shrinking a training dataset to a set of representative vectors | <ul><li>[Vladimir Iashin](https://iashin.ai/)</li> <li>[Esa Rahtu](https://esa.rahtu.fi/)</li></ul> | [![](https://img.shields.io/github/stars/v-iashin/SpecVQGAN?style=social)](https://github.com/v-iashin/SpecVQGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2110.08791), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.00937), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00820), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.01393), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1512.08512)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/PeihaoChen/regnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/toshas/torch-fidelity), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/descriptinc/melgan-neurips), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/lyra)</li><li>[project](https://iashin.ai/SpecVQGAN)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Foley_(filmmaking)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Row-_and_column-major_order), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Bucb3nAa398)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0) | 03.02.2022 |
| JoJoGAN | One Shot Face Stylization | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | [![](https://img.shields.io/github/stars/mchong6/JoJoGAN?style=social)](https://github.com/mchong6/JoJoGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.11641)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/replicate/cog)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mchong6/JoJoGAN/blob/master/stylize.ipynb/) | 02.02.2022 |
| Pose with Style | Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN | <ul><li>[Badour AlBahar](https://badouralbahar.github.io/)</li> <li>[Jingwan Lu](https://research.adobe.com/person/jingwan-lu/)</li> <li>[Jimei Yang](https://github.com/jimeiyang)</li><details><summary>others</summary><li>[Zhixin Shu](https://zhixinshu.github.io/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Jia-Bin Huang](https://jbhuang0604.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/BadourAlBahar/pose-with-style?style=social)](https://github.com/BadourAlBahar/pose-with-style) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.06166)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://pose-with-style.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/d_ETeAVLilw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tg-bomze/collection-of-notebooks/blob/master/HomeStylist.ipynb/) | 19.01.2022 |
| ConvNeXt | A pure ConvNet model constructed entirely from standard ConvNet modules | <ul><li>[Zhuang Liu](https://liuzhuang13.github.io/)</li> <li>[Hanzi Mao](https://hanzimao.me/)</li> <li>[Chao-Yuan Wu](https://chaoyuan.org/)</li><details><summary>others</summary><li>[Christoph Feichtenhofer](https://feichtenhofer.github.io/)</li> <li>[Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/)</li> <li>[Saining Xie](https://www.sainingxie.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/ConvNeXt?style=social)](https://github.com/facebookresearch/ConvNeXt) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.03545)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/deit), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/unilm/tree/master/beit)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/convnext)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/QzCjXqFnWPE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/idiIllIQOfU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/QqejV0LNDHA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO) | 19.01.2022 |
| diffsort | Differentiable Sorting Networks | <ul><li>[Felix Petersen](http://petersen.ai/)</li> <li>[Christian Borgelt](https://borgelt.net/)</li> <li>[Hilde Kuehne](https://hildekuehne.github.io/)</li> <li>[Oliver Deussen](https://www.cgmi.uni-konstanz.de/personen/prof-dr-oliver-deussen/)</li></ul> | [![](https://img.shields.io/github/stars/Felix-Petersen/diffsort?style=social)](https://github.com/Felix-Petersen/diffsort) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.04019), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.09630)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Rl-sFaE1z4M)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1q0TZFFYB9FlOJYWKt0_7ZaXQT190anhm) | 17.01.2022 |
| Taming Transformers for High-Resolution Image Synthesis | We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer | <ul><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Robin Rombach](https://github.com/rromb)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li></ul> | [![](https://img.shields.io/github/stars/CompVis/taming-transformers?style=social)](https://github.com/CompVis/taming-transformers) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841)</li><li>[project](https://compvis.github.io/taming-transformers/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb/) | 13.01.2022 |
| FuseDream | Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization | <ul><li>[Xingchao Liu](https://scholar.google.com/citations?user=VOTVE0UAAAAJ)</li> <li>[Chengyue Gong](https://github.com/ChengyueGongR)</li> <li>[Lemeng Wu](https://github.com/klightz)</li><details><summary>others</summary><li>[Hao Su](https://cseweb.ucsd.edu//~haosu/)</li> <li>[Qiang Liu](https://www.cs.utexas.edu/~lqiang/)</li></ul></details> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.01573)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/190tKQf0aFj-Hi8STUrLc2m4DeOviv7NO) | 02.01.2022 |
| GLIDE | Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models | <ul><li>[Alex Nichol](https://aqnichol.com/)</li> <li>[Prafulla Dhariwal](https://github.com/prafullasd)</li> <li>[Aditya Ramesh](http://adityaramesh.com/)</li><details><summary>others</summary><li>[Pranav Shyam](https://github.com/pranv)</li> <li>[Pamela Mishkin](https://manlikemishap.github.io/)</li> <li>[Bob McGrew](https://github.com/bmcgrew)</li> <li>[Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/)</li> <li>[Mark Chen](https://scholar.google.com/citations?user=5fU-QMwAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/openai/glide-text2im?style=social)](https://github.com/openai/glide-text2im) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10741)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ItKi3h7IY2o)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/openai/glide-text2im/blob/master/notebooks/inpaint.ipynb/) | 22.12.2021 |
| Music Composer | Synthesizing symbolic music in MIDI format using the Music Transformer model | [bazanovvanya](https://github.com/bazanovvanya) | [![](https://img.shields.io/github/stars/ai-forever/music-composer?style=social)](https://github.com/ai-forever/music-composer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1909.05858)</li><li>[blog post](https://habr.com/ru/company/sberbank/blog/583592/)</li><li>[data](https://magenta.tensorflow.org/datasets/maestro), [data](https://colinraffel.com//projects/lmd/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/gwinndr/MusicTransformer-Pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/bytedance/GiantMIDI-Piano), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdeff/fma)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ai-forever/music-composer/blob/master/src/Music_Composer_Demo_Colab_en.ipynb/) | 20.12.2021 |
| PoolFormer | MetaFormer Is Actually What You Need for Vision | <ul><li>[Weihao Yu](https://whyu.me/)</li> <li>[Mi Luo](https://luomi97.github.io/)</li> <li>[Pan Zhou](https://panzhous.github.io/)</li><details><summary>others</summary><li>[Chenyang Si](https://github.com/ChenyangSi)</li> <li>[Yichen Zhou](https://dblp.org/pid/55/10422.html)</li> <li>[Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)</li> <li>[Jiashi Feng](https://sites.google.com/site/jshfeng/)</li> <li>[Shuicheng Yan](https://yanshuicheng.ai/)</li></ul></details> | [![](https://img.shields.io/github/stars/sail-sg/poolformer?style=social)](https://github.com/sail-sg/poolformer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.11418)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fvcore), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/apex)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/poolformer)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/sail-sg/poolformer/blob/main/misc/poolformer_demo.ipynb/) | 05.12.2021 |
| HyperStyle | A hypernetwork that learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Omer Tov](https://github.com/omertov)</li> <li>[Ron Mokady](https://rmokady.github.io/)</li><details><summary>others</summary><li>[Rinon Gal](https://rinongal.github.io/)</li> <li>[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)</li></ul></details> | [![](https://img.shields.io/github/stars/yuval-alaluf/hyperstyle?style=social)](https://github.com/yuval-alaluf/hyperstyle) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.15666), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.03189), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09036), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.07727)</li><li>[data](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/clovaai/stargan-v2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HuangYG123/CurricularFace), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/dvschultz/stylegan2-ada-pytorch)</li><li>[project](https://yuval-alaluf.github.io/hyperstyle/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/_sbXmLY2jMw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yuval-alaluf/hyperstyle/blob/master/notebooks/inference_playground.ipynb/) | 03.12.2021 |
| encoder4editing | Designing an Encoder for StyleGAN Image Manipulation | <ul><li>[Omer Tov](https://github.com/omertov)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li><details><summary>others</summary><li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/omertov/encoder4editing?style=social)](https://github.com/omertov/encoder4editing) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/omertov/encoder4editing/blob/master/notebooks/inference_playground.ipynb/) | 02.12.2021 |
| StyleCariGAN | Caricature Generation via StyleGAN Feature Map Modulation | <ul><li>[Wonjong Jang](https://wonjongg.github.io/)</li> <li>[Gwangjin Ju](https://github.com/jugwangjin)</li> <li>[Yucheol Jung](https://ycjung.info/)</li><details><summary>others</summary><li>[Jiaolong Yang](https://jlyang.org/)</li> <li>[Xin Tong](https://www.microsoft.com/en-us/research/people/xtong/)</li> <li>[Seungyong Lee](https://scholar.google.com/citations?user=yGPH-nAAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/wonjongg/StyleCariGAN?style=social)](https://github.com/wonjongg/StyleCariGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.04331)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://wonjongg.github.io/StyleCariGAN/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=kpHbGOlI-BU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1HDRQGm7pvC9mAb6Lktoft_SmY9sCq_Qg) | 30.11.2021 |
| CartoonGAN | The implementation of the cartoon GAN model with PyTorch | [Tobias Sunderdiek](https://github.com/TobiasSunderdiek) | <ul><li>[CVPR](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/alamson/safebooru)</li><li>[project](https://tobiassunderdiek.github.io/cartoon-gan/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb/) | 24.11.2021 |
| SimSwap | An efficient framework, called Simple Swap, aiming for generalized and high fidelity face swapping | <ul><li>[Xuanhong Chen](https://github.com/neuralchen)</li> <li>[Bingbing Ni](https://scholar.google.com.sg/citations?user=eUbmKwYAAAAJ)</li> <li>[Yanhao Ge](https://scholar.google.com/citations?user=h6tuBAcAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/neuralchen/SimSwap?style=social)](https://github.com/neuralchen/SimSwap) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06340)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepinsight/insightface)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/neuralchen/SimSwap/blob/master/SimSwap%20colab.ipynb/) | 24.11.2021 |
| RVM | Robust High-Resolution Video Matting with Temporal Guidance | <ul><li>[Peter Lin](https://github.com/PeterL1n)</li> <li>[Linjie Yang](https://sites.google.com/site/linjieyang89/)</li> <li>[Imran Saleemi](http://www.cs.ucf.edu/~imran/)</li> <li>[Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)</li></ul> | [![](https://img.shields.io/github/stars/PeterL1n/RobustVideoMatting?style=social)](https://github.com/PeterL1n/RobustVideoMatting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2108.11515)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/VideoProcessingFramework), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting)</li><li>[project](https://peterl1n.github.io/RobustVideoMatting)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Jvzltozpbpk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Ay-mGCEYEzM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm) | 24.11.2021 |
| AnimeGANv2 | An improved version of AnimeGAN - it prevents the generation of high-frequency artifacts by simply changing the normalization of features in the network | <ul><li>[Xin Chen](https://dl.acm.org/profile/99659508903)</li> <li>[Gang Liu](https://dl.acm.org/profile/81493643454)</li> <li>[bryandlee](https://github.com/bryandlee)</li></ul> | [![](https://img.shields.io/github/stars/bryandlee/animegan2-pytorch?style=social)](https://github.com/bryandlee/animegan2-pytorch) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/TachibanaYoshino/AnimeGANv2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TachibanaYoshino/AnimeGAN)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/AnimeGANv2)</li><li>[project](https://tachibanayoshino.github.io/AnimeGANv2/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/bryandlee/animegan2-pytorch/blob/master/colab_demo.ipynb/) | 17.11.2021 |
| SOAT | StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[Hsin-Ying Lee](http://hsinyinglee.com/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | [![](https://img.shields.io/github/stars/mchong6/SOAT?style=social)](https://github.com/mchong6/SOAT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.01619)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/toonify), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/akhaliq/SOAT)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mchong6/SOAT/blob/master/infinity.ipynb/) | 13.11.2021 |
| Arnheim | Generative Art Using Neural Visual Grammars and Dual Encoders | <ul><li>[Chrisantha Fernando](https://www.chrisantha.co.uk/)</li> <li>[Ali Eslami](http://arkitus.com/)</li> <li>[Jean-Baptiste Alayrac](https://www.jbalayrac.com/)</li><details><summary>others</summary><li>[Piotr Mirowski](https://piotrmirowski.com/)</li> <li>[Dylan Banarse](https://www.2ne1.com/)</li> <li>[Simon Osindero](https://scholar.google.com/citations?user=Jq8ZS5kAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/arnheim?style=social)](https://github.com/deepmind/arnheim) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.00162), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.14843), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.07729), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.02580), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1609.09106)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/dall-e)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Compositional_pattern-producing_network)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=U7guaMdeF4g), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=zh0goLbS-l0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=SYJGNt7yu6M), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=MxkYKa0x5AU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/arnheim/blob/master/arnheim_2.ipynb/) | 11.11.2021 |
| StyleGAN 2 | Generation of faces, cars, etc. | [Mikael Christensen](https://github.com/Syntopia) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) | 05.11.2021 |
| ruDALL·E | Generate images from texts in Russian | [Alex Shonenkov](https://github.com/shonenkov) | [![](https://img.shields.io/github/stars/ai-forever/ru-dalle?style=social)](https://github.com/ai-forever/ru-dalle) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/bes-dev/vqvae_dwt_distiller.pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/boomb0om/Real-ESRGAN-colab)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/multimodalart/rudalle)</li><li>[project](https://rudalle.ru/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ai-forever/ru-dalle/blob/master/jupyters/ruDALLE-example-generation-A100.ipynb/) | 03.11.2021 |
| ByteTrack | Multi-Object Tracking by Associating Every Detection Box | <ul><li>[Yifu Zhang](https://github.com/ifzhang)</li> <li>[Peize Sun](https://peizesun.github.io/)</li> <li>[Yi Jiang](https://github.com/iFighting)</li><details><summary>others</summary><li>[Dongdong Yu](https://miracle-fmh.github.io/)</li> <li>[Ping Luo](http://luoping.me/)</li> <li>[Xinggang Wang](https://xinggangw.info/)</li></ul></details> | [![](https://img.shields.io/github/stars/ifzhang/ByteTrack?style=social)](https://github.com/ifzhang/ByteTrack) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2110.06864)</li><li>[data](https://motchallenge.net/), [data](https://www.crowdhuman.org/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Megvii-BaseDetection/YOLOX), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ifzhang/FairMOT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/PeizeSun/TransTrack), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/samylee/Towards-Realtime-MOT-Cpp)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/multi-object-tracking)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0) | 30.10.2021 |
| StyleGAN3 | Alias-Free Generative Adversarial Networks | <ul><li>[Tero Karras](https://research.nvidia.com/person/tero-karras)</li> <li>[Miika Aittala](https://research.nvidia.com/person/Miika-Aittala)</li> <li>[Samuli Laine](https://research.nvidia.com/person/Samuli-Laine)</li><details><summary>others</summary><li>[Erik Härkönen](https://github.com/harskish)</li> <li>[Janne Hellsten](https://research.nvidia.com/person/Janne-Hellsten)</li> <li>[Jaakko Lehtinen](https://users.aalto.fi/~lehtinj7/)</li> <li>[Timo Aila](https://research.nvidia.com/person/timo-aila)</li></ul></details> | [![](https://img.shields.io/github/stars/NVlabs/stylegan3?style=social)](https://github.com/NVlabs/stylegan3) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.12423), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.01401), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.06991), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.04948), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.03498)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan3-detector), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/metfaces-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada)</li><li>[project](https://nvlabs.github.io/stylegan3)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1BXNHZBai-pXtP-ncliouXo_kUiG1Pq7M) | 19.10.2021 |
| GPT-2 | Retrain an advanced text generating neural network on any text dataset using gpt-2-simple! | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/gpt-2-simple?style=social)](https://github.com/minimaxir/gpt-2-simple) <ul><li>[blog post](https://minimaxir.com/2019/09/howto-gpt2/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) | 18.10.2021 |
| ConvMixer | An extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network | <ul><li>[Asher Trockman](http://ashertrockman.com/)</li> <li>[Zico Kolter](http://zicokolter.com/)</li></ul> | [![](https://img.shields.io/github/stars/locuslab/convmixer?style=social)](https://github.com/locuslab/convmixer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.09792)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/locuslab/convmixer-cifar10), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Gl0s0GDqN3c?t=990)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/locuslab/convmixer/blob/main/pytorch-image-models/notebooks/EffResNetComparison.ipynb/) | 05.10.2021 |
| IC-GAN | Instance-Conditioned GAN | <ul><li>[Arantxa Casanova](https://github.com/ArantxaCasanova)</li> <li>[Marlène Careil](https://www.linkedin.com/in/marl%C3%A8ne-careil-901804155)</li> <li>[Jakob Verbeek](http://thoth.inrialpes.fr/~verbeek/)</li><details><summary>others</summary><li>[Michał Drożdżal](https://scholar.google.com/citations?user=XK_ktwQAAAAJ)</li> <li>[Adriana Romero-Soriano](https://sites.google.com/site/adriromsor)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/ic_gan?style=social)](https://github.com/facebookresearch/ic_gan) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.05070)</li><li>[blog post](https://ai.facebook.com/blog/instance-conditioned-gans/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/faiss), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ajbrock/BigGAN-PyTorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/bioinf-jku/TTUR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/mit-han-lab/data-efficient-gans)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/ic_gan/blob/master/inference/icgan_colab.ipynb/) | 01.10.2021 |
| Skillful Precipitation Nowcasting Using Deep Generative Models of Radar | Open-sourced dataset and model snapshot for precipitation nowcasting | <ul><li>[Suman Ravuri](https://www.linkedin.com/in/suman-ravuri-81928082)</li> <li>[Karel Lenc](https://www.robots.ox.ac.uk/~karel/)</li> <li>[Matthew Willson](https://www.linkedin.com/in/matthew-willson-6a1b422)</li><details><summary>others</summary><li>[Dmitry Kangin](https://scholar.google.com/citations?user=vv-leaMAAAAJ)</li> <li>[Rémi Lam](https://scholar.google.com/citations?user=Sm7xCbEAAAAJ)</li> <li>[Piotr Mirowski](https://piotrmirowski.com/)</li> <li>[Maria Athanassiadou](https://scholar.google.com/citations?user=VtkgHP0AAAAJ)</li> <li>[Sheleem Kashem](https://www.linkedin.com/in/sheleemkashem/)</li> <li>[Rachel Prudden](https://computerscience.exeter.ac.uk/staff/rep218)</li> <li>[Amol Mandhane](https://github.com/amol-mandhane)</li> <li>[Aidan Clark](https://scholar.google.com/citations?user=_19DrfIAAAAJ)</li> <li>[Andrew Brock](https://github.com/ajbrock)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li> <li>[Raia Hadsell](https://github.com/raiah)</li> <li>[Niall Robinson](https://github.com/niallrobinson)</li> <li>[Ellen Clancy](https://www.linkedin.com/in/ellen-clancy-815967124)</li> <li>[Shakir Mohamed](https://www.shakirm.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/deepmind-research?style=social)](https://github.com/deepmind/deepmind-research/tree/master/nowcasting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00954)</li><li>[blog post](https://deepmind.com/blog/article/nowcasting)</li><li>[local kernel](https://research.google.com/colaboratory/local-runtimes.html)</li><li>[paper](https://www.nature.com/articles/s41586-021-03854-z)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/deepmind-research/blob/master/nowcasting/Open_sourced_dataset_and_model_snapshot_for_precipitation_nowcasting.ipynb/) | 29.09.2021 |
| Live Speech Portraits | Real-Time Photorealistic Talking-Head Animation | <ul><li>[Yuanxun Lu](https://github.com/YuanxunLu)</li> <li>[Jinxiang Chai](https://scholar.google.com/citations?user=OcN1_gwAAAAJ)</li> <li>[Xun Cao](https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html)</li></ul> | [![](https://img.shields.io/github/stars/YuanxunLu/LiveSpeechPortraits?style=social)](https://github.com/YuanxunLu/LiveSpeechPortraits) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.10595)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lelechen63/ATVGnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/DinoMan/speech-driven-animation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)</li><li>[project](https://yuanxunlu.github.io/projects/LiveSpeechPortraits/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1tKvi-9kY3GkEK8lgtfTSM70rMFo_TY50) | 26.09.2021 |
| StylEx | Training a GAN to explain a classifier in StyleSpace | <ul><li>[Oran Lang](https://research.google/people/105975/)</li> <li>[Yossi Gandelsman](https://yossigandelsman.github.io/)</li> <li>[Michal Yarom](https://scholar.google.com/citations?user=GMVxiYgAAAAJ)</li><details><summary>others</summary><li>[Yoav Wald](https://scholar.google.com/citations?user=hh5nOn4AAAAJ)</li> <li>[Gal Elidan](https://research.google/people/105719/)</li> <li>[Avinatan Hassidim](https://research.google/people/105831/)</li> <li>[William Freeman](https://billf.mit.edu/)</li> <li>[Phillip Isola](http://web.mit.edu/phillipi/)</li> <li>[Amir Globerso](https://cs3801.wixsite.com/amirgloberson)</li> <li>[Michal Irani](http://www.weizmann.ac.il/math/irani/)</li> <li>[Inbar Mosseri](https://research.google/people/InbarMosseri/)</li></ul></details> | [![](https://img.shields.io/github/stars/google/explaining-in-style?style=social)](https://github.com/google/explaining-in-style) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.13369), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.10112), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.12799), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.01711)</li><li>[blog post](https://ai.googleblog.com/2022/01/introducing-stylex-new-approach-for.html)</li><li>[project](https://explaining-in-style.github.io/)</li><li>[supplementary](https://explaining-in-style.github.io/supmat.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/wLk2eBdXH4M)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/explaining-in-style/blob/main/Explaining_in_Style_AttFind.ipynb/) | 25.08.2021 |
| Bringing Old Photo Back to Life | Restoring old photos that suffer from severe degradation through a deep learning approach | <ul><li>[Ziyu Wan](http://raywzy.com/)</li> <li>[Bo Zhang](https://bo-zhang.me/)</li> <li>[Dongdong Chen](http://www.dongdongchen.bid/)</li><details><summary>others</summary><li>[Pan Zhang](https://panzhang0212.github.io/)</li> <li>[Dong Chen](http://www.dongchen.pro/)</li> <li>[Jing Liao](https://liaojing.github.io/html/)</li> <li>[Fang Wen](https://www.microsoft.com/en-us/research/people/fangwen/)</li></ul></details> | [![](https://img.shields.io/github/stars/microsoft/Bringing-Old-Photos-Back-to-Life?style=social)](https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.09484)</li><li>[demo](https://replicate.com/microsoft/bringing-old-photos-back-to-life)</li><li>[project](http://raywzy.com/Old_Photo/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Q5bhszQq9eA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA) | 13.07.2021 |
| PTI | Pivotal Tuning Inversion enables employing off-the-shelf latent based semantic editing techniques on real images using StyleGAN | <ul><li>[Daniel Roich](https://github.com/danielroich)</li> <li>[Ron Mokady](https://rmokady.github.io/)</li> <li>[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | [![](https://img.shields.io/github/stars/danielroich/PTI?style=social)](https://github.com/danielroich/PTI) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.05744)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2-ada-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/richzhang/PerceptualSimilarity)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb/) | 01.07.2021 |
| TediGAN | Framework for multi-modal image generation and manipulation with textual descriptions | <ul><li>[Weihao Xia](https://github.com/weihaox)</li> <li>[Yujiu Yang](http://www.fiesta.tsinghua.edu.cn/pi/3/24)</li> <li>[Jing-Hao Xue](http://www.homepages.ucl.ac.uk/~ucakjxu/)</li> <li>[Baoyuan Wu](https://sites.google.com/site/baoyuanwu2015/home)</li></ul> | [![](https://img.shields.io/github/stars/IIGROUP/TediGAN?style=social)](https://github.com/IIGROUP/TediGAN) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.03308), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.08910)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/weihaox/Multi-Modal-CelebA-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L8Na2f5viAM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](http://colab.research.google.com/github/weihaox/TediGAN/blob/master/playground.ipynb/) | 30.06.2021 |
| GANs N' Roses | Stable, Controllable, Diverse Image to Image Translation | <ul><li>[Min Jin Chong](https://mchong6.github.io/)</li> <li>[David Forsyth](http://luthuli.cs.uiuc.edu/~daf/)</li></ul> | [![](https://img.shields.io/github/stars/mchong6/GANsNRoses?style=social)](https://github.com/mchong6/GANsNRoses) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06561), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.06600)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/znxlwm/UGATIT-pytorch)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/VNg0NyCGl_4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mchong6/GANsNRoses/blob/master/inference_colab.ipynb/) | 19.06.2021 |
| Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes | A method to stylize images by optimizing parameterized brushstrokes instead of pixels | <ul><li>[Dmytro Kotovenko](https://scholar.google.de/citations?user=T_U8yxwAAAAJ)</li> <li>[Matthias Wright](https://matthias-wright.github.io/)</li> <li>[Arthur Heimbrecht](https://github.com/arwehei)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li></ul> | [![](https://img.shields.io/github/stars/CompVis/brushstroke-parameterized-style-transfer?style=social)](https://github.com/CompVis/brushstroke-parameterized-style-transfer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17185)</li><li>[project](https://compvis.github.io/brushstroke-parameterized-style-transfer/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CompVis/brushstroke-parameterized-style-transfer/blob/tensorflow_v2/notebooks/BrushstrokeStyleTransfer_TF2.ipynb/) | 02.06.2021 |
| Pixel2Style2Pixel | Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation | <ul><li>[Elad Richardson](https://github.com/eladrich)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | [![](https://img.shields.io/github/stars/eladrich/pixel2style2pixel?style=social)](https://github.com/eladrich/pixel2style2pixel) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HuangYG123/CurricularFace)</li><li>[project](https://eladrich.github.io/pixel2style2pixel/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/bfvSwhqsTgM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb/) | 01.06.2021 |
| Fine-tuning a BERT | We will work through fine-tuning a BERT model using the tensorflow-models PIP package | <ul><li>[Chen Chen](https://github.com/chenGitHuber)</li> <li>[Claire Yao](https://github.com/claireyao-fen)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tensorflow.org/hub)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb/) | 24.05.2021 |
| ReStyle | A Residual-Based StyleGAN Encoder via Iterative Refinement | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | [![](https://img.shields.io/github/stars/yuval-alaluf/restyle-encoder?style=social)](https://github.com/yuval-alaluf/restyle-encoder) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.02699), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch)</li><li>[project](https://yuval-alaluf.github.io/restyle-encoder/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb/) | 21.05.2021 |
| Motion Representations for Articulated Animation | Novel motion representations for animating articulated objects consisting of distinct parts | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Oliver Woodford](https://ojwoodford.github.io/)</li> <li>[Jian Ren](https://alanspike.github.io/)</li><details><summary>others</summary><li>[Menglei Chai](https://mlchai.com/)</li> <li>[Sergey Tulyakov](http://www.stulyakov.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/snap-research/articulated-animation?style=social)](https://github.com/snap-research/articulated-animation) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.11280)</li><li>[project](https://snap-research.github.io/articulated-animation/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=gpBYN8t8_yY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/AliaksandrSiarohin/articulated-animation/blob/master/demo.ipynb/) | 29.04.2021 |
| SAM | Age Transformation Using a Style-Based Regression Model | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | [![](https://img.shields.io/github/stars/yuval-alaluf/SAM?style=social)](https://github.com/yuval-alaluf/SAM) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02754)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://yuval-alaluf.github.io/SAM/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/X_pYC_LtBFw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](http://colab.research.google.com/github/yuval-alaluf/SAM/blob/master/notebooks/animation_inference_playground.ipynb/) | 26.04.2021 |
| SkinDeep | Remove Body Tattoo Using Deep Learning | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://img.shields.io/github/stars/vijishmadhavan/SkinDeep?style=social)](https://github.com/vijishmadhavan/SkinDeep) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb/) | 24.04.2021 |
| Geometry-Free View Synthesis | Is a geometric model required to synthesize novel views from a single image? | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li></ul> | [![](https://img.shields.io/github/stars/CompVis/geometry-free-view-synthesis?style=social)](https://github.com/CompVis/geometry-free-view-synthesis) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.07652)</li><li>[data](https://google.github.io/realestate10k/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/colmap/colmap)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CompVis/geometry-free-view-synthesis/blob/master/scripts/braindance.ipynb/) | 22.04.2021 |
| NeRViS | An algorithm for full-frame video stabilization by first estimating dense warp fields | <ul><li>[Yu-Lun Liu](http://www.cmlab.csie.ntu.edu.tw/~yulunliu/)</li> <li>[Wei-Sheng Lai](https://www.wslai.net/)</li> <li>[Ming-Hsuan Yang](https://faculty.ucmerced.edu/mhyang/)</li><details><summary>others</summary><li>[Yung-Yu Chuang](https://www.csie.ntu.edu.tw/~cyy/)</li> <li>[Jia-Bin Huang](https://jbhuang0604.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/alex04072000/NeRViS?style=social)](https://github.com/alex04072000/NeRViS) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06205)</li><li>[data](http://liushuaicheng.org/SIGGRAPH2013/database.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cxjyxxme/deep-online-video-stabilization), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jinsc37/DIFRINT)</li><li>[project](https://alex04072000.github.io/NeRViS/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/KO3sULs4hso)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH) | 11.04.2021 |
| NeX | View synthesis based on enhancements of multiplane image that can reproduce NeXt-level view-dependent effects in real time | <ul><li>[Suttisak Wizadwongsa](https://www.linkedin.com/in/suttisak-wizadwongsa-763a931a5/)</li> <li>[Pakkapon Phongthawee](http://pureexe.github.io/)</li> <li>[Jiraphon Yenphraphai](https://www.linkedin.com/in/jiraphon-yenphraphai-990ba6175/)</li> <li>[Supasorn Suwajanakorn](https://www.supasorn.com/)</li></ul> | [![](https://img.shields.io/github/stars/nex-mpi/nex-code?style=social)](https://github.com/nex-mpi/nex-code) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.05606)</li><li>[data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9WXRVQTQtQTcyVWc), [data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VyalBSUkw5Sm5GSXA4TU42ZDFqRXVvQjNYVm94SmtmZlBqZm9QeWhIa2owZGc%5FcnRpbWU9bC0yYWctRTcyVWc&id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fmodified%5Fdataset)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Fyusion/LLFF)</li><li>[project](https://nex-mpi.github.io/)</li><li>[vistec](https://vistec.ist/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=HyfkF7Z-ddA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU) | 25.03.2021 |
| Score SDE | Score-Based Generative Modeling through Stochastic Differential Equations | <ul><li>[Yang Song](https://yang-song.net/)</li> <li>[Jascha Sohl-Dickstein](http://www.sohldickstein.com/)</li> <li>[Diederik Kingma](http://dpkingma.com/)</li><details><summary>others</summary><li>[Abhishek Kumar](https://abhishek.umiacs.io/)</li> <li>[Stefano Ermon](https://cs.stanford.edu/~ermon/)</li> <li>[Ben Poole](https://cs.stanford.edu/~poole/)</li></ul></details> | [![](https://img.shields.io/github/stars/yang-song/score_sde?style=social)](https://github.com/yang-song/score_sde) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.13456), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.05600), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09011), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.11239)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yang-song/score_sde_pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/ml_collections)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L9ZegT87QK8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yang-song/score_sde/blob/main/Score_SDE_demo.ipynb/) | 18.03.2021 |
| Big Sleep | Text to image generation, using OpenAI's CLIP and a BigGAN | [Phil Wang](https://github.com/lucidrains) | [![](https://img.shields.io/github/stars/lucidrains/big-sleep?style=social)](https://github.com/lucidrains/big-sleep) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/comments/lxawb4/how_to_use_some_of_the_newer_features_of/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb) | 17.03.2021 |
| Deep Daze | Text to image generation using OpenAI's CLIP and Siren | [Phil Wang](https://github.com/lucidrains) | [![](https://img.shields.io/github/stars/lucidrains/deep-daze?style=social)](https://github.com/lucidrains/deep-daze) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/deepdaze/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj) | 17.03.2021 |
| Talking Head Anime from a Single Image | The network takes as input an image of an anime character's face and a desired pose, and it outputs another image of the same character in the given pose | [Pramook Khungurn](https://pkhungurn.github.io/) | [![](https://img.shields.io/github/stars/pkhungurn/talking-head-anime-demo?style=social)](https://github.com/pkhungurn/talking-head-anime-demo) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lincolnhard/head-pose-estimation)</li><li>[project](https://pkhungurn.github.io/talking-head-anime/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Virtual_YouTuber), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/MikuMikuDance)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/kMQCERkTdO0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/T1Gp-RxFZwU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/FioRJ6x_RbI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb/) | 23.02.2021 |
| NFNet | An adaptive gradient clipping technique, a significantly improved class of Normalizer-Free ResNets | <ul><li>[Andrew Brock](https://github.com/ajbrock)</li> <li>[Soham De](https://sohamde.github.io/)</li> <li>[Samuel L. Smith](https://scholar.google.co.uk/citations?user=fyEqU5oAAAAJ)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/deepmind/deepmind-research?style=social)](https://github.com/deepmind/deepmind-research/tree/master/nfnets) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06171), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.08692)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/jaxline)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb/) | 17.02.2021 |
| CLIP | A neural network which efficiently learns visual concepts from natural language supervision | <ul><li>[Jong Wook](https://jongwook.kim/)</li> <li>[Alec Radford](http://newmu.github.io/)</li> <li>[Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/)</li></ul> | [![](https://img.shields.io/github/stars/openai/CLIP?style=social)](https://github.com/openai/CLIP) <ul><li>[data](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)</li><li>[project](https://openai.com/blog/clip/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb/) | 29.01.2021 |
| Adversarial Patch | A method to create universal, robust, targeted adversarial image patches in the real world | [Tom Brown](https://github.com/nottombrown) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.09665)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/cleverhans-lab/cleverhans/blob/master/examples/adversarial_patch/AdversarialPatch.ipynb/) | 27.01.2021 |
| MSG-Net | Multi-style Generative Network with a novel Inspiration Layer, which retains the functionality of optimization-based approaches and has the fast speed of feed-forward networks | <ul><li>[Hang Zhang](https://hangzhang.org/)</li> <li>[Kristin Dana](https://www.ece.rutgers.edu/~kdana/dana.html)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06953)</li><li>[project](http://computervisionrutgers.github.io/MSG-Net/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oy6pWNWBt4Y)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/zhanghang1989/PyTorch-Multi-Style-Transfer/blob/master/msgnet.ipynb/) | 25.01.2021 |
| HiDT | A generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution | <ul><li>[Denis Korzhenkov](https://github.com/denkorzh)</li> <li>[Gleb Sterkin](https://github.com/belkakari)</li> <li>[Sergey Nikolenko](https://logic.pdmi.ras.ru/~sergey/)</li> <li>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/)</li></ul> | [![](https://img.shields.io/github/stars/saic-mdal/HiDT?style=social)](https://github.com/saic-mdal/HiDT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.08791)</li><li>[project](https://saic-mdal.github.io/HiDT/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLuvGzlEQXT1KQuKrfBBEWh2f3PToxyeM5), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=EWKAgwgqXB4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/saic-mdal/hidt/blob/master/notebooks/HighResolutionDaytimeTranslation.ipynb/) | 24.01.2021 |
| Neural Style Transfer | Implementation of Neural Style Transfer in Keras 2.0+ | [Somshubra Majumdar](http://titu1994.github.io/) | [![](https://img.shields.io/github/stars/titu1994/Neural-Style-Transfer?style=social)](https://github.com/titu1994/Neural-Style-Transfer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1508.06576), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1605.04603), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.05897)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/titu1994/Neural-Style-Transfer/blob/master/NeuralStyleTransfer.ipynb/) | 22.01.2021 |
| SkyAR | A vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles | [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/) | [![](https://img.shields.io/github/stars/jiupinjia/SkyAR?style=social)](https://github.com/jiupinjia/SkyAR) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11800)</li><li>[project](https://jiupinjia.github.io/skyar/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=zal9Ues0aOQ)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jiupinjia/SkyAR/blob/master/colab_demo.ipynb/) | 18.01.2021 |
| Big GAN | Large Scale GAN Training for High Fidelity Natural Image Synthesis | [Google](https://www.tensorflow.org/hub) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb/) | 12.01.2021 |
| MusicXML Documentation | The goal of this notebook is to explore one of the magenta libraries for music | <ul><li>[Prakruti Joshi](https://github.com/prakruti-joshi)</li> <li>[Falak Shah](https://falaktheoptimist.github.io/)</li> <li>[Twisha Naik](https://github.com/twisha96)</li></ul> | <ul><li>[magenta](https://magenta.tensorflow.org/)</li><li>[music theory](http://musictheoryblog.blogspot.com/2008/02/learn-music-theory.html)</li><li>[musicXML](https://www.musicxml.com/for-developers/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicXML_Document_Structure_Documentation.ipynb/) | 08.01.2021 |
| SVG VAE | A colab demo for the SVG VAE model | [Raphael Gontijo Lopes](https://raphagl.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.02632)</li><li>[blog post](https://magenta.tensorflow.org/svg-vae)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/magenta/magenta-demos/blob/master/colab-notebooks/vae_svg_decoding.ipynb/) | 08.01.2021 |
| Neural Magic Eye | Learning to See and Understand the Scene Behind an Autostereogram | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | [![](https://img.shields.io/github/stars/jiupinjia/neural-magic-eye?style=social)](https://github.com/jiupinjia/neural-magic-eye) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.15692)</li><li>[project](https://jiupinjia.github.io/neuralmagiceye/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Fkh7DEblqJ8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1f59dFLJ748i2TleE54RkbUZSMo9Hyx7l) | 01.01.2021 |
| Flow-edge Guided Video Completion | Method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges | <ul><li>[Chen Gao](http://chengao.vision/)</li> <li>[Ayush Saraf](https://github.com/ayush29feb)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://jbhuang0604.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/vt-vl-lab/FGVC?style=social)](https://github.com/vt-vl-lab/FGVC) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2009.01835)</li><li>[project](http://chengao.vision/FGVC/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=CHHVPxHT7rc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc) | 30.12.2020 |
| ArtLine | A Deep Learning based project for creating line art portraits | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://img.shields.io/github/stars/vijishmadhavan/ArtLine?style=social)](https://github.com/vijishmadhavan/ArtLine) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[data](https://cg.cs.tsinghua.edu.cn/people/~Yongjin/APDrawingDB.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yiranran/APDrawingGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/vijishmadhavan/Light-Up/blob/master/ArtLine(AR).ipynb/) | 24.12.2020 |
| WikiArt (stylegan2-ada) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.06676)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Norod/my-colab-experiments/blob/master/WikiArt_ADA_Example_Generation.ipynb/) | 08.12.2020 |
| GANSpace | A simple technique to analyze GANs and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day | <ul><li>[Erik Härkönen](https://github.com/harskish)</li> <li>[Aaron Hertzmann](http://www.dgp.toronto.edu/~hertzman/)</li> <li>[Jaakko Lehtinen](https://users.aalto.fi/~lehtinj7/)</li> <li>[Sylvain Paris](http://people.csail.mit.edu/sparis/)</li></ul> | [![](https://img.shields.io/github/stars/harskish/ganspace?style=social)](https://github.com/harskish/ganspace) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.02546)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/GANDissect)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/jdTICDa_eAI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/harskish/ganspace/blob/master/notebooks/Ganspace_colab.ipynb/) | 06.12.2020 |
| SeFa | A closed-form approach for unsupervised latent semantic factorization in GANs | <ul><li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Bolei Zhou](https://boleizhou.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/genforce/sefa?style=social)](https://github.com/genforce/sefa) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.06600)</li><li>[project](https://genforce.github.io/sefa/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=OFHW2WbXXIQ)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/genforce/sefa/blob/master/docs/SeFa.ipynb/) | 06.12.2020 |
| Stylized Neural Painting | An image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | [![](https://img.shields.io/github/stars/jiupinjia/stylized-neural-painting?style=social)](https://github.com/jiupinjia/stylized-neural-painting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08114)</li><li>[project](https://jiupinjia.github.io/neuralpainter/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oerb-nwrXhk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1ch_41GtcQNQT1NLOA21vQJ_rQOjjv9D8) | 01.12.2020 |
| MakeItTalk | A method that generates expressive talking-head videos from a single facial image with audio as the only input | <ul><li>[Yang Zhou](https://people.umass.edu/~yangzhou/)</li> <li>[Xintong Han](http://users.umiacs.umd.edu/~xintong/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li><details><summary>others</summary><li>[Jose Echevarria](http://www.jiechevarria.com/)</li> <li>[Evangelos Kalogerakis](https://people.cs.umass.edu/~kalo/)</li> <li>[Dingzeyu Li](https://dingzeyu.li/)</li></ul></details> | [![](https://img.shields.io/github/stars/yzhou359/MakeItTalk?style=social)](https://github.com/yzhou359/MakeItTalk) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.12992)</li><li>[data](https://drive.google.com/drive/folders/1EwuAy3j1b9Zc1MsidUfxG_pJGc_cV60O)</li><li>[project](https://people.umass.edu/~yangzhou/MakeItTalk/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=vUMGKASgbf8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/yzhou359/MakeItTalk/blob/master/quick_demo.ipynb/) | 10.11.2020 |
| LaSAFT | Latent Source Attentive Frequency Transformation for Conditioned Source Separation | [Woosung Choi](https://ws-choi.github.io/) | [![](https://img.shields.io/github/stars/ws-choi/Conditioned-Source-Separation-LaSAFT?style=social)](https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11631)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[project](https://lasaft.github.io/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ws-choi/Conditioned-Source-Separation-LaSAFT/blob/master/colab_demo/LaSAFT_with_GPoCM_Stella_Jang_Example.ipynb/) | 01.11.2020 |
| Lifespan Age Transformation Synthesis | Multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process | <ul><li>[Roy Or-El](https://homes.cs.washington.edu/~royorel/)</li> <li>[Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)</li> <li>[Ohad Fried](https://www.ohadf.com/)</li><details><summary>others</summary><li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/royorel/Lifespan_Age_Transformation_Synthesis?style=social)](https://github.com/royorel/Lifespan_Age_Transformation_Synthesis) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.09764)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/royorel/FFHQ-Aging-Dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/pix2pixHD), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/style-based-gan-pytorch)</li><li>[project](https://grail.cs.washington.edu/projects/lifespan_age_transformation_synthesis/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/_jTFcjN2hBk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/9fulnt2_q_Y)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb/) | 31.10.2020 |
| HiGAN | Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis | <ul><li>[Ceyuan Yang](https://ceyuan.me/)</li> <li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Bolei Zhou](https://boleizhou.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/genforce/higan?style=social)](https://github.com/genforce/higan) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1911.09267), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6856), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.10112)</li><li>[project](https://genforce.github.io/higan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=X5yWu2Jwjpg)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/genforce/higan/blob/master/docs/HiGAN_Bedroom.ipynb/) | 14.10.2020 |
| InterFaceGAN | Interpreting the Latent Space of GANs for Semantic Face Editing | <ul><li>[Yujun Shen](https://shenyujun.github.io/)</li> <li>[Jinjin Gu](https://www.jasongt.com/)</li> <li>[Xiaoou Tang](https://www.ie.cuhk.edu.hk/people/xotang.shtml)</li> <li>[Bolei Zhou](https://boleizhou.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/genforce/interfacegan?style=social)](https://github.com/genforce/interfacegan) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.10786), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.09635), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tkarras/progressive_growing_of_gans), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan)</li><li>[project](https://genforce.github.io/interfacegan/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=uoftpl3Bj6w)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/genforce/interfacegan/blob/master/docs/InterFaceGAN.ipynb/) | 13.10.2020 |
| Faceswap-GAN | A minimum demo for faceswap-GAN v2.2 | [shaoanlu](https://shaoanlu.github.io/) | [![](https://img.shields.io/github/stars/shaoanlu/faceswap-GAN?style=social)](https://github.com/shaoanlu/faceswap-GAN)  | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/shaoanlu/faceswap-GAN/blob/master/colab_demo/faceswap-GAN_colab_demo.ipynb/) | 12.09.2020 |
| Instance-aware Image Colorization | Novel deep learning framework to achieve instance-aware colorization | [Jheng-Wei Su](https://github.com/ericsujw) | [![](https://img.shields.io/github/stars/ericsujw/InstColorization?style=social)](https://github.com/ericsujw/InstColorization) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.10825)</li><li>[project](https://ericsujw.github.io/InstColorization/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Zj1N4uE1ehk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb/) | 30.08.2020 |
| MoCo | Momentum Contrast for unsupervised visual representation learning | <ul><li>[Kaiming He](https://kaiminghe.github.io/)</li> <li>[Haoqi Fan](https://haoqifan.github.io/)</li> <li>[Yuxin Wu](https://ppwwyyxx.com/)</li><details><summary>others</summary><li>[Saining Xie](http://sainingxie.com/)</li> <li>[Ross Girshick](https://www.rossgirshick.info/)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1109/CVPR42600.2020.00975)](https://doi.org/10.1109/CVPR42600.2020.00975) [![](https://img.shields.io/github/stars/facebookresearch/moco?style=social)](https://github.com/facebookresearch/moco) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1911.05722), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.04297), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.02677)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ppwwyyxx/moco.tensorflow)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/LvHwBQF14zs), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/4VVGtYPM8JE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/o5Qh61dLDf0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb/) | 20.08.2020 |
| Rewriting a Deep Generative Model | We ask if a deep network can be reprogrammed to follow different rules, by enabling a user to directly change the weights, instead of training with a data set | <ul><li>[David Bau](https://people.csail.mit.edu/davidbau/home/)</li> <li>[Steven Liu](http://people.csail.mit.edu/stevenliu/)</li> <li>[Tongzhou Wang](https://ssnl.github.io/)</li><details><summary>others</summary><li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul></details> | [![](https://img.shields.io/github/stars/davidbau/rewriting?style=social)](https://github.com/davidbau/rewriting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.15646), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://rewriting.csail.mit.edu/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=i2_-zNqtEPk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://rewriting.csail.mit.edu/video/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/davidbau/rewriting/blob/master/notebooks/rewriting-interface.ipynb/) | 31.07.2020 |
| BERT score | An automatic evaluation metric for text generation | [Tianyi Zhang](https://tiiiger.github.io/) | [![](https://img.shields.io/github/stars/Tiiiger/bert_score?style=social)](https://github.com/Tiiiger/bert_score) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.09675)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q) | 17.07.2020 |
| SIREN | Implicit Neural Representations with Periodic Activation Functions | <ul><li>[Vincent Sitzmann](https://vsitzmann.github.io/)</li> <li>[Julien Martel](http://web.stanford.edu/~jnmartel/)</li></ul> | [![](https://img.shields.io/github/stars/vsitzmann/siren?style=social)](https://github.com/vsitzmann/siren) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[data](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K)</li><li>[project](https://vsitzmann.github.io/siren/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Q2fLWGBeaiI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/vsitzmann/siren/blob/master/explore_siren.ipynb/) | 24.06.2020 |
| PIFu | Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization | <ul><li>[Ryota Natsume](https://github.com/nanopoteto)</li> <li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Zeng Huang](https://zeng.science/)</li><details><summary>others</summary><li>[Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)</li> <li>[Hao Li](http://hao.li)</li></ul></details> | [![](https://img.shields.io/github/stars/shunsukesaito/PIFu?style=social)](https://github.com/shunsukesaito/PIFu) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.05172)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=S1FpjwKqtPs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY) | 17.06.2020 |
| 3D Ken Burns | A reference implementation of 3D Ken Burns Effect from a Single Image using PyTorch - given a single input image, it animates this still image with a virtual camera scan and zoom subject to motion parallax | [Manuel Romero](https://mrm8488.github.io/) | [![](https://img.shields.io/github/stars/sniklaus/3d-ken-burns?style=social)](https://github.com/sniklaus/3d-ken-burns) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1909.05483)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=WrajxHHfRBA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mrm8488/shared_colab_notebooks/blob/master/3D_Ken_Burns.ipynb/) | 13.06.2020 |
| HRFAE | An encoder-decoder architecture for face age editing | <ul><li>[Xu Yao](https://xu-yao.github.io/)</li> <li>[Gilles Puy](https://sites.google.com/site/puygilles/home)</li> <li>[Alasdair Newson](https://sites.google.com/site/alasdairnewson/)</li><details><summary>others</summary><li>[Yann Gousseau](https://gousseau.wp.imt.fr/)</li> <li>[Pierre Hellier](https://scholar.google.com/citations?user=U2BX6Q8AAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/InterDigitalInc/HRFAE?style=social)](https://github.com/InterDigitalInc/HRFAE) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.04410)</li><li>[data](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vadimkantorov/caffemodel2pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/InterDigitalInc/HRFAE/blob/master/test.ipynb/) | 14.05.2020 |
| Jukebox | A neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles | [Christine Payne](http://christinemcleavey.com/) | [![](https://img.shields.io/github/stars/openai/jukebox?style=social)](https://github.com/openai/jukebox) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.00341)</li><li>[blog post](https://openai.com/blog/jukebox)</li><li>[explorer](http://jukebox.openai.com/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb/) | 04.05.2020 |
| 3D Photo Inpainting | Method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view | <ul><li>[Meng-Li Shih](https://shihmengli.github.io/)</li> <li>[Shih-Yang Su](https://lemonatsu.github.io/)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://jbhuang0604.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/vt-vl-lab/3d-photo-inpainting?style=social)](https://github.com/vt-vl-lab/3d-photo-inpainting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.04727)</li><li>[project](https://shihmengli.github.io/3D-Photo-Inpainting/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz) | 04.05.2020 |
| Global Flow Local Attention | Differentiable global-flow local-attention framework to reassemble the inputs at the feature level | <ul><li>[Yurui Ren](https://renyurui.github.io/)</li> <li>[Xiaoming Yu](https://xiaoming-yu.github.io/)</li> <li>[Junming Chen](https://github.com/R-JunmingChen)</li><details><summary>others</summary><li>[Thomas Li](https://ieeexplore.ieee.org/author/37086497292)</li> <li>[Ge Li](https://ieeexplore.ieee.org/author/37085815762)</li></ul></details> | [![](https://img.shields.io/github/stars/RenYurui/Global-Flow-Local-Attention?style=social)](https://github.com/RenYurui/Global-Flow-Local-Attention) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.00696), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1605.03557)</li><li>[data](https://shapenet.org/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ondyari/FaceForensics), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/vid2vid), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tengteng95/Pose-Transfer)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://gt3rs.medium.com/compile-with-nvcc-3566fbdfdbf)</li><li>[project](https://renyurui.github.io/GFLA-web/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Ju0hBzCwsyU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/RenYurui/Global-Flow-Local-Attention/blob/master/demo.ipynb/) | 30.04.2020 |
| Motion Supervised co-part Segmentation | A self-supervised deep learning method for co-part segmentation | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Subhankar Roy](https://github.com/roysubhankar)</li></ul> | [![](https://img.shields.io/github/stars/AliaksandrSiarohin/motion-cosegmentation?style=social)](https://github.com/AliaksandrSiarohin/motion-cosegmentation) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2004.03234)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/video-preprocessing)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RJ4Nj1wV5iA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/AliaksandrSiarohin/motion-cosegmentation/blob/master/part_swap.ipynb/) | 07.04.2020 |
| Onsets and Frames | Onsets and Frames is an automatic music transcription framework with piano and drums models | <ul><li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Erich Elsen](https://github.com/ekelsen)</li></ul> | [![](https://img.shields.io/github/stars/magenta/magenta?style=social)](https://github.com/magenta/magenta/tree/main/magenta/models/onsets_frames_transcription) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.11153), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.12247), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00188)</li><li>[blog post](http://g.co/magenta/onsets-frames)</li><li>[data](https://g.co/magenta/maestro-wave2midi2wave), [data](https://magenta.tensorflow.org/datasets/e-gmd)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/magenta/onsets_frames_transcription/onsets_frames_transcription.ipynb/) | 02.04.2020 |
| WikiArt (stylegan2) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Norod/my-colab-experiments/blob/master/WikiArt_Example_Generation_By_Peter_Baylies.ipynb/) | 27.01.2020 |
| Siamese NN | Implementation of Siamese Neural Networks built upon multihead attention mechanism for text semantic similarity task | [Tomasz Latkowski](https://github.com/tlatkowski) | [![](https://img.shields.io/github/stars/tlatkowski/multihead-siamese-nets?style=social)](https://github.com/tlatkowski/multihead-siamese-nets) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.14599)</li><li>[data](https://nlp.stanford.edu/projects/snli/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/anli/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/quora-question-pairs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tlatkowski/multihead-siamese-nets/blob/master/colab/multihead_siamese_nets.ipynb/) | 19.12.2019 |
| Generating Piano Music with Transformer | This Colab notebook lets you play with pretrained Transformer models for piano music generation, based on the Music Transformer | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Anna Huang](https://github.com/czhuang)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.04281)</li><li>[blog post](http://g.co/magenta/music-transformer)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb/) | 16.09.2019 |
| GMCNN | Generative Multi-column Convolutional Neural Networks inpainting model in Keras | [Tomasz Latkowski](https://github.com/tlatkowski) | [![](https://img.shields.io/github/stars/tlatkowski/inpainting-gmcnn-keras?style=social)](https://github.com/tlatkowski/inpainting-gmcnn-keras) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.08771)</li><li>[data](http://places2.csail.mit.edu/download.html), [data](https://nv-adlr.github.io/publication/partialconv-inpainting)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tlatkowski/inpainting-gmcnn-keras/blob/master/colab/Image_Inpainting_with_GMCNN_model.ipynb/) | 09.08.2019 |
| BERT with TPU | Using a free Colab Cloud TPU to fine-tune sentence and sentence-pair classification tasks built on top of pretrained BERT models and run predictions on tuned model | [Sourabh Bajaj](https://sourabhbajaj.com/) | <ul><li>[TPU quickstart](https://cloud.google.com/tpu/docs/quickstart)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb/) | 29.03.2019 |
| GANSynth | This notebook is a demo GANSynth, which generates audio with Generative Adversarial Networks | [Jesse Engel](https://github.com/jesseengel) | [![](https://img.shields.io/github/stars/magenta/magenta?style=social)](https://github.com/magenta/magenta/tree/main/magenta/models/gansynth) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[project](https://storage.googleapis.com/magentadata/papers/gansynth/index.html)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb/) | 25.02.2019 |
| Latent Constraints | Conditional Generation from Unconditional Generative Models | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Matthew Hoffman](http://matthewdhoffman.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05772)</li><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/latent_constraints/latentconstraints.ipynb/) | 27.11.2017 |
| Performance RNN | This notebook shows you how to generate new performed compositions from a trained model | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Sageev Oore](https://github.com/osageev)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | [![](https://img.shields.io/github/stars/magenta/magenta?style=social)](https://github.com/magenta/magenta/tree/master/magenta/models/performance_rnn) <ul><li>[blog post](https://magenta.tensorflow.org/performance-rnn)</li><li>[data](http://www.piano-e-competition.com/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb/) | 11.07.2017 |
| NSynth | This colab notebook has everything you need to upload your own sounds and use NSynth models to reconstruct and interpolate between them | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Cinjon Resnick](https://github.com/cinjon)</li> <li>[Adam Roberts](https://github.com/adarob)</li><details><summary>others</summary><li>[Sander Dieleman](https://benanne.github.io/)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li> <li>[Mohammad Norouzi](https://norouzi.github.io/)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul></details> | [![](https://img.shields.io/github/stars/tensorflow/magenta?style=social)](https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1704.01279)</li><li>[blog post](https://magenta.tensorflow.org/nsynth)</li><li>[data](https://magenta.tensorflow.org/datasets/nsynth)</li><li>[tutorial](https://magenta.tensorflow.org/nsynth-fastgen)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=AaALLWQmCdI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=BOoSy-Pg8is)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb/) | 06.04.2017 |
## Tutorials
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| Nerfstudio | API that allows for a simplified end-to-end process of creating, training, and testing NeRFs | <ul><li>[Matthew Tancik](https://github.com/tancik)</li> <li>[Ethan Weber](https://ethanweber.me/)</li> <li>[Evonne Ng](http://people.eecs.berkeley.edu/~evonne_ng/)</li><details><summary>others</summary><li>[Ruilong Li](http://www.liruilong.cn/)</li> <li>[Brent Yi](https://github.com/brentyi)</li> <li>[Justin Kerr](https://kerrj.github.io/)</li> <li>[Terrance Wang](https://github.com/terrancewang)</li> <li>[Alexander Kristoffersen](https://akristoffersen.com/)</li> <li>[Jake Austin](https://github.com/jake-austin)</li> <li>[Kamyar Salahi](https://github.com/TheQuantumFractal)</li> <li>[Abhik Ahuja](https://abhikahuja.com/)</li> <li>[David McAllister](https://github.com/mcallisterdavid)</li> <li>[Angjoo Kanazawa](https://github.com/akanazawa)</li></ul></details> | [![](https://img.shields.io/github/stars/nerfstudio-project/nerfstudio?style=social)](https://github.com/nerfstudio-project/nerfstudio) <ul><li>[Viewer](https://viewer.nerf.studio/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2302.04264)</li><li>[<img src="images/discord.svg" alt="discord" height=20/>](https://discord.gg/uMbNqcraFc)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.nerf.studio/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/tiny-cuda-nn)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/nerfstudioteam)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/XwKq7qDQCQk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nSFsugarWzk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/h5EWiRRxYEQ), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/8cv9G7izdPY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb/) | 31.05.2023 |
| Stable Diffusion 2 | New stable diffusion model at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses OpenCLIP-ViT/H as the text encoder and is trained from scratch | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Andreas Blattmann](https://github.com/ablattmann)</li> <li>[Dominik Lorenz](https://github.com/qp-qp)</li><details><summary>others</summary><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li> <li>[qunash](https://github.com/qunash)</li></ul></details> | [![](https://img.shields.io/github/stars/Stability-AI/stablediffusion?style=social)](https://github.com/Stability-AI/stablediffusion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10752), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.00512), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.02502), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.01073), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.09778), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2206.00927)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/qunash/stable-diffusion-2-gui), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/isl-org/MiDaS), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/denoising-diffusion-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/runwayml/stable-diffusion/blob/main/scripts/inpaint_st.py), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/crowsonkb/k-diffusion)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/stabilityai/stable-diffusion-2-1), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/stabilityai/stable-diffusion-2-1-base), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/stabilityai/stable-diffusion-2-depth), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/HytucGhwTRs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/qunash/stable-diffusion-2-gui/blob/main/stable_diffusion_2_0.ipynb/) | 26.05.2023 |
| SoftVC VITS | Singing Voice Conversion | [svc develop team](https://github.com/svc-develop-team) | [![](https://img.shields.io/github/stars/svc-develop-team/so-vits-svc?style=social)](https://github.com/svc-develop-team/so-vits-svc) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NaruseMioShirakana/MoeVoiceStudio), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/auspicious3000/contentvec), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/yxlllc/DDSP-SVC), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/flutydeer/audio-slicer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openvpi/audio-slicer)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel/tree/main)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb/) | 26.05.2023 |
| Detectron2 | FAIR's next-generation platform for object detection and segmentation | [Yuxin Wu](http://ppwwyyxx.com/) | [![](https://img.shields.io/github/stars/facebookresearch/detectron2?style=social)](https://github.com/facebookresearch/detectron2) <ul><li>[blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://detectron2.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) | 26.05.2023 |
| Anomalib | Deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets | <ul><li>[Samet Akcay](https://github.com/samet-akcay)</li> <li>[Dick Ameln](https://github.com/djdameln)</li> <li>[Ashwin Vaidya](https://ashwinvaidya.com/)</li><details><summary>others</summary><li>[Barath Lakshmanan](https://github.com/blakshma)</li> <li>[Nilesh Ahuja](https://github.com/nahuja-intel)</li> <li>[Utku Genc](https://github.com/ugenc-intel)</li></ul></details> | [![](https://img.shields.io/github/stars/openvinotoolkit/anomalib?style=social)](https://github.com/openvinotoolkit/anomalib) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08785)</li><li>[data](https://www.mvtec.com/company/research/datasets/mvtec-ad)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://openvinotoolkit.github.io/anomalib/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/vnk8071/anomaly-detection-in-industry-manufacturing/tree/master/anomalib_contribute)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/lib/timm)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/openvinotoolkit/anomalib/blob/main/notebooks/000_getting_started/001_getting_started.ipynb/) | 24.05.2023 |
| Deforum Stable Diffusion | Open source project is designed to be free to use and easy to modify for custom needs and pipelines | <ul><li>[EnzymeZoo](https://linktr.ee/enzymezoo)</li> <li>[Артем Храпов](https://github.com/kabachuha)</li> <li>[Forest Star Walz](https://github.com/reallybigname)</li> <li>[pharmapsychotic](https://github.com/pharmapsychotic)</li></ul> | [![](https://img.shields.io/github/stars/deforum-art/deforum-stable-diffusion?style=social)](https://github.com/deforum-art/deforum-stable-diffusion) <ul><li>[<img src="images/discord.svg" alt="discord" height=20/>](https://discord.gg/deforum)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.google.com/document/d/1RrQv7FntzOuLg4ohjRZPVL7iptIyBhwwbcEYEW2OfcI)</li><li>[project](https://deforum.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/w_sxuDMt_V0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/bicPayZDI60), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/dqkQo2alZvU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deforum-art/deforum-stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb/) | 18.05.2023 |
| Building Your Own Federated Learning Algorithm | We discuss how to implement federated learning algorithms without deferring to the tff.learning API | [Zachary Charles](https://zachcharles.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.08610)</li><li>[blog post](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb/) | 18.05.2023 |
| Federated Learning for Image Classification | We use the classic MNIST training example to introduce the Federated Learning API layer of TFF, tff.learning - a set of higher-level interfaces that can be used to perform common types of federated learning tasks, such as federated training, against user-supplied models implemented in TensorFlow | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data](https://www.nist.gov/srd/nist-special-database-19)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb/) | 18.05.2023 |
| Federated Learning for Text Generation | We start with a RNN that generates ASCII characters, and refine it via federated learning | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.01097), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt), [data](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb/) | 18.05.2023 |
| Custom Federated Algorithms, Part 1: Introduction to the Federated Core | This tutorial is the first part of a two-part series that demonstrates how to implement custom types of federated algorithms in TensorFlow Federated using the Federated Core - a set of lower-level interfaces that serve as a foundation upon which we have implemented the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb/) | 18.05.2023 |
| Custom Federated Algorithms, Part 2: Implementing Federated Averaging | This tutorial is the second part of a two-part series that demonstrates how to implement custom types of federated algorithms in TFF using the Federated Core, which serves as a foundation for the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | [![](https://img.shields.io/github/stars/tensorflow/federated?style=social)](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py) <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_2.ipynb/) | 18.05.2023 |
| TFF for Federated Learning Research: Model and Update Compression | We use the EMNIST dataset to demonstrate how to enable lossy compression algorithms to reduce communication cost in the Federated Averaging algorithm | [Weikang Song](https://github.com/swkpku) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[tensor encoding](http://jakubkonecny.com/files/tensor_encoding.pdf)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/tff_for_federated_learning_research_compression.ipynb/) | 18.05.2023 |
| High-performance simulations with TFF | This tutorial will describe how to setup high-performance simulations with TFF in a variety of common scenarios | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/simulations.ipynb/) | 18.05.2023 |
| Deep RL Course | The Hugging Face Deep Reinforcement Learning Course | <ul><li>[Thomas Simonini](https://www.simoninithomas.com/)</li> <li>[Omar Sanseviero](https://osanseviero.github.io/hackerllama/)</li> <li>[Sayak Paul](https://sayak.dev/)</li></ul> | [![](https://img.shields.io/github/stars/huggingface/deep-rl-class?style=social)](https://github.com/huggingface/deep-rl-class) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/alex-petrenko/sample-factory)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/deep-rl-course/unit0/introduction), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)</li><li>[syllabus](https://simoninithomas.github.io/deep-rl-course)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/2GwBez0D20A), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/CsuIANBnSq8), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/AQKAOXJa6qg)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb/) | 16.05.2023 |
| MMAction2 | An open-source toolbox for video understanding based on PyTorch | [MMAction2 Contributors](https://openmmlab.com/aboutus) | [![](https://img.shields.io/github/stars/open-mmlab/mmaction2?style=social)](https://github.com/open-mmlab/mmaction2) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.13230), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.10161), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17263), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.13586), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.05095), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.13042)</li><li>[data](https://sdolivia.github.io/FineGym/), [data](http://www.svcl.ucsd.edu/projects/resound/dataset.html), [data](https://research.google.com/ava/index.html), [data](https://www.deepmind.com/open-source/kinetics)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://mmaction2.readthedocs.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/open-mmlab/mmcv), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/SwinTransformer/Video-Swin-Transformer), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Cogito2012/DEAR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xvjiarui/VFS), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/holistic-video-understanding/HVU-Dataset)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb/) | 15.05.2023 |
| Ray | Unified framework for scaling AI and Python applications | <ul><li>[Philipp Moritz](https://github.com/pcmoritz)</li> <li>[Robert Nishihara](https://github.com/robertnishihara)</li> <li>[Stephanie Wang](https://stephanie-wang.github.io/)</li><details><summary>others</summary><li>[Alexey Tumanov](https://faculty.cc.gatech.edu/~atumanov/)</li> <li>[Richard Liaw](https://github.com/richardliaw)</li> <li>[Eric Liang](https://github.com/ericl)</li> <li>[Melih Elibol](https://research.nvidia.com/person/melih-elibol)</li> <li>[Zongheng Yang](https://zongheng.me/)</li> <li>[William Paul](https://github.com/Wapaul1)</li> <li>[Michael Jordan](https://people.eecs.berkeley.edu/~jordan/)</li> <li>[Ion Stoica](https://people.eecs.berkeley.edu/~istoica/)</li></ul></details> | [![](https://img.shields.io/github/stars/ray-project/ray?style=social)](https://github.com/ray-project/ray) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.05889), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.05072), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.09381), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.05118), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.03924)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.ray.io/en/latest/index.html)</li><li>[website](https://www.ray.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/LmROEotKhJA), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/uzt-CwohQC8), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/XME90SGL6Vs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb/) | 08.05.2023 |
| Python Data Science Handbook | Jupyter notebook version of the Python Data Science Handbook by Jake VanderPlas | [Jake Vanderplas](http://vanderplas.com/) | [![](https://img.shields.io/github/stars/jakevdp/PythonDataScienceHandbook?style=social)](https://github.com/jakevdp/PythonDataScienceHandbook) <ul><li>[project](https://jakevdp.github.io/PythonDataScienceHandbook/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb/) | 05.05.2023 |
| PGMax | General factor graphs for discrete probabilistic graphical models, and hardware-accelerated differentiable loopy belief propagation in JAX | <ul><li>[Guangyao Zhou](https://stanniszhou.github.io/)</li> <li>[Nishanth Kumar](http://nishanthjkumar.com/)</li> <li>[Antoine Dedieu](https://github.com/antoine-dedieu)</li><details><summary>others</summary><li>[Miguel Lázaro-Gredilla](https://www.tsc.uc3m.es/~miguel/)</li> <li>[Shrinu Kushagra](https://cs.uwaterloo.ca/~skushagr/)</li> <li>[Dileep George](https://dileeplearning.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/PGMax?style=social)](https://github.com/deepmind/PGMax) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.04110)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Belief_propagation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/PGMax/blob/main/examples/rcn.ipynb/) | 05.05.2023 |
| MyoSuite | A collection of musculoskeletal environments and tasks simulated with the MuJoCo physics engine and wrapped in the OpenAI gym API to enable the application of Machine Learning to bio-mechanic control problems | <ul><li>[Vittorio Caggiano](https://github.com/Vittorio-Caggiano)</li> <li>[Huawei Wang](https://huaweiwang.github.io/)</li> <li>[Guillaume Durandau](https://people.utwente.nl/g.v.durandau)</li><details><summary>others</summary><li>[Massimo Sartori](https://people.utwente.nl/m.sartori)</li> <li>[Vikash Kumar](https://vikashplus.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/myosuite?style=social)](https://github.com/facebookresearch/myosuite) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.13600)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://myosuite.readthedocs.io/en/latest/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1U6vo6Q_rPhDaq6oUMV7EAZRm6s0fD1wn) | 29.04.2023 |
| StableLM | Stability AI Language Models | [Stability AI](https://stability.ai/careers) | [![](https://img.shields.io/github/stars/Stability-AI/StableLM?style=social)](https://github.com/Stability-AI/StableLM) <ul><li>[blog post](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/llama), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tatsu-lab/stanford_alpaca), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/nomic-ai/gpt4all), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/databrickslabs/dolly), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/anthropics/hh-rlhf), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ggerganov/llama.cpp)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/lmsys/vicuna-13b-delta-v0), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/datasets/RyokoAI/ShareGPT52K), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/stabilityai)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/dypPSs4t77g), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nWf1StvtoRw), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Hg-s2RTaTFE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/qXtJjoEfTnA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Stability-AI/StableLM/blob/main/notebooks/stablelm-alpha.ipynb/) | 27.04.2023 |
| DeepFloyd IF | State-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding | <ul><li>[Alex Shonenkov](https://linktr.ee/shonenkovAI)</li> <li>[Misha Konstantinov](https://github.com/zeroshot-ai)</li> <li>[Daria Bakshandaeva](https://github.com/Gugutse)</li><details><summary>others</summary><li>[Christoph Schuhman](http://christoph-schuhmann.de/)</li> <li>[Ksenia Ivanova](https://github.com/ivksu)</li> <li>[Nadiia Klokova](https://github.com/vauimpuls)</li></ul></details> | [![](https://img.shields.io/github/stars/deep-floyd/IF?style=social)](https://github.com/deep-floyd/IF) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.11487)</li><li>[<img src="images/discord.svg" alt="discord" height=20/>](https://discord.gg/umz62Mgr)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/DeepFloyd), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-speed), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-memory), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/blog/if), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/docs/diffusers/main/en/api/pipelines/if)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/deepfloydai)</li><li>[website](https://deepfloyd.ai/deepfloyd-if)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/4Zkipll5Rjc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/tq5ZXZWwTPA), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/rLtfd1TvYJk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb/) | 27.04.2023 |
| TTS | A library for advanced Text-to-Speech generation, built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality | <ul><li>[Eren Gölge](https://github.com/erogol)</li> <li>[Aya-AlJafari](https://github.com/Aya-AlJafari)</li> <li>[Edresson Casanova](https://github.com/Edresson)</li><details><summary>others</summary><li>[Josh Meyer](http://jrmeyer.github.io/)</li> <li>[Kelly Davis](https://github.com/kdavis-coqui)</li> <li>[Reuben Morais](https://github.com/reuben)</li></ul></details> | [![](https://img.shields.io/github/stars/coqui-ai/TTS?style=social)](https://github.com/coqui-ai/TTS) <ul><li>[blog post](https://coqui.ai/blog/tts/solving-attention-problems-of-tts-models-with-double-decoder-consistency)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://tts.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/coqui-ai/TTS-papers)</li><li>[samples](https://erogol.github.io/ddc-samples/)</li><li>[website](https://coqui.ai/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ADnBCz0Wd1U), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Yglxf2WbkLU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/alpI-DnVlO0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/coqui-ai/TTS/blob/dev/notebooks/Tutorial_2_train_your_first_TTS_model.ipynb/) | 26.04.2023 |
| highway-env | A collection of environments for autonomous driving and tactical decision-making tasks | [Edouard Leurent](https://edouardleurent.com/) | [![](https://img.shields.io/github/stars/eleurent/highway-env?style=social)](https://github.com/eleurent/highway-env) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.03483), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.05701), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.07140)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://highway-env.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/rl-agents), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/finite-mdp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/baselines/tree/master/baselines/her)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/eleurent/highway-env/blob/master/scripts/parking_model_based.ipynb/) | 22.04.2023 |
| dm_control | DeepMind Infrastructure for Physics-Based Simulation | <ul><li>[Saran Tunyasuvunakool](https://github.com/saran-t)</li> <li>[Alistair Muldal](https://github.com/alimuldal)</li> <li>[Yotam Doron](http://www.yotamdoron.com/)</li><details><summary>others</summary><li>[Siqi Liu](http://siqi.fr/)</li> <li>[Steven Bohez](https://github.com/sbohez)</li> <li>[Josh Merel](https://sites.google.com/site/jsmerel/)</li> <li>[Tom Erez](https://github.com/erez-tom)</li> <li>[Timothy Lillicrap](https://contrastiveconvergence.net/~timothylillicrap/index.php)</li> <li>[Nicolas Heess](https://scholar.google.com/citations?user=79k7bGEAAAAJ)</li> <li>[Yuval Tassa](https://github.com/yuvaltassa)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/dm_control?style=social)](https://github.com/deepmind/dm_control) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.12983), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.00690), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1902.07151), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02286), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.09564), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.10567)</li><li>[blog post](https://www.deepmind.com/publications/dm-control-software-and-tasks-for-continuous-control)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Tippe_top)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/CMjoiU482Jk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/rAai4QzcYbs), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/WhaRsrlaXLk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/dm_control/blob/master/tutorial.ipynb/) | 20.04.2023 |
| MuJoCo | A general purpose physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, machine learning, and other areas which demand fast and accurate simulation of articulated structures interacting with their environment | <ul><li>[Emo Todorov](https://homes.cs.washington.edu/~todorov/)</li> <li>[Tom Erez](https://github.com/erez-tom)</li> <li>[Yuval Tassa](https://github.com/yuvaltassa)</li></ul> | [![](https://img.shields.io/github/stars/deepmind/mujoco?style=social)](https://github.com/deepmind/mujoco) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.12983)</li><li>[blog post](https://www.deepmind.com/blog/opening-up-a-physics-simulator-for-robotics), [blog post](https://www.deepmind.com/blog/open-sourcing-mujoco)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://mujoco.readthedocs.io/en/latest/overview.html)</li><li>[website](https://mujoco.org/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Tippe_top), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Chaos_theory), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/3D_projection#Mathematical_formula)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/0ORsj_E17B0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/yHZVVfsJ8mc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/eyzzsGJ1iic)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/dm_control/blob/master/dm_control/mujoco/tutorial.ipynb/) | 20.04.2023 |
| Composer | PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy | [The Mosaic ML Team](https://www.mosaicml.com/team) | [![](https://img.shields.io/github/stars/mosaicml/composer?style=social)](https://github.com/mosaicml/composer) <ul><li>[app](https://app.mosaicml.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.05924), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2002.04688)</li><li>[blog post](https://www.mosaicml.com/blog/5-best-practices-for-efficient-model-training)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](http://docs.mosaicml.com/)</li><li>[<img src="images/slack.svg" alt="slack" height=20/>](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/mosaicml)</li><li>[website](https://www.mosaicml.com/composer)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Amdahl's_law)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/@mosaicml6047/videos), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/n-1WV5QdIDc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Xi_5wq2MpOw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb/) | 19.04.2023 |
| OpenCLIP | An open source implementation of CLIP | <ul><li>[Ross Wightman](https://rwightman.com/)</li> <li>[Cade Gordon](https://cadegordon.io/)</li> <li>[Vaishaal Shankar](http://vaishaal.com/)</li></ul> | [![](https://img.shields.io/github/stars/mlfoundations/open_clip?style=social)](https://github.com/mlfoundations/open_clip) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.01903), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.02114), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.04649), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1902.10811), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.04649)</li><li>[data](https://ai.google.com/research/ConceptualCaptions/download), [data](https://laion.ai/blog/laion-5b/), [data](https://laion.ai/blog/laion-400-open-dataset/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mlfoundations/wise-ft), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/webdataset/webdataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/webdataset/tarp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google-research-datasets/conceptual-12m)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/datasets/laion/laion2B-en), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb/) | 16.04.2023 |
| Stable Baselines3 | Set of reliable implementations of reinforcement learning algorithms in PyTorch | <ul><li>[Antonin Raffin](https://araffin.github.io/)</li> <li>[Ashley Hill](https://hill-a.me/)</li> <li>[Adam Gleave](https://www.gleave.me/)</li><details><summary>others</summary><li>[Anssi Kanervisto](https://github.com/Miffyli)</li> <li>[Maximilian Ernestus](https://github.com/ernestum)</li> <li>[Noah Dormann](https://github.com/ndormann)</li></ul></details> | [![](https://img.shields.io/github/stars/DLR-RM/stable-baselines3?style=social)](https://github.com/DLR-RM/stable-baselines3) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://stable-baselines3.readthedocs)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hill-a/stable-baselines), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/gym/wiki/Environments)</li><li>[paper](https://jmlr.org/papers/v22/20-1364.html)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/reinforcementlearning/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb/) | 14.04.2023 |
| RL Baselines3 Zoo | Training Framework for Stable Baselines3 Reinforcement Learning Agents | [Antonin Raffin](https://araffin.github.io/) | [![](https://img.shields.io/github/stars/DLR-RM/rl-baselines3-zoo?style=social)](https://github.com/DLR-RM/rl-baselines3-zoo) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05719)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://stable-baselines3.readthedocs.io/en/master/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/DLR-RM/rl-baselines3-zoo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/roboschool), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Farama-Foundation/Minigrid)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/sb3)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb/) | 14.04.2023 |
| Petals | Run 100B+ language models at home, BitTorrent-style | [BigScience](https://bigscience.huggingface.co/) | [![](https://img.shields.io/github/stars/bigscience-workshop/petals?style=social)](https://github.com/bigscience-workshop/petals) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.01188), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.07258)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/borzunov/chat.petals.ml), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/timDettmers/bitsandbytes)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/bigscience/bloom)</li><li>[project](https://petals.ml/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/BitTorrent)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ) | 12.04.2023 |
| SentencePiece | An unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training | <ul><li>[Taku Kudo](http://chasen.org/~taku/)</li> <li>[John Richardson](https://scholar.google.com/citations?user=PEvmYfgAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/google/sentencepiece?style=social)](https://github.com/google/sentencepiece) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1808.06226), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.07909), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1804.10959), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.13267), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1609.08144)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rsennrich/subword-nmt), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/gperftools/gperftools), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Microsoft/vcpkg)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/U51ranzJBpY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb/) | 08.04.2023 |
| Transformer | This tutorial trains a Transformer model to translate Portuguese to English | [Billy Lamberta](https://github.com/lamberta) | [![](https://img.shields.io/github/stars/neulab/word-embeddings-for-nmt?style=social)](https://github.com/neulab/word-embeddings-for-nmt) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.03878)</li><li>[link](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb/) | 07.04.2023 |
| Brax | A differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators | <ul><li>[Daniel Freeman](https://github.com/cdfreeman-google)</li> <li>[Erik Frey](https://fawx.com/)</li> <li>[Anton Raichuk](https://scholar.google.com/citations?user=fquIpvgAAAAJ)</li><details><summary>others</summary><li>[Sertan Girgin](https://sites.google.com/site/girgint/home)</li> <li>[Igor Mordatch](https://scholar.google.com/citations?user=Vzr1RukAAAAJ)</li> <li>[Olivier Bachem](http://olivierbachem.ch/)</li></ul></details> | [![](https://img.shields.io/github/stars/google/brax?style=social)](https://github.com/google/brax) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.13281)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://neurips.cc/Conferences/2021/CallForDatasetsBenchmarks)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/brax/blob/main/notebooks/basics.ipynb/) | 30.03.2023 |
| TorchGeo | PyTorch domain library that provides datasets, transforms, samplers, and pre-trained models specific to geospatial data | <ul><li>[Adam Stewart](https://github.com/adamjstewart)</li> <li>[Caleb Robinson](https://calebrob.com/)</li> <li>[Isaac Corley](https://github.com/isaaccorley)</li><details><summary>others</summary><li>[Anthony Ortiz](https://github.com/anthonymlortiz)</li> <li>[Juan Lavista Ferres](https://www.microsoft.com/en-us/research/people/jlavista/)</li> <li>[Arindam Banerjee](https://arindam.cs.illinois.edu/)</li></ul></details> | [![](https://img.shields.io/github/stars/microsoft/torchgeo?style=social)](https://github.com/microsoft/torchgeo) <ul><li>[NDBI](https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri/)</li><li>[NDVI](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/)</li><li>[NDWI](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.08872)</li><li>[data](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/), [data](https://www.cogeo.org/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/davemlz/awesome-spectral-indices)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/microsoft/torchgeo/blob/main/docs/tutorials/indices.ipynb/) | 29.03.2023 |
| LAVIS | Python deep learning library for LAnguage-and-VISion intelligence research and applications | <ul><li>[Dongxu Li](https://github.com/dxli94)</li> <li>[Junnan Li](https://github.com/LiJunnan1992)</li> <li>[Hung Le](https://sites.google.com/view/henryle2018/home)</li><details><summary>others</summary><li>[Guangsen Wang](https://github.com/guangsen-wang)</li> <li>[Silvio Savarese](https://scholar.google.com/citations?user=ImpbxLsAAAAJ)</li> <li>[Steven Hoi](https://sites.google.com/view/stevenhoi)</li></ul></details> | [![](https://img.shields.io/github/stars/salesforce/LAVIS?style=social)](https://github.com/salesforce/LAVIS) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2209.09019), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2305.06500), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2301.12597), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2212.10846), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2210.08773)</li><li>[blog post](https://blog.salesforceairesearch.com/lavis-language-vision-library/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://opensource.salesforce.com/LAVIS//latest/index.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Merlion)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb/) | 24.03.2023 |
| Hello, many worlds | This tutorial shows how a classical neural network can learn to correct qubit calibration errors | [Michael Broughton](https://github.com/MichaelBroughton) | <ul><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/quantum/api_docs/python/tfq/layers), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/quantum/api_docs/python/tfq/get_expectation_op), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/keras/functional)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pauli_matrices)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-o9AhIz1uvo)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/quantum/blob/master/docs/tutorials/hello_many_worlds.ipynb/) | 20.03.2023 |
| Image segmentation | This tutorial focuses on the task of image segmentation, using a modified U-Net | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](https://www.robots.ox.ac.uk/~vgg/data/pets/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/carvana-image-masking-challenge/overview)</li><li>[u-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb/) | 17.03.2023 |
| Tzer | Coverage-Guided Tensor Compiler Fuzzing with Joint IR-Pass Mutation | <ul><li>[Jiawei Liu](https://jiawei-site.github.io/)</li> <li>[Yuxiang Wei](https://yuxiang.cs.illinois.edu/)</li> <li>[Sen Yang](https://github.com/syang-ng)</li><details><summary>others</summary><li>[Yinlin Deng](https://dengyinlin.github.io/)</li> <li>[Lingming Zhang](http://lingming.cs.illinois.edu/)</li></ul></details> | [![](https://img.shields.io/github/stars/ise-uiuc/tzer?style=social)](https://github.com/ise-uiuc/tzer) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.09947)</li><li>[docker](https://hub.docker.com/repository/docker/tzerbot/oopsla)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://tzer.readthedocs.io/en/latest/index.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ganler/memcov)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ise-uiuc/tzer/blob/main/bug-report.ipynb/) | 09.03.2023 |
| Pix2Pix | This notebook demonstrates image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1611.07004)</li><li>[data](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb/) | 09.03.2023 |
| Image classification | This tutorial shows how to classify images of flowers | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb/) | 05.03.2023 |
| Haiku | A library built on top of JAX designed to provide simple, composable abstractions for machine learning research | <ul><li>[Tom Hennigan](https://github.com/tomhennigan)</li> <li>[Trevor Cai](https://github.com/trevorcai)</li> <li>[Tamara Norman](https://github.com/tamaranorman)</li> <li>[Igor Babuschkin](https://www.babushk.in/)</li></ul> | [![](https://img.shields.io/github/stars/deepmind/dm-haiku?style=social)](https://github.com/deepmind/dm-haiku) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://dm-haiku.readthedocs.io/en/latest/)</li><li>[website](https://www.haiku-os.org/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/dm-haiku/blob/main/examples/haiku_lstms.ipynb/) | 02.03.2023 |
| Data augmentation | This tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random transformations such as image rotation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/data-augmentation)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/tf_flowers)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Data_augmentation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb/) | 02.03.2023 |
| The Autodiff Cookbook | You'll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics | <ul><li>[Alex Wiltschko](https://github.com/alexbw)</li> <li>[Matthew Johnson](http://people.csail.mit.edu/mattjj/)</li></ul> | [![](https://img.shields.io/github/stars/google/jax?style=social)](https://github.com/google/jax/issues/446#issuecomment-467105048) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1406.2572), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.04454), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.03451), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.07062)</li><li>[book](https://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html), [book](https://mitpress.mit.edu/books/functional-differential-geometry)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/jax#auto-vectorization-with-vmap), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hips/autograd)</li><li>[tutorial](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Truncated_Newton_method), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Holomorphic_function), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb/) | 01.03.2023 |
| Simple audio recognition | This tutorial will show you how to build a basic speech recognition network that recognizes ten different words | [Google](https://www.tensorflow.org/) | <ul><li>[coursera](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/speech-recognition)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/speech_commands)</li><li>[tf.js](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb/) | 25.02.2023 |
| normflows | PyTorch implementation of discrete normalizing flows | <ul><li>[Vincent Stimper](https://is.mpg.de/person/vstimper)</li> <li>[David Liu](https://davindicode.github.io/)</li> <li>[Andrew Campbell](https://github.com/andrew-cr)</li><details><summary>others</summary><li>[Vincent Berenz](http://vincentberenz.is.tuebingen.mpg.de/)</li> <li>[Lukas Ryll](https://github.com/lukasryll)</li> <li>[Bernhard Schölkopf](https://scholar.google.com/citations?user=DZ-fHPgAAAAJ)</li> <li>[José Miguel Hernández-Lobato](https://jmhl.org/)</li></ul></details> | [![](https://img.shields.io/github/stars/VincentStimper/normalizing-flows?style=social)](https://github.com/VincentStimper/normalizing-flows) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2302.12014)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://vincentstimper.github.io/normalizing-flows/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/VincentStimper/resampled-base-flows), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/VincentStimper/hmc-hyperparameter-tuning)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Von_Mises_distribution)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/VincentStimper/normalizing-flows/blob/master/examples/paper_example_nsf_colab.ipynb/) | 24.02.2023 |
| SAHI | A lightweight vision library for performing large scale object detection & instance segmentation | <ul><li>[Fatih Cagatay Akyon](https://github.com/fcakyon)</li> <li>[Sinan Onur ALTINUÇ](https://github.com/sinanonur)</li> <li>[Alptekin Temizel](https://blog.metu.edu.tr/atemizel/)</li><details><summary>others</summary><li>[Cemil Cengiz](https://scholar.google.com/citations?user=1Ull07EAAAAJ)</li> <li>[Devrim Çavuşoğlu](https://github.com/devrimcavusoglu)</li> <li>[Kadir Şahin](https://github.com/ssahinnkadir)</li> <li>[Oğulcan Eryüksel](https://github.com/oulcan)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1109/ICIP46576.2022.9897990)](https://doi.org/10.1109/ICIP46576.2022.9897990) [![](https://img.shields.io/github/stars/obss/sahi?style=social)](https://github.com/obss/sahi) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.06934)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/fcakyon/small-object-detection-benchmark)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/remekkinas/sahi-slicing-aided-hyper-inference-yv5-and-yx)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/codable/sahi-a-vision-library-for-performing-sliced-inference-on-large-images-small-objects-c8b086af3b80), [<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/codable/convert-any-dataset-to-coco-object-detection-format-with-sahi-95349e1fe2b7)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb/) | 23.02.2023 |
| AmpliGraph | A suite of neural machine learning models for relational Learning, a branch of machine learning that deals with supervised learning on knowledge graphs | <ul><li>[Luca Costabello](https://luca.costabello.info/)</li> <li>[Adrianna Janik](https://github.com/adrijanik)</li> <li>[Chan Le Van](https://github.com/chanlevan)</li><details><summary>others</summary><li>[Nicholas McCarthy](https://github.com/NicholasMcCarthy)</li> <li>[Rory McGrath](http://www.rorymcgrath.ie/)</li> <li>[Sumit Pai](https://github.com/sumitpai)</li></ul></details> | [![](https://img.shields.io/github/stars/Accenture/AmpliGraph?style=social)](https://github.com/Accenture/AmpliGraph) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1702.05563), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1705.10744), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.08683), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1612.03975), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.10000), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6575)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.ampligraph.org)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/gX_KHaU8ChI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Accenture/AmpliGraph/blob/main/docs/tutorials/AmpliGraphBasicsTutorial.ipynb/) | 23.02.2023 |
| Classify text with BERT | This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05101)</li><li>[data](https://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tfhub.dev/google/collections/bert/1)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb/) | 15.02.2023 |
| NMT with attention | This notebook trains a seq2seq model for Spanish to English translation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.0473), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.04025v5)</li><li>[data](http://www.manythings.org/anki/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Neural_machine_translation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb/) | 15.02.2023 |
| GLUE using BERT on TPU | This tutorial contains complete end-to-end code to train models on a TPU | [Google](https://www.tensorflow.org/) | <ul><li>[GLUE](https://gluebenchmark.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/tpu)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb/) | 15.02.2023 |
| Kornia | Library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors | <ul><li>[Edgar Riba](https://github.com/edgarriba)</li> <li>[Dmytro Mishkin](https://dmytro.ai/)</li> <li>[Daniel Ponsa](https://github.com/DanielPonsa)</li><details><summary>others</summary><li>[Ethan Rublee](https://github.com/ethanrublee)</li> <li>[Gary Bradski](https://github.com/garybradski)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1109/WACV45572.2020.9093363)](https://doi.org/10.1109/WACV45572.2020.9093363) [![](https://img.shields.io/github/stars/kornia/kornia?style=social)](https://github.com/kornia/kornia) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.02190)</li><li>[blog post](https://opencv.org/kornia-an-open-source-differentiable-computer-vision-library-for-pytorch/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://kornia.readthedocs.io/en/latest/)</li><li>[<img src="images/slack.svg" alt="slack" height=20/>](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)</li><li>[<img src="images/twitter.svg" alt="twitter" height=20/>](https://twitter.com/kornia_foss)</li><li>[website](https://kornia.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCI1SE1Ij2Fast5BSKxoa7Ag), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/3RmCYFhwclE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/AAZa-mXjYF0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/kornia/kornia/blob/master/examples/augmentation/kornia_augmentation.ipynb/) | 11.02.2023 |
| Feast | An open source feature store for machine learning | <ul><li>[Willem Pienaar](https://github.com/woop)</li> <li>[Danny Chiao](https://github.com/adchia)</li> <li>[Achal Shah](http://achals.com/)</li><details><summary>others</summary><li>[Terence Lim](https://terryyylim.github.io/portfolio/)</li> <li>[Ches Martin](https://github.com/ches)</li> <li>[Judah Rand](https://github.com/judahrand)</li> <li>[Matt Delacour](https://github.com/MattDelac)</li> <li>[Miguel Trejo Marrufo](https://github.com/TremaMiguel)</li> <li>[Francisco Javier Arceo](https://franciscojavierarceo.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/feast-dev/feast?style=social)](https://github.com/feast-dev/feast) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.feast.dev/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/baineng/feast-hive), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Shopify/feast-trino), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Azure/feast-azure), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/feast_extractor.py)</li><li>[website](https://feast.dev/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/DaNv-Wf1MBA), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/p2cuq4eJ2BY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/feast-dev/feast/blob/master/examples/quickstart/quickstart.ipynb/) | 07.02.2023 |
| High-performance Simulation with Kubernetes | This tutorial will describe how to set up high-performance simulation using a TFF runtime running on Kubernetes | [Jason Roselander](https://github.com/roselander) | <ul><li>[GKE](https://cloud.google.com/kubernetes-engine/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[shell](https://cloud.google.com/shell/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb/) | 31.01.2023 |
| DALL·E Flow | An interactive workflow for generating high-definition images from text prompt | <ul><li>[Han Xiao](https://hanxiao.io/)</li> <li>[Delgermurun Purevkhuu](https://delgermurun.com/)</li> <li>[Alex Cureton-Griffiths](http://blog.alexcg.net/)</li></ul> | [![](https://img.shields.io/github/stars/jina-ai/dalle-flow?style=social)](https://github.com/jina-ai/dalle-flow) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Jack000/glid-3-xl), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jina-ai/docarray)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/c/jina-ai)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jina-ai/dalle-flow/blob/main/client.ipynb/) | 26.01.2023 |
| Home Robot | Low-level API for controlling various home robots | [Chris Paxton](https://cpaxton.github.io/) | [![](https://img.shields.io/github/stars/facebookresearch/home-robot?style=social)](https://github.com/facebookresearch/home-robot) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/cpaxton/contact_graspnet/tree/cpaxton/devel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fairo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hello-robot/stretch_body), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hello-robot/stretch_firmware), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hello-robot/stretch_ros), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hello-robot/stretch_ros2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hello-robot/stretch_web_interface), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/RoboStack/ros-noetic), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/codekansas/stretch-robot)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/home-robot/blob/master/src/home_robot_sim/notebooks/velocity_control_sim.ipynb/) | 25.01.2023 |
| Diffusers | Provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models | [Hugging Face](https://huggingface.co/) | [![](https://img.shields.io/github/stars/huggingface/diffusers?style=social)](https://github.com/huggingface/diffusers) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.11239), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.11239), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.02502), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.09778), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.13902)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/hojonathanho/diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pesser/pytorch_diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ermongroup/ddim), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/heejkoo/Awesome-Diffusion-Models)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/CompVis/text2img-latent-diffusion), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/CompVis/celeba-latent-diffusion), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/fusing/celeba-diffusion), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/huggingface/diffuse-the-rest), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/Shuang59/Composable-Diffusion)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/hugging-face-just-released-the-diffusers-library-846f32845e65)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/UzkdOg7wWmI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb/) | 17.01.2023 |
| Sample Factory | One of the fastest RL libraries focused on very efficient synchronous and asynchronous implementations of policy gradients | <ul><li>[Aleksei Petrenko](https://alex-petrenko.github.io/)</li> <li>[Zhehui Huang](https://zhehui-huang.github.io/)</li> <li>[Tushar Kumar](https://github.com/tushartk)</li><details><summary>others</summary><li>[Gaurav Sukhatme](http://robotics.usc.edu/~gaurav/)</li> <li>[Vladlen Koltun](http://vladlen.info/)</li></ul></details> | [![](https://img.shields.io/github/stars/alex-petrenko/sample-factory?style=social)](https://github.com/alex-petrenko/sample-factory) <ul><li>[ICML](http://proceedings.mlr.press/v119/petrenko20a.html)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.11751)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://www.samplefactory.dev/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/alex-petrenko/faster-fifo)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/lLG17LKKSZc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/alex-petrenko/sample-factory/blob/master/sf_examples/notebooks/samplefactory_hub_example.ipynb/) | 17.01.2023 |
| Open-Assistant | Chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so | <ul><li>[Andreas Köpf](https://github.com/andreaskoepf)</li> <li>[Yannic Kilcher](https://github.com/yk)</li> <li>[Huu Nguyen](https://github.com/ontocord)</li><details><summary>others</summary><li>[Christoph Schuhmann](http://christoph-schuhmann.de/)</li> <li>[Keith Stevens](https://fozziethebeat.github.io/)</li> <li>[Abdullah Barhoum](https://github.com/AbdBarho)</li> <li>[Nguyen Minh Duc](https://github.com/notmd)</li> <li>[Oliver Stanley](https://olliestanley.github.io/)</li> <li>[James Melvin Ebenezer](https://github.com/melvinebenezer)</li></ul></details> | [![](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)](https://github.com/LAION-AI/Open-Assistant) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2203.02155)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://projects.laion.ai/Open-Assistant/)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/OpenAssistant)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://generativeai.pub/open-assistant-a-free-and-open-source-alternative-to-chatgpt-67d15229813)</li><li>[website](https://open-assistant.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/64Izfm24FKA), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ddG2fM9i4Kk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/FQIHLFLrTw0)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/LAION-AI/Open-Assistant/blob/main/notebooks/data-augmentation/stackexchange-builder/stackexchange-builder.ipynb/) | 14.01.2023 |
| CleanRL | Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features | <ul><li>[Shengyi Huang](https://costa.sh/)</li> <li>[Rousslan Dossa](https://dosssman.github.io/)</li> <li>[Chang Ye](https://github.com/yooceii)</li><details><summary>others</summary><li>[Jeff Braga](https://github.com/bragajj)</li> <li>[Dipam Chakraborty](https://github.com/dipamc)</li> <li>[Kinal Mehta](https://kinalmehta.github.io/)</li> <li>[João Araújo](https://github.com/joaogui1)</li></ul></details> | [![](https://img.shields.io/github/stars/vwxyzjn/cleanrl?style=social)](https://github.com/vwxyzjn/cleanrl) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.06347), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.06887), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.05905), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1509.02971), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.09477), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2009.04416), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.12894)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.cleanrl.dev/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tinkoff-ai/CORL), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Farama-Foundation/Gymnasium), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/baselines), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ikostrikov/jaxrl)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/cleanrl)</li><li>[paper](https://www.jmlr.org/papers/v23/21-1342.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCDdC6BIFRI0jvcwuhi3aI6w), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/dm4HdGujpPs)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/vwxyzjn/cleanrl/blob/master/docs/get-started/CleanRL_Huggingface_Integration_Demo.ipynb/) | 12.01.2023 |
| NeMo | A conversational AI toolkit built for researchers working on automatic speech recognition, natural language processing, and text-to-speech synthesis | <ul><li>[Oleksii Kuchaiev](http://kuchaev.com/)</li> <li>[Jason Li](https://scholar.google.com/citations?user=V28bxDwAAAAJ)</li> <li>[Chip Huyen](https://huyenchip.com/)</li><details><summary>others</summary><li>[Oleksii Hrinchuk](https://github.com/AlexGrinch)</li> <li>[Ryan Leary](https://github.com/ryanleary)</li> <li>[Boris Ginsburg](https://github.com/borisgin)</li> <li>[Samuel Kriman](https://github.com/sam1373)</li> <li>[Stanislav Beliaev](https://github.com/stasbel)</li> <li>[Vitaly Lavrukhin](https://github.com/vsl9)</li> <li>[Jack Cook](https://jackcook.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/NVIDIA/NeMo?style=social)](https://github.com/NVIDIA/NeMo) <ul><li>[project](https://docs.nvidia.com/deeplearning/nemo/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/wBgpMf_KQVw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/NVIDIA/NeMo/blob/master/tutorials/00_NeMo_Primer.ipynb/) | 05.01.2023 |
| BANMo | Given multiple casual videos capturing a deformable object, BANMo reconstructs an animatable 3D model, including an implicit canonical 3D shape, appearance, skinning weights, and time-varying articulations, without pre-defined shape templates or registered cameras | <ul><li>[Gengshan Yang](https://gengshan-y.github.io/)</li> <li>[Minh Vo](https://minhpvo.github.io/)</li> <li>[Natalia Neverova](https://nneverova.github.io/)</li><details><summary>others</summary><li>[Deva Ramanan](http://www.cs.cmu.edu/~deva/)</li> <li>[Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/)</li> <li>[Hanbyul Joo](https://jhugestar.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/banmo?style=social)](https://github.com/facebookresearch/banmo) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.12761)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kwea123/nerf_pl), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/gengshan-y/rigidmask), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ShichenLiu/SoftRas), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ThibaultGROUEIX/ChamferDistancePytorch)</li><li>[project](https://banmo-www.github.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/1NUa-yvFGA0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/jDTy-liFoCQ)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1dQJn1vsuz0DkyRZbOA1SulkVQ0V1kMUP) | 30.12.2022 |
| Actor-Critic | This tutorial demonstrates how to implement the Actor-Critic method using TensorFlow to train an agent on the Open AI Gym CartPole-V0 environment | [Mark Daoust](https://github.com/MarkDaoust) | <ul><li>[gym](https://gym.openai.com/)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Temporal_difference_learning)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb/) | 15.12.2022 |
| PyG | Library built upon PyTorch to easily write and train Graph Neural Networks for a wide range of applications related to structured data | <ul><li>[Matthias Fey](https://rusty1s.github.io/#/)</li> <li>[Jan Eric Lenssen](https://github.com/janericlenssen)</li></ul> | [![](https://img.shields.io/github/stars/pyg-team/pytorch_geometric?style=social)](https://github.com/pyg-team/pytorch_geometric) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.02428), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.07829), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1609.02907), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.03123), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.05178), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08566), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.10903), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.07953)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://pytorch-geometric.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/snap-stanford/ogb/tree/master/examples), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pyg-team/pyg-lib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rusty1s/pytorch_scatter), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rusty1s/pytorch_sparse), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rusty1s/pytorch_cluster), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AntonioLonga/PytorchGeometricTutorial)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://nips.cc/virtual/2020/public/poster_3fe230348e9a12c13120749e3f9fa4cd.html)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLGMXrbDNfqTzqxB1IGgimuhtfAhGd8lHF), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLGMXrbDNfqTwPxitLVHEbT9Pd6-oR_cud), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-UjytpbqX4A)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8) | 08.12.2022 |
| ruGPT3 | Example of inference of RuGPT3XL | [Anton Emelyanov](https://github.com/king-menin) | [![](https://img.shields.io/github/stars/ai-forever/ru-gpts?style=social)](https://github.com/ai-forever/ru-gpts) <ul><li>[cristofari](https://sbercloud.ru/ru/christofari)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)</li><li>[sparse attention](https://www.deepspeed.ai/tutorials/sparse-attention/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ai-forever/ru-gpts/blob/master/examples/ruGPT3XL_generation.ipynb/) | 07.12.2022 |
| Stable Diffusion Videos | Create videos with Stable Diffusion by exploring the latent space and morphing between text prompts | [Nathan Raw](https://github.com/nateraw) | [![](https://img.shields.io/github/stars/nateraw/stable-diffusion-videos?style=social)](https://github.com/nateraw/stable-diffusion-videos) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355), [<img src="images/git.svg" alt="git" height=20/>](https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb/) | 05.12.2022 |
| PyTerrier | A Python framework for performing information retrieval experiments | <ul><li>[Craig Macdonald](https://www.dcs.gla.ac.uk/~craigm/)</li> <li>[Nicola Tonellotto](https://github.com/tonellotto)</li></ul> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1145/3459637.3482013)](https://doi.org/10.1145/3459637.3482013) [![](https://img.shields.io/github/stars/terrier-org/pyterrier?style=social)](https://github.com/terrier-org/pyterrier) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2007.14271)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://pyterrier.readthedocs.io)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrier-org/ecir2021tutorial), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_ance), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_colbert), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_pisa), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_t5), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_doc2query), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/terrierteam/pyterrier_deepct)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/terrier-org/pyterrier/blob/master/examples/notebooks/non_en_retrieval.ipynb/) | 02.11.2022 |
| Image captioning | Given an image our goal is to generate a caption | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1502.03044)</li><li>[data](https://cocodataset.org/#home)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb/) | 26.10.2022 |
| DSP theory | Theory of digital signal processing: signals, filtration (IIR, FIR, CIC, MAF), transforms (FFT, DFT, Hilbert, Z-transform) etc | <ul><li>[Alexander Kapitanov](https://github.com/hukenovs)</li> <li>[Vladimir Fadeev](https://github.com/kirlf)</li> <li>[Karina Kvanchiani](https://github.com/karinakvanchiani)</li><details><summary>others</summary><li>[Elizaveta Petrova](https://github.com/kleinsbotle)</li> <li>[Andrei Makhliarchuk](https://github.com/anotherhelloworld)</li></ul></details> | [![](https://img.shields.io/github/stars/hukenovs/dsp-theory?style=social)](https://github.com/hukenovs/dsp-theory) <ul><li>[blog post](https://habr.com/ru/articles/460445/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/hukenovs/dsp-theory/blob/master/src/dsp_theory_1_signals.ipynb/) | 18.10.2022 |
| Mubert | Prompt-based music generation via Mubert API | [Ilya Belikov](https://github.com/ferluht) | [![](https://img.shields.io/github/stars/MubertAI/Mubert-Text-to-Music?style=social)](https://github.com/MubertAI/Mubert-Text-to-Music) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://mubert2.docs.apiary.io/)</li><li>[project](https://mubert.com/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YJu0iXn-T_U), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/5UsaxJsFvAI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/B0kkIpWifG4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/ferluht/Mubert-Text-to-Music/blob/main/Mubert_Text_to_Music.ipynb/) | 18.10.2022 |
| Neural style transfer | This tutorial uses deep learning to compose one image in the style of another image | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb/) | 26.09.2022 |
| ACME | A library of reinforcement learning components and agents | <ul><li>[Matt Hoffman](https://www.mwhoffman.com/)</li> <li>[Bobak Shahriari](https://github.com/bshahr)</li> <li>[John Aslanides](https://www.aslanides.io/)</li><details><summary>others</summary><li>[Gabriel Barth-Maron](https://github.com/fastturtle)</li> <li>[Feryal Behbahani](https://feryal.github.io/)</li> <li>[Tamara Norman](https://github.com/tamaranorman)</li> <li>[Abbas Abdolmaleki](https://scholar.google.com/citations?user=cCYTVWQAAAAJ)</li> <li>[Albin Cassirer](https://github.com/acassirer)</li> <li>[Fan Yang](https://github.com/ddmbr)</li> <li>[Kate Baumli](https://github.com/katebaumli)</li> <li>[Sarah Henderson](https://www.linkedin.com/in/sarah-henderson-agilecoach/)</li> <li>[Alex Novikov](https://scholar.google.ru/citations?user=jMUkLqwAAAAJ)</li> <li>[Sergio Gómez Colmenarejo](https://scholar.google.ru/citations?user=0Dkf68EAAAAJ)</li> <li>[Serkan Cabi](https://scholar.google.ru/citations?&user=l-HhJaUAAAAJ)</li> <li>[Caglar Gulcehre](https://www.caglarg.com/)</li> <li>[Tom Le Paine](http://tomlepaine.github.io/)</li> <li>[Andrew Cowie](https://scholar.google.ru/citations?&user=aTvi5mUAAAAJ)</li> <li>[Ziyu Wang](https://ziyuw.github.io/)</li> <li>[Bilal Piot](https://scholar.google.ru/citations?&user=fqxNUREAAAAJ)</li> <li>[Nando de Freitas](https://github.com/nandodf)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/acme?style=social)](https://github.com/deepmind/acme) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.00979)</li><li>[blog post](https://www.deepmind.com/publications/acme-a-new-framework-for-distributed-reinforcement-learning)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://dm-acme.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/dm_env)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/NUwDr42bPOw), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/J1XCWjuyRaI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/pFMuQWpHI5k)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/acme/blob/master/examples/tutorial.ipynb/) | 26.09.2022 |
| Word2Vec | Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1301.3781)</li><li>[link](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/cbow-word2vec), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/skip-gram-word2vec)</li><li>[projector](http://projector.tensorflow.org/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Zipf%27s_law)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb/) | 23.09.2022 |
| NetKet | Open-source project delivering cutting-edge methods for the study of many-body quantum systems with artificial neural networks and machine learning techniques | <ul><li>[Filippo Vicentini](https://filippovicentini.com/)</li> <li>[Damian Hofmann](https://github.com/femtobit)</li> <li>[Attila Szabó](https://github.com/attila-i-szabo)</li><details><summary>others</summary><li>[Dian Wu](https://github.com/wdphy16)</li> <li>[Christopher Roth](https://github.com/chrisrothUT)</li> <li>[Clemens Giuliani](https://github.com/inailuig)</li> <li>[Gabriel Pescia](https://github.com/gpescia)</li> <li>[Jannes Nys](https://github.com/jwnys)</li> <li>[Vladimir Vargas-Calderón](https://github.com/VolodyaCO)</li> <li>[Nikita Astrakhantsev](https://github.com/nikita-astronaut)</li> <li>[Giuseppe Carleo](https://github.com/gcarleo)</li> <li>[Kenny Choo](https://github.com/kchoo1118)</li> <li>[James Smith](https://jamesetsmith.github.io/)</li> <li>[Tom Westerhout](https://github.com/twesterhout)</li> <li>[Fabien Alet](https://github.com/fabienalet)</li> <li>[Emily Davis](https://github.com/emilyjd)</li> <li>[Stavros Efthymiou](https://github.com/stavros11)</li> <li>[Ivan Glasser](https://www.researchgate.net/profile/Ivan-Glasser)</li> <li>[Sheng-Hsuan Lin](https://shhslin.github.io/)</li> <li>[Marta Mauri](https://github.com/martamau)</li> <li>[Mazzola Guglielmo](https://www.ics.uzh.ch/en/research/research-groups/Guglielmo-Mazzola0.html)</li> <li>[Christian Mendl](http://christian.mendl.net/)</li> <li>[Evert Nieuwenburg](https://evert.info/)</li> <li>[Ossian O'Reilly](https://github.com/ooreilly)</li> <li>[Hugo Théveniaut](https://github.com/theveniaut)</li> <li>[Giacomo Torlai](https://github.com/GTorlai)</li> <li>[Alexander Wietek](https://awietek.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/netket/netket?style=social)](https://github.com/netket/netket) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.10526)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://netket.readthedocs.io/en/latest/index.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mpi4jax/mpi4jax), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cloudhan/jax-windows-builder)</li><li>[website](https://www.netket.org/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Ryz-o71tuy8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/PhilipVinc/Lectures/blob/main/2202_NetKet/01_intro.ipynb/) | 15.09.2022 |
| pymdp | Package for simulating Active Inference agents in Markov Decision Process environments | <ul><li>[Conor Heins](https://github.com/conorheins)</li> <li>[Alec Tschantz](https://github.com/alec-tschantz)</li> <li>[Beren Millidge](https://www.beren.io/)</li><details><summary>others</summary><li>[Brennan Klein](https://github.com/jkbren)</li> <li>[Arun Niranjan](https://github.com/Arun-Niranjan)</li> <li>[Daphne Demekas](https://github.com/daphnedemekas)</li></ul></details> | [![](https://img.shields.io/github/stars/infer-actively/pymdp?style=social)](https://github.com/infer-actively/pymdp) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.03904)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://pymdp-rtd.readthedocs.io/en/stable/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/infer-actively/pymdp/blob/master/docs/notebooks/active_inference_from_scratch.ipynb/) | 24.08.2022 |
| Stable Diffusion | A latent text-to-image diffusion model | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Andreas Blattmann](https://github.com/ablattmann)</li> <li>[Dominik Lorenz](https://github.com/qp-qp)</li><details><summary>others</summary><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li></ul></details> | [![](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social)](https://github.com/CompVis/stable-diffusion) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2205.11487), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2207.12598), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.09778), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.01073)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://arxiv.org/abs/2112.10752), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/christophschuhmann/improved-aesthetic-predictor), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/ShieldMnt/invisible-watermark), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/guided-diffusion), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/denoising-diffusion-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/x-transformers)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/CompVis), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/datasets/laion/laion2B-en), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/datasets/laion/laion-high-resolution)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CompVis/stable-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb/) | 10.08.2022 |
| Deep-MAC | Welcome to the Novel class segmentation demo | [Vighnesh Birodkar](http://vighneshbirodkar.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00613)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/deep-mac)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/deepmac_colab.ipynb/) | 09.08.2022 |
| NL-Augmenter | A collaborative effort intended to add transformations of datasets dealing with natural language | <ul><li>[Aadesh Gupta](https://github.com/aadesh11)</li> <li>[Timothy Sum Hon Mun](https://github.com/timothy22000)</li> <li>[Aditya Srivatsa](https://github.com/kvadityasrivatsa)</li><details><summary>others</summary><li>[Xudong Shen](https://github.com/XudongOliverShen)</li> <li>[Juan Diego Rodriguez](https://github.com/juand-r)</li> <li>[Ashish Shrivastava](https://github.com/ashish3586)</li> <li>[Nagender Aneja](https://researchid.co/naneja)</li> <li>[Zijie Wang](https://zijie.wang/)</li> <li>[Yiwen Shi](https://github.com/Yiwen-Shi)</li> <li>[Afnan Mir](https://github.com/afnanmmir)</li> <li>[William Soto](https://github.com/sotwi)</li> <li>[Chandan Singh](https://csinva.io/)</li> <li>[Claude Roux](https://github.com/ClaudeRoux)</li> <li>[Abinaya Mahendiran](https://github.com/AbinayaM02)</li> <li>[Anna Shvets](https://github.com/asnota)</li> <li>[Kaustubh Dhole](https://github.com/kaustubhdhole)</li> <li>[Bryan Wilie](https://github.com/bryanwilie)</li> <li>[Jamie Simon](https://james-simon.github.io/)</li> <li>[Mukund Varma](https://github.com/MukundVarmaT)</li> <li>[Sang Han](https://github.com/jjangsangy)</li> <li>[Denis Kleyko](https://github.com/denkle)</li> <li>[Samuel Cahyawijaya](https://github.com/SamuelCahyawijaya)</li> <li>[Filip Cornell](https://github.com/Filco306)</li> <li>[Tanay Dixit](https://tanay2001.github.io/)</li> <li>[Connor Boyle](https://github.com/boyleconnor)</li> <li>[Genta Indra Winata](https://gentawinata.com/)</li> <li>[Seungjae Ryan Lee](https://github.com/seungjaeryanlee)</li> <li>[Marcin Namysl](https://github.com/mnamysl)</li> <li>[Roman Sitelew](https://github.com/RomanPlusPlus)</li> <li>[Zhenhao Li](https://zhenhaoli.net/)</li> <li>[Fiona Tan](https://tanfiona.github.io/)</li></ul></details> | [![](https://img.shields.io/github/stars/GEM-benchmark/NL-Augmenter?style=social)](https://github.com/GEM-benchmark/NL-Augmenter) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2112.02721)</li><li>[website](https://gem-benchmark.com/nl_augmenter)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/GEM-benchmark/NL-Augmenter/blob/main/notebooks/Write_a_sample_transformation.ipynb/) | 06.08.2022 |
| CycleGAN | This notebook demonstrates unpaired image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.10593)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/cycle_gan)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb/) | 02.08.2022 |
| Accelerate | A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision | [Hugging Face](https://huggingface.co/) | [![](https://img.shields.io/github/stars/huggingface/accelerate?style=social)](https://github.com/huggingface/accelerate) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://huggingface.co/docs/accelerate/index)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/huggingface/notebooks/blob/master/examples/accelerate/simple_nlp_example.ipynb/) | 27.07.2022 |
| YOLOv5 on Custom Objects | This notebook shows training on your own custom objects | [Jacob Solawetz](https://blog.roboflow.com/author/jacob/) | [![](https://img.shields.io/github/stars/ultralytics/yolov5?style=social)](https://github.com/ultralytics/yolov5) <ul><li>[blog post](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/)</li><li>[data](https://public.roboflow.ai/object-detection/bccd)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ) | 20.07.2022 |
| Epistemic Neural Networks | A library for neural networks that know what they don't know | <ul><li>[Ian Osband](http://iosband.github.io/)</li> <li>[Zheng Wen](http://zheng-wen.com/)</li> <li>[Seyed Mohammad Asghari](https://github.com/mohammadasghari)</li><details><summary>others</summary><li>[Vikranth Dwaracherla](https://github.com/dvikranth)</li> <li>[Morteza Ibrahimi](https://github.com/mibrahimi)</li> <li>[Xiuyuan Lu](https://scholar.google.com/citations?user=SPL_2lIAAAAJ)</li> <li>[Benjamin Van Roy](https://web.stanford.edu/~bvr/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/enn?style=social)](https://github.com/deepmind/enn) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2107.08924)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/syncedreview/deepminds-epistemic-neural-networks-open-new-avenues-for-uncertainty-modelling-in-large-and-fa83ab00aba3)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/j8an0dKcX4A)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/enn/blob/master/enn/colabs/enn_demo.ipynb/) | 12.07.2022 |
| MindsEye | Graphical user interface built to run multimodal ai art models for free from a Google Colab, without needing edit a single line of code or know any programming | <ul><li>[multimodal.art](https://multimodal.art/)</li> <li>[João Paulo Apolinário Passos](http://www.apolinariopassos.com.br/portfolio/)</li></ul> | [![](https://img.shields.io/github/stars/multimodalart/mindseye?style=social)](https://github.com/multimodalart/mindseye) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/guided-diffusion)</li><li>[project](https://multimodal.art/mindseye)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1cg0LZ5OfN9LAIB37Xq49as0fSJxcKtC5) | 06.07.2022 |
| py-irt | Fitting Item Response Theory models using variational inference | <ul><li>[John Lalor](https://jplalor.github.io/)</li> <li>[Hong Yu](https://scholar.google.com/citations?user=TyXe64wAAAAJ)</li> <li>[Pedro Rodriguez](https://www.pedro.ai/)</li><details><summary>others</summary><li>[Joe Barrow](https://jbarrow.ai/)</li> <li>[Alexander Hoyle](https://alexanderhoyle.com/)</li> <li>[Robin Jia](https://robinjia.github.io/)</li> <li>[Jordan Boyd-Graber](https://github.com/ezubaric)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.18653/v1/2021.acl-long.346)](https://doi.org/10.18653/v1/2021.acl-long.346) [![](https://img.shields.io/github/stars/nd-ball/py-irt?style=social)](https://github.com/nd-ball/py-irt) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1908.11421)</li><li>[paper](https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01422/full)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/akUxtt21Mlc)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/nd-ball/py-irt/blob/master/examples/py-irt_example.ipynb/) | 30.06.2022 |
| Integrated gradients | This tutorial demonstrates how to implement Integrated Gradients, an Explainable AI technique | [Google](https://www.tensorflow.org/) | [![](https://img.shields.io/github/stars/GoogleCloudPlatform/training-data-analyst?style=social)](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/integrated_gradients) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.01365)</li><li>[visualizing](https://distill.pub/2020/attribution-baselines/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Linear_interpolation), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Riemann_sum)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/interpretability/integrated_gradients.ipynb/) | 30.06.2022 |
| SberSwap | A new face swap method for image and video domains | <ul><li>[Daniil Chesakov](https://github.com/Danyache)</li> <li>[Anastasia Maltseva](https://github.com/NastyaMittseva)</li> <li>[Alexander Groshev](https://github.com/AlexanderGroshev)</li><details><summary>others</summary><li>[Andrey Kuznetsov](https://www.linkedin.com/in/andrey-kuznetsov-70ab12127)</li> <li>[Denis Dimitrov](https://github.com/denndimitrov)</li></ul></details> | [![](https://img.shields.io/github/stars/ai-forever/sber-swap?style=social)](https://github.com/ai-forever/sber-swap) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.03046), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.13457), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1901.08971), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06340), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05005), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.09965)</li><li>[blog post](https://habr.com/ru/company/sberbank/blog/645919/)</li><li>[data](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7) | 29.06.2022 |
| BIG-bench | A collaborative benchmark intended to probe large language models and extrapolate their future capabilities | <ul><li>[Jaehoon Lee](https://jaehlee.github.io/)</li> <li>[Jascha Sohl-Dickstein](http://www.sohldickstein.com/)</li> <li>[Vinay Ramasesh](https://ramasesh.github.io/)</li><details><summary>others</summary><li>[Sajant Anand](https://github.com/sajantanand)</li> <li>[Alicia Parrish](https://aliciaparrish.com/)</li> <li>[Ethan Dyer](https://github.com/ethansdyer)</li> <li>[Liam Dugan](http://liamdugan.com/)</li> <li>[Dieuwke Hupkes](https://github.com/dieuwkehupkes)</li> <li>[Daniel Freeman](https://github.com/cdfreeman-google)</li> <li>[Guy Gur-Ari](https://github.com/guygurari)</li> <li>[Aitor Lewkowycz](https://github.com/lewkowycz)</li></ul></details> | [![](https://img.shields.io/github/stars/google/BIG-bench?style=social)](https://github.com/google/BIG-bench) <ul><li>[API](https://google.github.io/BIG-bench/docs/html/bigbench/index.html)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2206.04615)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/seqio)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/BIG-bench/blob/master/notebooks/colab_examples.ipynb/) | 27.06.2022 |
| HuggingArtists | Choose your favorite Artist and train a language model to write new lyrics based on their unique voice | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![](https://img.shields.io/github/stars/AlekseyKorshuk/huggingartists?style=social)](https://github.com/AlekseyKorshuk/huggingartists) <ul><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/AlekseyKorshuk/huggingartists), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/huggingartists)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb/) | 25.06.2022 |
| Introduction to the TensorFlow Models NLP library | You will learn how to build transformer-based models for common NLP tasks including pretraining, span labelling and classification using the building blocks from NLP modeling library | [Chen Chen](https://github.com/chenGitHuber) | [![](https://img.shields.io/github/stars/tensorflow/models?style=social)](https://github.com/tensorflow/models/tree/master/official/nlp/modeling) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb/) | 22.06.2022 |
| Cirq | A python framework for creating, editing, and invoking Noisy Intermediate Scale Quantum circuits | <ul><li>[Balint Pato](https://refactorium.com/)</li> <li>[Matthew Harrigan](https://mpharrigan.com/)</li> <li>[Animesh Sinha](https://github.com/AnimeshSinha1309)</li><details><summary>others</summary><li>[Matthew Neeley](https://github.com/maffoo)</li> <li>[Dave Bacon](https://dabacon.org/)</li> <li>[Matteo Pompili](https://github.com/matpompili)</li> <li>[Michael Broughton](https://github.com/MichaelBroughton)</li></ul></details> | [![](https://img.shields.io/github/stars/quantumlib/Cirq?style=social)](https://github.com/quantumlib/Cirq) <ul><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Quantum_logic_gate#Hadamard_gate)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/16ZfkPRVf2w)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/quantumlib/Cirq/blob/master/docs/tutorials/basics.ipynb/) | 21.06.2022 |
| CLIP-as-service | A low-latency high-scalability service for embedding images and text | [Han Xiao](https://hanxiao.io/) | [![](https://img.shields.io/github/stars/jina-ai/clip-as-service?style=social)](https://github.com/jina-ai/clip-as-service) <ul><li>[data](https://sites.google.com/view/totally-looks-like-dataset)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jina-ai/docarray)</li><li>[website](https://clip-as-service.jina.ai/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/c/jina-ai)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb/) | 19.06.2022 |
| Jina | MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud | [Han Xiao](https://hanxiao.io/) | [![](https://img.shields.io/github/stars/jina-ai/jina?style=social)](https://github.com/jina-ai/jina) <ul><li>[data](https://sites.google.com/view/totally-looks-like-dataset)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.jina.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow.json)</li><li>[hub](https://hub.jina.ai/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/c/jina-ai)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/jina-ai/jina/blob/master/docs/Using_Jina_on_Colab.ipynb/) | 11.06.2022 |
| Transfer learning and fine-tuning | You will learn how to classify images of cats and dogs by using transfer learning from a pre-trained network | [François Chollet](https://fchollet.com/) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/transfer-learning)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Transfer_learning)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb/) | 07.06.2022 |
| Evidently | An open-source framework to evaluate, test and monitor ML models in production | <ul><li>[Elena Samuylova](https://github.com/elenasamuylova)</li> <li>[Emeli Dral](https://github.com/emeli-dral)</li> <li>[Olga Filippova](https://github.com/0lgaF)</li></ul> | [![](https://img.shields.io/github/stars/evidentlyai/evidently?style=social)](https://github.com/evidentlyai/evidently) <ul><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.evidentlyai.com/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/0lgaF/my_tab_with_evidently)</li><li>[website](https://evidentlyai.com/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/c/EvidentlyAI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L4Pv6ExBQPM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1Dd6ZzIgeBYkD_4bqWZ0RAdUpCU0b6Y6H) | 30.05.2022 |
| Tortoise | A multi-voice TTS system trained with an emphasis on quality | [James Betker](https://nonint.com/) | [![](https://img.shields.io/github/stars/neonbjb/tortoise-tts?style=social)](https://github.com/neonbjb/tortoise-tts) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.12092), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.09672), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.07889)</li><li>[examples](https://nonint.com/static/tortoise_v2_examples.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/neonbjb/DL-Art-School)</li><li>[<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/patrickvonplaten), [<img src="images/huggingface.svg" alt="huggingface" height=20/>](https://huggingface.co/spaces/osanseviero/tortoisse-tts)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/J3-jfS29RF4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/neonbjb/tortoise-tts/blob/main/tortoise_tts.ipynb/) | 03.05.2022 |
| Text generation with RNN | This tutorial demonstrates how to generate text using a character-based RNN | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb/) | 02.05.2022 |
| CLIPDraw | Synthesize drawings to match a text prompt | <ul><li>[Kevin Frans](https://www.kvfrans.com/)</li> <li>[Lisa Soros](https://scholar.google.com/citations?user=iUkpvMUAAAAJ)</li> <li>[Olaf Witkowski](https://olafwitkowski.com/)</li></ul> | [![](https://img.shields.io/github/stars/kvfrans/clipdraw?style=social)](https://github.com/kvfrans/clipdraw) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.14843), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.00162)</li><li>[blog post](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/BachiLi/diffvg/blob/master/apps/painterly_rendering.py)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb/) | 28.04.2022 |
| deep-significance | Easy-to-use package containing different significance tests and utility functions specifically tailored towards research needs and usability | <ul><li>[Dennis Ulmer](http://dennisulmer.eu/)</li> <li>[Christian Hardmeier](https://christianhardmeier.rax.ch/)</li> <li>[Jes Frellsen](https://frellsen.org/)</li></ul> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.18653/v1/p19-1266)](https://doi.org/10.18653/v1/p19-1266) [![](https://img.shields.io/github/stars/Kaleidophon/deep-significance?style=social)](https://github.com/Kaleidophon/deep-significance) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2204.06815)</li><li>[blog post](https://machinelearningmastery.com/statistical-hypothesis-tests/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://deep-significance.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/rtmdrr/replicability-analysis-NLP), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rtmdrr/testSignificanceNLP), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rtmdrr/DeepComparison)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/Kaleidophon/deep-significance/blob/main/paper/deep-significance%20demo.ipynb/) | 12.04.2022 |
| Autoencoders | This tutorial introduces autoencoders with three examples: the basics, image denoising, and anomaly detection | [Google](https://www.tensorflow.org/) | <ul><li>[blog post](https://blog.keras.io/building-autoencoders-in-keras.html)</li><li>[book](https://www.deeplearningbook.org/contents/autoencoders.html)</li><li>[data](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000)</li><li>[examples](https://anomagram.fastforwardlabs.com/#/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/autoencoder)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb/) | 05.04.2022 |
| Text classification with RNN | This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[link](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb/) | 17.03.2022 |
| Real-Time Voice Cloning | SV2TTS with a vocoder that works in real-time | <ul><li>[Corentin Jemine](https://github.com/CorentinJ)</li> <li>[Erdene-Ochir Tuguldur](https://github.com/tugstugi)</li></ul> | [![](https://img.shields.io/github/stars/CorentinJ/Real-Time-Voice-Cloning?style=social)](https://github.com/CorentinJ/Real-Time-Voice-Cloning) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.04558), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.08435), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.10135), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10467)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/fatchord/WaveRNN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/coqui-ai/tts), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/resemble-ai/Resemblyzer)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-O_hYhToKoA)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb/) | 07.03.2022 |
| BLIP | VLP framework which transfers flexibly to both vision-language understanding and generation tasks | <ul><li>[Junnan Li](https://github.com/LiJunnan1992)</li> <li>[Dongxu Li](https://sites.google.com/view/dongxu-li/home)</li> <li>[Caiming Xiong](http://cmxiong.com/)</li> <li>[Steven Hoi](https://sites.google.com/view/stevenhoi)</li></ul> | [![](https://img.shields.io/github/stars/salesforce/BLIP?style=social)](https://github.com/salesforce/BLIP) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.12086)</li><li>[blog post](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/fairscale), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/salesforce/ALPRO), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/dmlc/decord), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/salesforce/ALBEF), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rwightman/pytorch-image-models/tree/main/timm)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/X2k7n4FuI7c)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/salesforce/BLIP/blob/main/demo.ipynb/) | 02.03.2022 |
| Silero Models | Pre-trained speech-to-text, text-to-speech and text-enhancement models made embarrassingly simple | [Silero team](https://www.silero.ai/about/) | [![](https://img.shields.io/github/stars/snakers4/silero-models?style=social)](https://github.com/snakers4/silero-models) <ul><li>[STT](https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/), [STT](https://thegradient.pub/a-speech-to-text-practitioners-criticisms-of-industry-and-academia/), [STT](https://habr.com/ru/post/519562/)</li><li>[TTS](https://habr.com/ru/post/660571/), [TTS](https://habr.com/ru/post/549482/)</li><li>[Text Enhancement](https://habr.com/ru/post/581960/)</li><li>[VAD](https://thegradient.pub/one-voice-detector-to-rule-them-all/), [VAD](https://habr.com/ru/post/537276/)</li><li>[website](https://www.silero.ai/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/snakers4/silero-models/blob/master/examples.ipynb/) | 27.02.2022 |
| ArcaneGAN | Process video in the style of the Arcane animated series | [Alexander Spirin](https://github.com/Sxela) | [![](https://img.shields.io/github/stars/Sxela/ArcaneGAN?style=social)](https://github.com/Sxela/ArcaneGAN) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Sxela/stylegan3_blending)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Fi199uFW6jE), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/AJG4X7IokG8)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1r1hhciakk5wHaUn1eJk7TP58fV9mjy_W) | 17.02.2022 |
| textlesslib | A library aimed to facilitate research in Textless NLP | <ul><li>[Eugene Kharitonov](https://eugene-kharitonov.github.io/)</li> <li>[Jade Copet](https://scholar.google.com/citations?user=GRMLwjAAAAAJ)</li> <li>[Kushal Lakhotia](https://about.me/hikushalhere)</li><details><summary>others</summary><li>[Nguyễn Tú Anh](https://tuanh208.github.io/)</li> <li>[Paden Tomasello](https://scholar.google.com/citations?user=sBtWMGYAAAAJ)</li> <li>[Ann Lee](https://ai.facebook.com/people/ann-lee)</li> <li>[Ali Elkahky](https://scholar.google.com/citations?user=KB3S8RoAAAAJ)</li> <li>[Wei-Ning Hsu](https://wnhsu.github.io/)</li> <li>[Abdelrahman Mohamed](https://ai.facebook.com/people/abdelrahman-mohamed/)</li> <li>[Emmanuel Dupoux](http://www.lscp.net/persons/dupoux/)</li> <li>[Yossi Adi](https://www.cs.huji.ac.il/~adiyoss/)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/textlesslib?style=social)](https://github.com/facebookresearch/textlesslib) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2202.07359)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/waveglow), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/keithito/tacotron), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/tacotron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pseeth/torch-stft)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/dataset/librispeech)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/textlesslib/blob/main/examples/resynthesis_and_continuation.ipynb/) | 15.02.2022 |
| AV-HuBERT | Self-supervised representation learning framework for audio-visual speech | <ul><li>[Bowen Shi](https://home.ttic.edu/~bshi/)</li> <li>[Wei-Ning Hsu](http://people.csail.mit.edu/wnhsu/)</li> <li>[Kushal Lakhotia](https://about.me/hikushalhere)</li> <li>[Abdelrahman Mohamed](http://www.cs.toronto.edu/~asamir/)</li></ul> | [![](https://img.shields.io/github/stars/facebookresearch/av_hubert?style=social)](https://github.com/facebookresearch/av_hubert) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.02184), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2201.01763), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1911.04890)</li><li>[blog post](https://ai.facebook.com/blog/ai-that-understands-speech-by-looking-as-well-as-hearing/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1bNXkfpHiVHzXQH8WjGhzQ-fsDxolpUjD) | 12.02.2022 |
| Word embeddings | This tutorial contains an introduction to word embeddings | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[projector](http://projector.tensorflow.org/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb/) | 15.01.2022 |
| RuDOLPH | A fast and light text-image-text transformer designed for a quick and easy fine-tuning setup for the solution of various tasks: from generating images by text description and image classification to visual question answering and more | <ul><li>[Alex Shonenkov](https://github.com/shonenkov)</li> <li>[Michael Konstantinov](https://github.com/zeroshot-ai)</li></ul> | [![](https://img.shields.io/github/stars/ai-forever/ru-dolph?style=social)](https://github.com/ai-forever/ru-dolph) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.14165), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.12092), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1gmTDA13u709OXiAeXWGm7sPixRhEJCga) | 14.01.2022 |
| DeepDream | This tutorial contains a minimal implementation of DeepDream: an experiment that visualizes the patterns learned by a neural network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.4842)</li><li>[blog post](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Inception)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb/) | 13.01.2022 |
| MLP | The most basic neural network architectures, a multilayer perceptron, also known as a feedforward network | [Ben Trevett](https://bentrevett.com/) | <ul><li>[NN and DL](http://neuralnetworksanddeeplearning.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1702.03118), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2108.12943), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.04020)</li><li>[optimization](https://ruder.io/optimizing-gradient-descent/)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only), [<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Multilayer_perceptron)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb/) | 26.12.2021 |
| AlexNet | A neural network model that uses convolutional neural network layers and was designed for the ImageNet challenge | [Ben Trevett](https://bentrevett.com/) | [![](https://img.shields.io/github/stars/davidtvs/pytorch-lr-finder?style=social)](https://github.com/davidtvs/pytorch-lr-finder) <ul><li>[ILSVRC](https://image-net.org/challenges/LSVRC/)</li><li>[LR](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)</li><li>[PMLR](https://proceedings.mlr.press/v9/glorot10a.html)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.0575)</li><li>[cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[dropout](https://sebastianraschka.com/faq/docs/dropout-activation.html)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/alexnet)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/models.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Regularization_(mathematics)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/AlexNet)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb/) | 26.12.2021 |
| VGG | Very Deep Convolutional Networks for Large-Scale Image Recognition | [Ben Trevett](https://bentrevett.com/) | [![](https://img.shields.io/github/stars/pytorch/vision?style=social)](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py#L47) <ul><li>[ILSVRC](https://image-net.org/challenges/LSVRC/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.1556), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1506.01186), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1801.06146), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1502.03167), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.11604)</li><li>[cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/vgg)</li><li>[<img src="images/pt.svg" alt="pt" height=20/>](https://pytorch.org/vision/stable/models.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/HR0lt1hlR6U?t=5900), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/j1jIoHN3m0s), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/RNnKtNrsrmg)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb/) | 26.12.2021 |
| LeNet | A neural network model that uses convolutional neural network layers and was designed for classifying handwritten characters | [Ben Trevett](https://bentrevett.com/) | <ul><li>[CNN](https://cs231n.github.io/convolutional-networks/)</li><li>[LeNet-5](http://yann.lecun.com/exdb/lenet/)</li><li>[guide](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)</li><li>[paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/lenet)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Convolution), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Sobel_operator), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Gaussian_blur)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb/) | 26.12.2021 |
| FLAML | Lightweight Python library that finds accurate machine learning models automatically, efficiently and economically | <ul><li>[Chi Wang](https://github.com/sonichi)</li> <li>[Qingyun Wu](https://qingyun-wu.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/microsoft/FLAML?style=social)](https://github.com/microsoft/FLAML) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.04815), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.01571)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://microsoft.github.io/FLAML/)</li><li>[paper](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCfU0zfFXHXdAd5x-WvFBk5A), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/euXpDYGgkGM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/microsoft/FLAML/blob/master/notebook/flaml_automl.ipynb/) | 17.12.2021 |
| CompilerGym | A reinforcement learning toolkit for compiler optimizations | <ul><li>[Chris Cummins](https://chriscummins.cc/)</li> <li>[Bram Wasti](https://github.com/bwasti)</li> <li>[Jiadong Guo](https://jd-eth.github.io/)</li><details><summary>others</summary><li>[Brandon Cui](https://www.linkedin.com/in/bcui19/)</li> <li>[Jason Ansel](https://jasonansel.com/)</li> <li>[Sahir Gomez](https://github.com/sahirgomez1)</li> <li>[Olivier Teytaud](https://github.com/teytaud)</li> <li>[Benoit Steiner](http://bsteiner.info/)</li> <li>[Yuandong Tian](http://yuandong-tian.com/)</li> <li>[Hugh Leather](https://github.com/hughleat)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/CompilerGym?style=social)](https://github.com/facebookresearch/CompilerGym) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2109.08267)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://facebookresearch.github.io/CompilerGym/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/CompilerGym/blob/development/examples/getting-started.ipynb/) | 16.11.2021 |
| DeepStyle | The Neural Style algorithm synthesizes a pastiche by separating and combining the content of one image with the style of another image using convolutional neural networks | <ul><li>[Cameron Smith](https://github.com/cysmith)</li> <li>[Alexander Spirin](https://github.com/Sxela)</li></ul> | [![](https://img.shields.io/github/stars/cysmith/neural-style-tf?style=social)](https://github.com/cysmith/neural-style-tf) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1604.08610), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.05897), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576)</li><li>[cvpr](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pastiche), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/The_Starry_Night), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/YUV), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Lab_color_space), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/YCbCr), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/CIELUV), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pareidolia)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/14aJ7HQPbcP0sNRIY-FRO4u6lxtlyyxI_) | 01.10.2021 |
| Text2Animation | Generate images from text phrases with VQGAN and CLIP with animation and keyframes | <ul><li>[Katherine Crowson](https://kath.io/)</li> <li>[Ryan Murdock](https://twitter.com/advadnoun)</li> <li>[Chigozie Nri](https://github.com/chigozienri)</li> <li>[Denis Malimonov](https://github.com/tg-bomze)</li></ul> | [![](https://img.shields.io/github/stars/chigozienri/VQGAN-CLIP-animations?style=social)](https://github.com/chigozienri/VQGAN-CLIP-animations) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCToztRy9FSTIhEen_1x4FAw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tg-bomze/collection-of-notebooks/blob/master/Text2Animation.ipynb/) | 29.09.2021 |
| EfficientNetV2 | A family of image classification models, which achieve better parameter efficiency and faster training speed than prior arts | <ul><li>[Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ)</li> <li>[Quoc Le](https://cs.stanford.edu/~quocle/)</li></ul> | [![](https://img.shields.io/github/stars/google/automl?style=social)](https://github.com/google/automl/tree/master/efficientnetv2) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.00298), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.11946)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/TensorRT/tree/master/samples/python/efficientnet)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/automl/blob/master/efficientnetv2/tutorial.ipynb/) | 24.09.2021 |
| Droidlet | A modular embodied agent architecture and platform for building embodied agents | <ul><li>[Anurag Pratik](https://github.com/anuragprat1k)</li> <li>[Soumith Chintala](https://soumith.ch/)</li> <li>[Kavya Srinet](https://github.com/kavyasrinet)</li><details><summary>others</summary><li>[Dhiraj Gandhi](https://dhiraj100892.github.io/)</li> <li>[Rebecca Qian](https://github.com/Rebecca-Qian)</li> <li>[Yuxuan Sun](https://github.com/snyxan)</li> <li>[Ryan Drew](https://rdrew.dev/)</li> <li>[Sara Elkafrawy](https://github.com/saraEbrahim)</li> <li>[Anoushka Tiwari](https://www.linkedin.com/in/anoushka-tiwari)</li> <li>[Tucker Hart](https://www.linkedin.com/in/tucker-hart-05a638133)</li> <li>[Mary Williamson](https://scholar.google.com/citations?user=Ys4xB-QAAAAJ)</li> <li>[Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/)</li> <li>[Arthur Szlam](https://scholar.google.com/citations?user=u3-FxUgAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/facebookresearch/droidlet?style=social)](https://github.com/facebookresearch/droidlet) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.10384), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.08584)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://facebookresearch.github.io/droidlet/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/droidlet/blob/master/examples_and_tutorials/tutorials/droidlet_for_physical_robots.ipynb/) | 15.09.2021 |
| GPT-J-6B | A 6 billion parameter, autoregressive text generation model trained on The Pile | <ul><li>[Ben Wang](https://benwang.dev/)</li> <li>[Aran Komatsuzaki](https://arankomatsuzaki.wordpress.com/about-me/)</li> <li>[Janko Prester](https://www.jankoprester.com/)</li></ul> | [![](https://img.shields.io/github/stars/kingoflolz/mesh-transformer-jax?style=social)](https://github.com/kingoflolz/mesh-transformer-jax) <ul><li>[The Pile](https://pile.eleuther.ai/)</li><li>[blog post](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeed)</li><li>[web demo](https://6b.eleuther.ai/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb/) | 15.09.2021 |
| Sentence Transformers | Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co | <ul><li>[Nils Reimers](https://www.nils-reimers.de/)</li> <li>[Iryna Gurevych](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp)</li></ul> | [![](https://img.shields.io/github/stars/UKPLab/sentence-transformers?style=social)](https://github.com/UKPLab/sentence-transformers) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1908.10084), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.09813), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.08240)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://www.sbert.net/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb/) | 13.09.2021 |
| Machine learning course | This course is broad and shallow, but author will provide additional links so that you can deepen your understanding of the ML method you need | [Тимчишин Віталій](https://github.com/fbeilstein) | [![](https://img.shields.io/github/stars/fbeilstein/machine_learning?style=social)](https://github.com/fbeilstein/machine_learning) <ul><li>[blog post](https://vas3k.com/blog/machine_learning/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLkDeTjsoxDVgnb2lIYo9-1l4XYhrIyS6A), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-RdOwhmqP5s), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/R13BD8qKeTg), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/ZkjP5RJLQF4), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/J4Wdy0Wc_xQ), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/mBcLRGuAFUk), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YIGtalP1mv0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Yz5pySyEtsU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/x5zLaWT5KPs), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/yBwpo-L80Mc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/fbeilstein/machine_learning/blob/master/lecture_01_introduction.ipynb/) | 02.09.2021 |
| Lucid Sonic Dreams | Syncs GAN-generated visuals to music | [Mikael Alafriz](https://github.com/mikaelalafriz) | [![](https://img.shields.io/github/stars/mikaelalafriz/lucid-sonic-dreams?style=social)](https://github.com/mikaelalafriz/lucid-sonic-dreams) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan2)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/introducing-lucid-sonic-dreams-sync-gan-art-to-music-with-a-few-lines-of-python-code-b04f88722de1)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/l-nGC-ve7sI)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1Y5i50xSFIuN3V4Md8TB30_GOAtts7RQD) | 24.08.2021 |
| textgenrnn | Generate text using a pretrained neural network with a few lines of code, or easily train your own text-generating neural network of any size and complexity | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/textgenrnn?style=social)](https://github.com/minimaxir/textgenrnn) <ul><li>[blog post](http://minimaxir.com/2018/05/text-neural-networks/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RW7mP6BfZuY)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK) | 13.07.2021 |
| BasicSR | Open Source Image and Video Restoration Toolbox for Super-resolution, Denoise, Deblurring, etc. | <ul><li>[Xintao Wang](https://xinntao.github.io/)</li> <li>[Liangbin Xie](https://liangbinxie.github.io/)</li> <li>[Ke Yu](https://github.com/yuke93)</li><details><summary>others</summary><li>[Kelvin Chan](https://ckkelvinchan.github.io/)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li> <li>[Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/XPixelGroup/BasicSR?style=social)](https://github.com/XPixelGroup/BasicSR) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.02181)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://basicsr.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/ESRGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xindongzhang/ECBSR), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Lotayou/Face-Renovation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/csxmli2016/DFDNet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/facexlib), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyView), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyFigure), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/SFTGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/DNI), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyCrawler), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/xinntao/HandyWriting)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/KaMYsxWkmww)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1JQScYICvEC3VqaabLu-lxvq9h7kSV1ML) | 07.06.2021 |
| Hyperopt | Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions | <ul><li>[James Bergstra](https://github.com/jaberg)</li> <li>[Dan Yamins](https://github.com/yamins81)</li> <li>[David Cox](https://scholar.google.com/citations?user=6S-WgLkAAAAJ)</li></ul> | [![](https://img.shields.io/github/stars/hyperopt/hyperopt?style=social)](https://github.com/hyperopt/hyperopt) <ul><li>[ICML](https://proceedings.mlr.press/v28/bergstra13.html)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](http://hyperopt.github.io/hyperopt/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/hyperopt/hyperopt-sklearn), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hyperopt/hyperopt-nnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hyperopt/hyperopt-nnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hyperopt/hyperopt-convnet), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hyperopt/hyperopt-gpsmbo)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Mp1xnPfE4PY), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/tdwgR1AqQ8Y), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/tteE_Vtmrv4)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/hyperopt/hyperopt/blob/master/tutorial/01.BasicTutorial.ipynb/) | 01.06.2021 |
| CNN | This tutorial demonstrates training a simple Convolutional Neural Network to classify CIFAR images | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[cifar](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[link](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb/) | 21.05.2021 |
| Custom GPT-2 + Tokenizer | Train a custom GPT-2 model for free on a GPU using aitextgen! | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/aitextgen?style=social)](https://github.com/minimaxir/aitextgen) <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.aitextgen.io/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh) | 17.05.2021 |
| Train a GPT-2 Text-Generating Model | Retrain an advanced text generating neural network on any text dataset for free on a GPU using Colaboratory using aitextgen! | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/aitextgen?style=social)](https://github.com/minimaxir/aitextgen) <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.aitextgen.io/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD) | 17.05.2021 |
| EasyNMT | Easy to use, state-of-the-art machine translation for more than 100+ languages | [Nils Reimers](https://www.nils-reimers.de/) | [![](https://img.shields.io/github/stars/UKPLab/EasyNMT?style=social)](https://github.com/UKPLab/EasyNMT) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00401), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11125)</li><li>[demo](http://easynmt.net/demo/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Helsinki-NLP/Opus-MT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/fairseq/tree/master/examples/multilingual)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1X47vgSiOphpxS5w_LPtjQgJmiSTNfRNC) | 26.04.2021 |
| OCTIS | Framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach | <ul><li>[Silvia Terragni](https://silviatti.github.io/)</li> <li>[Elisabetta Fersini](https://www.unimib.it/elisabetta-fersini)</li> <li>[Antonio Candelieri](https://www.unimib.it/antonio-candelieri)</li><details><summary>others</summary><li>[Pietro Tropeano](https://github.com/pietrotrope)</li> <li>[Bruno Galuzzi](https://github.com/brunoG89)</li> <li>[Lorenzo Famiglini](https://github.com/lorenzofamiglini)</li> <li>[Davide Pietrasanta](https://github.com/davidepietrasanta)</li></ul></details> | [![](https://img.shields.io/github/stars/mind-Lab/octis?style=social)](https://github.com/mind-Lab/octis) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.01488)</li><li>[data](https://www.dbpedia.org/resources/ontology/), [data](https://www.statmt.org/europarl/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/estebandito22/PyTorchAVITM)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6), [<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/a-beginners-guide-to-octis-vol-2-optimizing-topic-models-1214e58be1e5)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html)</li><li>[paper](https://aclanthology.org/2021.eacl-demos.31/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/dataset/20-newsgroups)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nPmiWBFFJ8E)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb/) | 19.04.2021 |
| PyTorchVideo | Deeplearning library with a focus on video understanding work | <ul><li>[Haoqi Fan](https://haoqifan.github.io/)</li> <li>[Tullie Murrell](https://github.com/tullie)</li> <li>[Heng Wang](https://hengcv.github.io/)</li><details><summary>others</summary><li>[Kalyan Vasudev Alwala](https://github.com/kalyanvasudev)</li> <li>[Yanghao Li](https://github.com/lyttonhao)</li> <li>[Yilei Li](https://liyilui.github.io/personal_page/)</li> <li>[Bo Xiong](https://github.com/bxiong1202)</li> <li>[Nikhila Ravi](https://nikhilaravi.com/)</li> <li>[Meng Li](https://mengli.me/)</li> <li>[Haichuan Yang](https://hyang1990.github.io/)</li> <li>[Jitendra Malik](https://scholar.google.com/citations?user=oY9R5YQAAAAJ)</li> <li>[Ross Girshick](https://github.com/rbgirshick)</li> <li>[Matt Feiszli](https://scholar.google.com/citations?user=A-wA73gAAAAJ)</li> <li>[Aaron Adcock](https://scholar.google.com/citations?&user=oa78zHUAAAAJ)</li> <li>[Wan-Yen Lo](https://github.com/wanyenlo)</li> <li>[Christoph Feichtenhofer](http://feichtenhofer.github.io/)</li></ul></details> | [![](https://api.juleskreuer.eu/citation-badge.php?doi=10.1145/3474085.3478329)](https://doi.org/10.1145/3474085.3478329) [![](https://img.shields.io/github/stars/facebookresearch/pytorchvideo?style=social)](https://github.com/facebookresearch/pytorchvideo) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2111.09887), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.11227)</li><li>[blog post](https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://pytorchvideo.readthedocs.io/en/latest/index.html)</li><li>[website](https://github.com/facebookresearch/pytorchvideo)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/b7-gnpqz9Qg)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/facebookresearch/pytorchvideo/blob/main/tutorials/accelerator/Build_your_model_with_PytorchVideo_Accelerator.ipynb/) | 13.04.2021 |
| GPT Neo | An implementation of model & data parallel GPT2 & GPT3 -like models, with the ability to scale up to full GPT3 sizes (and possibly more!), using the mesh-tensorflow library | [EleutherAI](https://www.eleuther.ai/) | [![](https://img.shields.io/github/stars/EleutherAI/gpt-neo?style=social)](https://github.com/EleutherAI/gpt-neo) <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.14165), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.05150), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.06538)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/mesh), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox/)</li><li>[pretrained](https://the-eye.eu/public/AI/gptneo-release/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb/) | 28.03.2021 |
| CVAE | This notebook demonstrates how train a Variational Autoencoder on the MNIST dataset | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1312.6114), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1401.4082)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb/) | 22.03.2021 |
| DCGAN | This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1511.06434), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.00160)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/jessicali9530/celeba-dataset)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb/) | 12.03.2021 |
| Adversarial FGSM | This tutorial creates an adversarial example using the Fast Gradient Signed Method attack. This was one of the first and most popular attacks to fool a neural network. | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6572)</li><li>[imagenet](http://www.image-net.org/)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb/) | 12.03.2021 |
| GAN steerability | We will navigate in GAN latent space to simulate various camera transformations | <ul><li>[Ali Jahanian](http://people.csail.mit.edu/jahanian/)</li> <li>[Lucy Chai](http://people.csail.mit.edu/lrchai/)</li> <li>[Phillip Isola](http://web.mit.edu/phillipi/)</li></ul> | [![](https://img.shields.io/github/stars/ali-design/gan_steerability?style=social)](https://github.com/ali-design/gan_steerability) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.07171), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[project](https://ali-design.github.io/gan_steerability/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/nS0V64sF7Cw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1kn6yG8PqD1U2bUcy32V1iAVjzlcQWcG3) | 04.03.2021 |
| bsuite | A collection of carefully-designed experiments that investigate core capabilities of an RL agent with two main objectives | <ul><li>[Ian Osband](http://iosband.github.io/)</li> <li>[Yotam Doron](http://www.yotamdoron.com/)</li> <li>[Matteo Hessel](https://github.com/mtthss)</li><details><summary>others</summary><li>[John Aslanides](https://www.aslanides.io/)</li> <li>[Eren Sezener](http://erensezener.com/)</li> <li>[Andre Saraiva](https://andresnds.wordpress.com/)</li> <li>[Katrina McKinney](https://medium.com/@katrinamckinney)</li> <li>[Tor Lattimore](http://tor-lattimore.com/)</li> <li>[Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/)</li> <li>[Satinder Singh](http://web.eecs.umich.edu/~baveja/)</li> <li>[Benjamin Van Roy](https://web.stanford.edu/~bvr/)</li> <li>[Richard Sutton](http://www.incompleteideas.net/)</li> <li>[David Silver](https://www.davidsilver.uk/)</li> <li>[Hado Van Hasselt](https://hadovanhasselt.com/)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/bsuite?style=social)](https://github.com/deepmind/bsuite) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/gym)</li><li>[paper](https://openreview.net/forum?id=rygf-kSYwH)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/Wcv4eU_qtZU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH) | 13.02.2021 |
| TF-Ranking | End-to-end walkthrough of training a TensorFlow Ranking neural network model which incorporates sparse textual features | [Rama Kumar](https://github.com/ramakumar1729) | [![](https://img.shields.io/github/stars/tensorflow/ranking?style=social)](https://github.com/tensorflow/ranking) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.09676), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.00073), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.08957), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.04415)</li><li>[data](http://hamedz.ir/resources/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto#L72)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb/) | 04.02.2021 |
| Toon-Me | A fun project to toon portrait images | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://img.shields.io/github/stars/vijishmadhavan/Toon-Me?style=social)](https://github.com/vijishmadhavan/Toon-Me) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/vijishmadhavan/Light-Up/blob/master/Toon_Me_(Try_it_on_Colab).ipynb/) | 22.01.2021 |
| TensorNetwork | A library for easy and efficient manipulation of tensor networks | [Chase Roberts](http://thenerdstation.github.io/) | [![](https://img.shields.io/github/stars/google/TensorNetwork?style=social)](https://github.com/google/TensorNetwork) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1708.00006), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1306.2164)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://tensornetwork.readthedocs.io/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YN2YBB0viKo)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/TensorNetwork/blob/master/colabs/Tensor_Networks_in_Neural_Networks.ipynb/) | 21.01.2021 |
| Spleeter | Deezer source separation library including pretrained models | <ul><li>[Romain Hennequin](http://romain-hennequin.fr/)</li> <li>[Anis Khlif](https://github.com/alreadytaikeune)</li> <li>[Félix Voituret](https://github.com/Faylixe)</li> <li>[Manuel Moussallam](https://mmoussallam.github.io/)</li></ul> | [![](https://img.shields.io/github/stars/deezer/spleeter?style=social)](https://github.com/deezer/spleeter) <ul><li>[blog post](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[project](https://research.deezer.com/projects/spleeter.html)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deezer/spleeter/blob/master/spleeter.ipynb/) | 10.01.2021 |
| Person Remover | Project that combines Pix2Pix and YOLO arhitectures in order to remove people or other objects from photos | <ul><li>[Javier Gamazo](https://www.javiergamazo.com/)</li> <li>[Daryl Autar](https://github.com/Daryl149)</li></ul> | [![](https://img.shields.io/github/stars/javirk/Person_remover?style=social)](https://github.com/javirk/Person_remover) <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/javirk/Person-remover-partial-convolutions), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zzh8829/yolov3-tf2)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_dRjY9gMcxE)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1JDpH8MAjaKoekQ_H9ZaxYJ9_axiDtDGm) | 22.08.2020 |
| Semantic Segmentation | Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset | <ul><li>[Bolei Zhou](https://boleizhou.github.io/)</li> <li>[Hang Zhao](https://hangzhaomit.github.io/)</li> <li>[Xavier Puig](https://people.csail.mit.edu/xavierpuig/)</li><details><summary>others</summary><li>[Sanja Fidler](http://www.cs.toronto.edu/~fidler/index.html)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul></details> | [![](https://img.shields.io/github/stars/CSAILVision/semantic-segmentation-pytorch?style=social)](https://github.com/CSAILVision/semantic-segmentation-pytorch) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1608.05442), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1612.01105), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.10221), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.04514)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/sceneparsing), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hszhao/semseg)</li><li>[project](http://sceneparsing.csail.mit.edu/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb/) | 21.08.2020 |
| CoVoST | A Large-Scale Multilingual Speech-To-Text Translation Corpus | <ul><li>[Changhan Wang](https://www.changhan.me/)</li> <li>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)</li> <li>[Jiatao Gu](http://jiataogu.me/)</li></ul> | [![](https://img.shields.io/github/stars/facebookresearch/covost?style=social)](https://github.com/facebookresearch/covost) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2002.01320), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/pdf/2007.10310), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.06670)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://www.changhan.me//SpeechTransProgress), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/pytorch/fairseq/tree/main/examples/speech_to_text), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/vizseq)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/11GK7k7G1CG1qHbdA9Pz1RtQ3vlCkuohV) | 07.08.2020 |
| Analyzing Tennis Serve | We'll use the Video Intelligence API to analyze a tennis serve, including the angle of the arms and legs during the serve | [Dale Markowitz](https://daleonai.com/) | [![](https://img.shields.io/github/stars/google/making_with_ml?style=social)](https://github.com/google/making_with_ml/tree/master/sports_ai) <ul><li>[blog post](https://daleonai.com/machine-learning-for-sports)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=yLrOy2Xedgk)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/google/making_with_ml/blob/master/sports_ai/Sports_AI_Analysis.ipynb/) | 14.07.2020 |
| Eager Few Shot Object Detection | Fine tuning of a RetinaNet architecture on very few examples of a novel class after initializing from a pre-trained COCO checkpoint | [kmindspark](https://github.com/kmindspark) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1708.02002)</li><li>[data](https://cocodataset.org/#explore)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/few-shot-object-detection)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb/) | 11.07.2020 |
| Optuna | An automatic hyperparameter optimization software framework, particularly designed for machine learning | <ul><li>[Takuya Akiba](https://iwiwi.github.io/)</li> <li>[Shotaro Sano](https://github.com/g-votte)</li> <li>[Toshihiko Yanase](https://github.com/toshihikoyanase)</li><details><summary>others</summary><li>[Takeru Ohta](https://github.com/sile)</li> <li>[Masanori Koyama](https://scholar.google.com/citations?user=oY1gA10AAAAJ)</li></ul></details> | [![](https://img.shields.io/github/stars/optuna/optuna?style=social)](https://github.com/optuna/optuna) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.10902)</li><li>[docker](https://hub.docker.com/r/optuna/optuna)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://optuna.readthedocs.io/en/stable/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/optuna/optuna-dashboard)</li><li>[website](https://optuna.org/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/J_aymk4YXhg), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/tcrcLRopTX0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/-UeC4MR3PHM), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/oC8zFYcfYXU)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/optuna/optuna-examples/blob/main/quickstart.ipynb/) | 08.07.2020 |
| YOLOv4 | This tutorial will help you build YOLOv4 easily in the cloud with GPU enabled so that you can run object detections in milliseconds! | [Alexey Bochkovskiy](http://www.alexeyab.com/) | [![](https://img.shields.io/github/stars/AlexeyAB/darknet?style=social)](https://github.com/AlexeyAB/darknet) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.10934), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08036)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982)</li><li>[project](https://pjreddie.com/darknet/)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/MachineLearning/comments/gydxzd/p_yolov4_the_most_accurate_realtime_neural/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/1_SiUOYUoOI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YDFf-TqJOFE)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg) | 25.06.2020 |
| Context R-CNN Demo | This notebook will walk you step by step through the process of using a pre-trained model to build up a contextual memory bank for a set of images, and then detect objects in those images+context using Context R-CNN | [pkulzc](https://github.com/pkulzc) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.03538)</li><li>[data](https://lila.science/datasets/snapshot-serengeti)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/context_rcnn_tutorial.ipynb/) | 17.06.2020 |
| Background Matting | The notebook is split into three parts: required setup, running the algorithm on photos, and running it on videos | [Andrey Ryabtsev](https://github.com/andreyryabtsev) | [![](https://img.shields.io/github/stars/senguptaumd/Background-Matting?style=social)](https://github.com/senguptaumd/Background-Matting) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00626)</li><li>[blog post](https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635)</li><li>[data](https://drive.google.com/open?id=1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/gist/andreyryabtsev/243aa3eefa6e06891dda7b1583d1d08f/backmatting.ipynb/) | 18.05.2020 |
| GAN Dissection | Visualizing and Understanding Generative Adversarial Networks | <ul><li>[David Bau](https://people.csail.mit.edu/davidbau/home/)</li> <li>[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)</li> <li>[Hendrik Strobelt](http://hendrik.strobelt.com/)</li><details><summary>others</summary><li>[Bolei Zhou](https://boleizhou.github.io/)</li> <li>[Joshua Tenenbaum](https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/)</li> <li>[William Freeman](https://billf.mit.edu/)</li> <li>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)</li></ul></details> | [![](https://img.shields.io/github/stars/CSAILVision/GANDissect?style=social)](https://github.com/CSAILVision/GANDissect) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.10597), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1901.09887), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1807.10221)</li><li>[demo](http://gandissect.res.ibm.com/ganpaint.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CSAILVision/NetDissect), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/junyanz/iGAN)</li><li>[project](https://gandissect.csail.mit.edu/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=yVCgUYe4JTM)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/SIDN-IAP/global-model-repr/blob/master/notebooks/gandissect_solutions.ipynb/) | 04.05.2020 |
| Sonnet | Library built on top of TensorFlow 2 designed to provide simple, composable abstractions for machine learning research | <ul><li>[Malcolm Reynolds](https://github.com/malcolmreynolds)</li> <li>[Jack Rae]()</li> <li>[Andreas Fidjeland](https://github.com/akfidjeland)</li><details><summary>others</summary><li>[Fabio Viola](https://github.com/fabioviola)</li> <li>[Adrià Puigdomènech](https://github.com/adria-p)</li> <li>[Frederic Besse](https://github.com/fbesse)</li> <li>[Tim Green](http://tfgg.me/)</li> <li>[Sébastien Racanière](https://scholar.google.com/citations?user=o-h0vrQAAAAJ)</li> <li>[Gabriel Barth-Maron](https://github.com/fastturtle)</li> <li>[Diego de Las Casas](https://github.com/diegolascasas)</li></ul></details> | [![](https://img.shields.io/github/stars/deepmind/sonnet?style=social)](https://github.com/deepmind/sonnet) <ul><li>[blog post](https://www.deepmind.com/blog/open-sourcing-sonnet-a-new-library-for-constructing-neural-networks)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://sonnet.readthedocs.io/en/latest/index.html)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/checkpoint), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/saved_model)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/rlpQjnUvoKw)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/deepmind/sonnet/blob/v2/examples/little_gan_on_mnist.ipynb/) | 17.04.2020 |
| Classification of chest vs. adominal X-rays | The goal of this tutorial is to build a deep learning classifier to accurately differentiate between chest and abdominal X-rays | [tmoneyx01](https://github.com/tmoneyx01) | [![](https://img.shields.io/github/stars/mdai/mdai-client-py?style=social)](https://github.com/mdai/mdai-client-py) <ul><li>[annotator](https://public.md.ai/annotator/project/PVq9raBJ)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.md.ai/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb/) | 07.03.2020 |
| Lung X-Rays Semantic Segmentation | This lesson applies a U-Net for Semantic Segmentation of the lung fields on chest x-rays | [tmoneyx01](https://github.com/tmoneyx01) | [![](https://img.shields.io/github/stars/mdai/mdai-client-py?style=social)](https://github.com/mdai/mdai-client-py) <ul><li>[annotator](https://public.md.ai/annotator/project/aGq4k6NW)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1505.04597)</li><li>[data](https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.md.ai/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mdai/ml-lessons/blob/master/lesson2-lung-xrays-segmentation.ipynb/) | 07.03.2020 |
| Earth Engine Python API and Folium Interactive Mapping | This notebook demonstrates how to setup the Earth Engine and provides several examples for visualizing Earth Engine processed data interactively using the folium library | [Qiusheng Wu](https://wetlands.io/) | [![](https://img.shields.io/github/stars/python-visualization/folium?style=social)](https://github.com/python-visualization/folium) <ul><li>[api](https://developers.google.com/earth-engine/python_install)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/giswqs/qgis-earthengine-examples/blob/master/Folium/ee-api-folium-setup.ipynb/) | 20.01.2020 |
| Train a GPT-2 Model on Tweets | Train the model on your downloaded tweets, and generate massive amounts of Tweets from it | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/download-tweets-ai-text-gen?style=social)](https://github.com/minimaxir/download-tweets-ai-text-gen) <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1qxcQ2A1nNjFudAGN_mcMOnvV9sF_PkEb) | 16.01.2020 |
| Traffic counting | Making Road Traffic Counting App based on Computer Vision and OpenCV | [Andrey Nikishaev](https://github.com/creotiv) | [![](https://img.shields.io/github/stars/creotiv/object_detection_projects?style=social)](https://github.com/creotiv/object_detection_projects/tree/master/opencv_traffic_counting) <ul><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/machine-learning-world/tutorial-making-road-traffic-counting-app-based-on-computer-vision-and-opencv-166937911660)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_o5iLbRHKao)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/12N4m_RYKqrpozRzh9qe7nQE_sIqQH9U8) | 10.01.2020 |
| Imagededup | This package provides functionality to make use of hashing algorithms that are particularly good at finding exact duplicates as well as convolutional neural networks which are also adept at finding near duplicates | <ul><li>[Tanuj Jain](https://github.com/tanujjain)</li> <li>[Christopher Lennan](https://github.com/clennan)</li> <li>[Dat Tran](https://dat-tran.com/)</li></ul> | [![](https://img.shields.io/github/stars/idealo/imagededup?style=social)](https://github.com/idealo/imagededup) <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1704.04861)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5)</li><li>[project](https://idealo.github.io/imagededup/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/idealo/imagededup/blob/master/examples/CIFAR10_duplicates.ipynb/) | 03.10.2019 |
| automl-gs on a TPU | Give an input CSV file and a target field you want to predict to automl-gs, and get a trained high-performing machine learning or deep learning model plus native Python code pipelines allowing you to integrate that model into any prediction workflow | [Max Woolf](https://minimaxir.com/) | [![](https://img.shields.io/github/stars/minimaxir/automl-gs?style=social)](https://github.com/minimaxir/automl-gs)  | [![Open In TIR](images/tir-logo-dark.svg)](https://colab.research.google.com/drive/1sbF8cqnOsdzN9Bdt74eER5s_xXcdvatV) | 26.03.2019 |
| RSNA Pneumonia Detection Challenge (Kaggel API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | [![](https://img.shields.io/github/stars/mdai/mdai-client-py?style=social)](https://github.com/mdai/mdai-client-py) <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.md.ai/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-kaggle.ipynb/) | 03.09.2018 |
| RSNA Pneumonia Detection Challenge (MD.ai API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | [![](https://img.shields.io/github/stars/mdai/mdai-client-py?style=social)](https://github.com/mdai/mdai-client-py) <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[<img src="images/docs.svg" alt="docs" height=20/>](https://docs.md.ai/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb/) | 29.08.2018 |
| HoF | This notebook will walk you step by step through the process of using a pre-trained model to detect faces in an image | [Lucas Persona](http://www.lucaspersona.com/) | [![](https://img.shields.io/github/stars/the-house-of-black-and-white/hall-of-faces?style=social)](https://github.com/the-house-of-black-and-white/hall-of-faces) <ul><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)</li><li>[yolo](https://pjreddie.com/darknet/yolo/)</li></ul> | [![Open In TIR](images/tir-logo-dark.svg)](https://thor-gpu.e2enetworks.net/github/the-house-of-black-and-white/hall-of-faces/blob/master/notebooks/Hall_of_Faces.ipynb/) | 15.03.2018 |
# Best of the best
| authors | repositories |
|---|---|
| <ul><li>[Billy Lamberta](https://github.com/lamberta)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li> <li>[Ziwei Liu](https://liuziwei7.github.io/)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Max Woolf](https://minimaxir.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Björn Ommer](https://ommer-lab.com/people/ommer/)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Google](https://www.tensorflow.org/)</li> <li>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)</li> <li>[Patrick Esser](https://github.com/pesser)</li> <li>[Robin Rombach](https://github.com/rromb)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Bolei Zhou](https://boleizhou.github.io/)</li> <li>[Krzysztof Ostrowski](https://github.com/krzys-ostrowski)</li></ul> | <ul><li>tensorflow/models [![](https://img.shields.io/github/stars/tensorflow/models?style=social)](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)</li> <li>CompVis/stable-diffusion [![](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social)](https://github.com/CompVis/stable-diffusion)</li> <li>CorentinJ/Real-Time-Voice-Cloning [![](https://img.shields.io/github/stars/CorentinJ/Real-Time-Voice-Cloning?style=social)](https://github.com/CorentinJ/Real-Time-Voice-Cloning)</li> <li>iperov/DeepFaceLab [![](https://img.shields.io/github/stars/iperov/DeepFaceLab?style=social)](https://github.com/iperov/DeepFaceLab)</li> <li>ultralytics/yolov5 [![](https://img.shields.io/github/stars/ultralytics/yolov5?style=social)](https://github.com/ultralytics/yolov5)</li> <li>jakevdp/PythonDataScienceHandbook [![](https://img.shields.io/github/stars/jakevdp/PythonDataScienceHandbook?style=social)](https://github.com/jakevdp/PythonDataScienceHandbook)</li> <li>openai/whisper [![](https://img.shields.io/github/stars/openai/whisper?style=social)](https://github.com/openai/whisper)</li> <li>facebookresearch/segment-anything [![](https://img.shields.io/github/stars/facebookresearch/segment-anything?style=social)](https://github.com/facebookresearch/segment-anything)</li> <li>LAION-AI/Open-Assistant [![](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)](https://github.com/LAION-AI/Open-Assistant)</li> <li>microsoft/visual-chatgpt [![](https://img.shields.io/github/stars/microsoft/visual-chatgpt?style=social)](https://github.com/microsoft/visual-chatgpt)</li> <li>google-research/google-research [![](https://img.shields.io/github/stars/google-research/google-research?style=social)](https://github.com/google-research/google-research/tree/master/dreamfields)</li> <li>TencentARC/GFPGAN [![](https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social)](https://github.com/TencentARC/GFPGAN)</li> <li>pytorch/fairseq [![](https://img.shields.io/github/stars/pytorch/fairseq?style=social)](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/xlsr/README.md)</li> <li>ray-project/ray [![](https://img.shields.io/github/stars/ray-project/ray?style=social)](https://github.com/ray-project/ray)</li> <li>facebookresearch/detectron2 [![](https://img.shields.io/github/stars/facebookresearch/detectron2?style=social)](https://github.com/facebookresearch/detectron2)</li> <li>Stability-AI/stablediffusion [![](https://img.shields.io/github/stars/Stability-AI/stablediffusion?style=social)](https://github.com/Stability-AI/stablediffusion)</li> <li>google/jax [![](https://img.shields.io/github/stars/google/jax?style=social)](https://github.com/google/jax/issues/446#issuecomment-467105048)</li></ul> |

[![Stargazers over time](https://starchart.cc/amrzv/awesome-colab-notebooks.svg)](https://starchart.cc/amrzv/awesome-colab-notebooks)

(generated by [generate_markdown.py](generate_markdown.py) based on [research.json](data/research.json) and [tutorials.json](data/tutorials.json))
